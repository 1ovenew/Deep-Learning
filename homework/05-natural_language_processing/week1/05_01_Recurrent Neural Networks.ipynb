{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursera | Andrew Ng (05-week1)—循环序列模型\n",
    "\n",
    "【第 5 部分-序列模型-第一周】在吴恩达深度学习视频基础上，笔记总结，添加个人理解。- ZJ\n",
    "    \n",
    ">[Coursera 课程](https://www.coursera.org/specializations/deep-learning) |[deeplearning.ai](https://www.deeplearning.ai/) |[网易云课堂](https://mooc.study.163.com/smartSpec/detail/1001319001.htm)\n",
    "\n",
    "[CSDN]()：\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# 循环序列模型 （Recurrent Neural Networks）\n",
    "\n",
    "\n",
    "## <font color=#0099ff>1.1 为什么选择序列模型？(Why sequence models?)\n",
    "    \n",
    "深度学习中最激动人心的领域之一：序列模型。\n",
    "\n",
    "例如循环神经网络（RNN）在语音识别，自然语言处理等其他领域中引起变革。\n",
    "\n",
    "能够应用序列模型的例子:语音识别、音乐生成、情感分类、DNA 序列分析、机器翻译、视频活动识别、名称实体识别等。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180224155604642?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "   \n",
    "**学习目标：学习构建自己的序列模型。**    \n",
    "    \n",
    "---\n",
    "## <font color=#0099ff>1.2 数学符号 (Notation)\n",
    "    \n",
    "从**定义数学符号**开始，一步步构建序列模型。    \n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180225121806505?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "输入 x : Harry Potter and Hermione Granger invented a new spell.\n",
    "\n",
    "**目标：构建**一个能够**自动识别句中人名位置**的**序列模型**。---命名实体识别问题（Named-entity recognition）NER\n",
    "\n",
    "此例子中为：识别  Harry Potter 和 Hermione Granger \n",
    "\n",
    "$x^{<1>}  x^{<2>}   x^{<3>}   ......x^{<t>} ......x^{<9>} $   9 组特征表示上述 x 中 9 个单词。\n",
    "\n",
    "输出 y : 1    1    0  1   1  0   0    0  0\n",
    "\n",
    "$y^{<1>}  y^{<2>}   y^{<3>}   ......y^{<t>} ......y^{<9>} $ \n",
    "\n",
    "t : 意味着这些是时间序列。\n",
    "\n",
    "输入序列的长度：$T_x=9$\n",
    "\n",
    "输出序列的长度：$T_y=9$\n",
    "\n",
    "$x^{(i)<t>}$: 第  i  个 训练样本 x 的第 t  个元素。 $T^{(I)}_x$ : 第 i 个训练样本的输入序列长度。\n",
    "\n",
    "$y^{(i)<t>}$: 第  i  个训练样本  y 的第 t  个元素。 $T^{(I)}_y$ : 第 i 个训练样本的输出序列长度。\n",
    "\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180225121824703?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "   \n",
    " 使用**自然语言处理 （NLP）**,**如何在序列中表示单个单词?**\n",
    "\n",
    "首先，需要词汇表（字典）。列出在表示中所使用的所有单词。如图中所表示，不同的单词在自己的词汇表中对应得索引不同。\n",
    "此例子中使用的 10000 个单词的词汇表，根据不同的商业模式，应用，所构建的词汇表不同，30000，50000，（大型互联网）1000000等。\n",
    "\n",
    "**One-hot 向量表示法：**\n",
    "\n",
    "如上图所示：$x^{<1>}$ 是一个 10000 维度的向量，只有 第 4075 索引位置是 1，其余位置都是 0 。    \n",
    "    \n",
    "对于不在词汇表中的单词的表示：返回   `<UNK>`    (Unknown Word)\n",
    "\n",
    "---\n",
    "## <font color=#0099ff>1.3 循环神经网络模型 (Recurrent Neural Network Model)   \n",
    " \n",
    "对于上述 命名实体识别问题 (NER) 之所以不使用标准神经网络来解决，是因为存在两个主要问题。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180225153812706?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70) \n",
    "\n",
    "- 1.首先是在不同的例子中输入和输出可以是不同的长度。\n",
    "- 2.像这样一个简单的神经网络架构，它不会共享在文本中不同位置上学到的特征。(例如卷积神经网络，可以传递学习到的特征等)\n",
    "- 此外，每一个训练样本都是一个 10,000 维的向量。所以，这只是一个非常大的输入层，同时，第一层的权重矩阵将最终具有大量的参数。\n",
    "\n",
    " \n",
    "![这里写图片描述](http://img.blog.csdn.net/20180225153839261?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "使用**循环神经网络（RNN）** 可以解决上述两个问题。\n",
    "\n",
    "什么是循环神经网络？\n",
    "\n",
    "如果你从左到右阅读句子，第一个单词 $x^{<1>}$ 以及 初始化 $a^{<0>}$ 零向量 ，带入到神经网络层（隐含层），尝试预测输出 $\\hat{y}^{<1>}$，同时在时间步 1 产生  $a^{<1>}$。然后，将 $a^{<1>}$ 和 $x^{<2>}$带入到神经网络层（隐含层），尝试预测输出 $\\hat{y}^{<2>}$，然后前向广播，进入下一个时间步......直到最后输入 $x^{<T_x>}$,输出 $\\hat{y}^{<T_y>}$。因此，在每个时间步，循环神经网络将这个激活传递给下一个时间步，供它使用。\n",
    "\n",
    "在这个例子中，$T_x = T_y$ ,若 $T_x ， T_y$  不同，体系结构会改变一点。\n",
    "\n",
    "上图中右侧图表，是在 论文或书籍中常见的图表，为了便于理解，Andrew Ng 使用左侧图表展开讲解。\n",
    "\n",
    "每个时间步的参数是共享的，每一部分都受到参数的影响，如 $W_{aa},W_{ax},W_{ya}$,后面在解释这些参数的工作原理。\n",
    "\n",
    "**RNN 的一个弱点：**，它只使用序列中较早的信息进行预测，特别是在预测  $\\hat{y}^{<3>}$ 时，不使用 $x^{<4>}，x^{<5>}，x^{<6>}$ 等单词的信息。\n",
    "\n",
    "例如： \n",
    "\n",
    "1. He said,\"Teddy Roosevelt was a great President.\"\n",
    "2. He said,\"Teddy bears are on sale!\"\n",
    "\n",
    "上述例子中，Teddy Roosevelt 是人名。Teddy bears 并非我们想要识别的内容，但是如果只知道前三个单词 是无法准确识别的，还需要后面的单词信息。所以这个特定的神经网络结构的一个限制是，在某一时刻的预测使用输入或使用来自序列中较早的输入的信息，而不是在该序列的后面的信息。\n",
    "\n",
    "后面的视频将讲解 **双向递归神经网络 （BRNN）**，来解决这个问题。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180225153859865?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "**前向传播：** \n",
    "\n",
    "$a^{<0>}=\\vec{0}$  \n",
    "\n",
    "$a^{<1>}=g(W_{aa}a^{<0>} + W_{ax}x^{<1>} + b_a)$ \n",
    "$\\hat{y}^{<1>} = g(W_{ya}a^{<1>} + b_y)$\n",
    "\n",
    "$a^{<t>}=g(W_{aa}a^{<t-1>} + W_{ax}x^{<t>} + b_a)$ \n",
    "$\\hat{y}^{<t>} = g(W_{ya}a^{<t>} + b_y)$\n",
    "\n",
    "对于激活函数计算 $a^{<t>}$常用的是 **tanh** , 也有使用 **Relu** 的。\n",
    "\n",
    "对于输出 y ,根据具体问题选取不同激活函数，如二分类问题可使用 **sigmoid**，其他 输出 如 softmax 等。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180225153912301?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "**简化 RNN 符号表示：**\n",
    "\n",
    "$a^{<t>}=g(W_{aa}a^{<t-1>} + W_{ax}x^{<t>} + b_a)$ \n",
    "$\\hat{y}^{<t>} = g(W_{ya}a^{<t>} + b_y)$\n",
    "\n",
    "简化为：\n",
    "\n",
    "$a^{<t>}=g(W_{a}[a^{<t-1>} ,x^{<t>} ]+ b_a)$ \n",
    "$\\hat{y}^{<t>} = g(W_{y}a^{<t>} + b_y)$    \n",
    "\n",
    "使用矩阵的叠加简化，如 a 的维度是 100，x 的维度是 10000，那么 $W_{aa}$ 的维度是 （100，100），$W_{ax}$ 的维度是 （100，10000）,叠加后 $W_{a}$ 的维度是 （100，100100），如上图中所示。\n",
    "\n",
    "    \n",
    "---\n",
    "## <font color=#0099ff>1.4 通过时间的反向传播  (Backpropogation through time)  \n",
    " \n",
    " 上一节讲述的是 RNN 的基本结构，以及前向广播，这一节学习目标是：了解 RNN 的反向传播是如何工作的。\n",
    " \n",
    "![这里写图片描述](http://img.blog.csdn.net/20180225180047463?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "红色箭头方向是反向传播。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180225181207104?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "    \n",
    "如上图所示：红色箭头方向是反向传播。绿色线参数相同。\n",
    "\n",
    "逻辑回归损失，也称为交叉熵损失。\n",
    "\n",
    "单个单一位置或单个时间单一预测的单个词汇的损失：\n",
    "\n",
    "$L^{<t>}(\\hat{y}^{<t>},y^{<t>})= - y^{(i)}log\\hat{y}^{<t>} - (1- y^{<t>}) log(1-\\hat{y}^{<t>} )$\n",
    "\n",
    "如上图所示，每个特征元素计算损失值。\n",
    "\n",
    "整个序列的整体损失：\n",
    "\n",
    "$L(\\hat{y},y)= \\sum_{t=1}^{T_y} L^{<t>}(\\hat{y}^{<t>},y^{<t>})$\n",
    "\n",
    "\n",
    "以上是 RNN 前向和反向的工作原理，以上讲解的例子都是输入序列的长度等于输出序列的长度，接下来会展示更广泛的RNN架构。\n",
    "\n",
    "---\n",
    "## <font color=#0099ff>1.5 不同类型的循环神经网络  （Different types of RNNs）  \n",
    "\n",
    "到目前为止，您已经看到了一种 RNN 架构，例如在 Name entity  recognition 中$T_x = T_y$。接下来介绍其他不同的 RNN 体系结构。\n",
    "    \n",
    "![这里写图片描述](http://img.blog.csdn.net/20180225184657976?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "上图所示例子中，展示了多种不同的 RNN 架构。$T_x 和 T_y$ 存在不相等的情况。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180225184708713?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "Many-to-Many(相等) : $T_x = T_y$ ,如 Name entity  recognition （命名实体识别）。\n",
    "\n",
    "Many-to-One :  $T_x \\neq T_y$，$x = text, y = 0/1 或 1...5$ 等，如 Sentiment Classification (情感分类)，\n",
    "\n",
    "One-to-One : 标准神经网络。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180225184718247?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "\n",
    "One-to- Many: $T_x \\neq T_y$，$x = Ø$ ，x 可以为空集， 如 Music generation (音乐生成) 。当你真正生成序列的时候，经常会把这些前一个合成的输出文件输入到下一层。\n",
    "\n",
    "Many-to-Many(不等) : $T_x \\neq T_y$ ，如 Machine translation (机器翻译)，法语翻译成英语，单词数量是不同的，则使前半部分为 encoder，共 $T_x$ 个，后半部分为 decoder  ,共 $T_y$ 个。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180225184727814?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "其中很有意思的一部分是 序列生成，下节讲解。\n",
    "\n",
    "---\n",
    "## <font color=#0099ff>1.6 语言模型和序列生成  (Language  model and sequence generation)  \n",
    "    \n",
    "   那么什么是语言模型？\n",
    " \n",
    " ![这里写图片描述](http://img.blog.csdn.net/2018022610264353?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)   \n",
    "\n",
    "在语音识别系统中，我说了一句话，然后一个语言模型能够告诉我，对于这句话识别得到的多种可能性中，哪种可能概率高。它所做的就是，可以告诉你某个特定的句子出现的概率是多少。\n",
    "\n",
    "语言模型的基本工作原理可以理解为，输出一段语句，它可以给出每一部分的概率，表示为$P(y^{<1>},y^{<2>}......,y^{<T_y>})$,注意，这里用 y 表示，比用 x 表示更合适。后面会解释。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180226102652256?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "如何建立一个语言模型呢？要使用 RNN 建立这样一个模型，你首先需要一个训练集包括大量的英文文本。NLP 术语：语料库。\n",
    "\n",
    "如上图所示，在有了以上基础的前提下，在训练集中的一个句子。Cats average 15  hours of sleep a day.\n",
    "\n",
    "这句话结尾需添加一个 Tokenize `<EOS>`代表句子结束。 然后 每一个单词 使用 One-hot vectors 表示。如 10000 维度的向量。\n",
    "\n",
    "如上图所示，不在词汇表中的使用 `<UNK>` 表示。\n",
    "\n",
    "对于训练样本 $x^{<t>} = y^{<t-1>}$, 下面解释为什么这么表示。\n",
    "\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180226102701203?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "如上图所示：注意一开始，从左向右看，第一个词是未知的。\n",
    "\n",
    "$x^{<1>}=\\vec0$,$a^{<0>}=\\vec0$,然后预测输出 $\\hat{y}^{<1>}$ 使用 softmax ,假设词汇表是 10000 个，则预测输出的 概率是 `P(a)P(aron)....P(cats)...P(Zulu)` 额外还有 `P(<UNK>)P(<EOS>)` 共 10002 个。\n",
    "\n",
    "然后在预测出第一个之后，会将第一个正确的单词作为输入，$x^{<2>} = y ^{<1>}$ 来预测第二个单词 $\\hat{y}^{<2>}$ , P(average | cats)。如上图所示，依次代入，一直到预测完最后一个单词。注意 因为句子结尾是 `<EOS>` 所以 最后一个的 输入为 $x^{<9>} = y ^{<8>}$ ，输出为 $\\hat{y}^{<9>}$ `P(<EOS>| ...... )`。 \n",
    "\n",
    "单个语句的损失函数（softmax 层后的函数输出）：\n",
    "\n",
    "$L(\\hat{y}^{<t>},y^{<t>}) = - \\sum_{i}y_{i}^{<t>}log\\hat{y}_{i}^{<t>}$ \n",
    "\n",
    "整个句子的损失函数：\n",
    "\n",
    "$L = \\sum L^{<t>}(\\hat{y}^{<t>},y^{<t>})$\n",
    "\n",
    "简单的例子，一个句子有三个单词，分别定义为 $y^{<1>},y^{<2>},y^{<3>}$,那么它的概率是： \n",
    "\n",
    "$P(y^{<1>},y^{<2>},y^{<3>}) =P(y^{<1>}) P(y^{<2>} | y^{<1>})P(y^{<3>} | y^{<1>},y^{<2>})$\n",
    "\n",
    "\n",
    "---\n",
    "## <font color=#0099ff>1.7 对新序列采样  （Sampling novel sequences）  \n",
    "\n",
    "在训练完一个序列模型之后，要想了解这个模型学习到了什么，可以采用一个非正式的方法是：进行一次新序列采样。\n",
    "\n",
    "记住：一个序列模型，模拟了任意特定单词序列的概率。\n",
    "\n",
    "我们要做的就是对这个概率分布进行采样，来生成一个新的单词序列。 \n",
    "    \n",
    "![这里写图片描述](http://img.blog.csdn.net/20180226102731195?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)    \n",
    "\n",
    "首先 ：$x^{<1>}=\\vec0$ , $a^{<0>}=\\vec0$, 然后预测输出 $\\hat{y}^{<1>}$ ，得出的概率比如是：P(a)P(aron)....P(cats)...P(Zulu)P(`<UNK>`) ,然后 使用采样函数，如 `np.random.choice` 对 softmax 分布进行随机采样。得出 $\\hat{y}^{<1>}$ 是 The, 然后输入到下一层，前向广播，进入下一个时间步。直到结束。\n",
    "\n",
    "如何标记结束？有两种方式： 1. 词汇表中含有 `<EOS>`  2. 限定时间步，20 ，100 等。\n",
    "\n",
    "而这个特定的程序有时会产生一个 `<UNK>` 的单词标记。如果你想确保你的算法从来没有产生这个标记，你可以做的一件事就是拒绝任何以未知的单词标记出来的样本，并且继续从其他词汇中重新采样，直到你得到一个不是未知的单词。\n",
    "\n",
    "最后，经过以上步骤，就随机产生了一些序列语句。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/2018022610274193?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "目前我们建立的都是基于词汇的 RNN 模型，根据实际应用，也可以建立一个基于字符的 RNN 结构。如上图所示。\n",
    "\n",
    "Vocabulary = [a,b,c,...0,1,...9, ; ,', : , ,...A....Z]\n",
    "\n",
    "相对应的  $y^{<1>},y^{<2>},y^{<3>}$ 都是字符，而不是单个词汇，如 Cats average ....  $y^{<1>}= C,y^{<2>}=a,y^{<3>}=t ,y^{<4>}= 空格$等\n",
    "\n",
    "基于字符的优点： 不用担心 UNK 出现。\n",
    "主要的缺点：最后会得到太多太长的序列，所以在捕捉句子中的依赖关系方面，效果并不是很好。计算训练成本也很高。\n",
    "\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/2018022610275051?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "    \n",
    "上图，左侧是 一则新闻，右侧是 模仿莎士比亚的诗。\n",
    "\n",
    "---\n",
    "## <font color=#0099ff>1.8 带有神经网络的梯度消失  （Vanishing gradients with RNNs）  \n",
    "    \n",
    "前面已经了解了 RNN 的工作原理，接下来了解其中存在的问题，以及解决办法。\n",
    "\n",
    "基本的 RNN 算法，存在一个很大的问题: 就是梯度消失。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180226150729846?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "如图上所示：第一句 前面的 cat 单数 决定后面 是 was。 第二句 前面的 cats 复数决定后面是 were。这就是语言具有非常长期的依赖关系的一个例子。\n",
    "\n",
    "而迄今为止我们已经看到了 RNN 的基础结构，它不是非常擅长捕获非常长期的依赖关系。\n",
    "\n",
    "解释为什么？回想之前学过的梯度消失问题。\n",
    "\n",
    "一个非常非常深的神经网络，前向传播，再反向传播，输出的 $\\hat{y}$ 很难再影响前面层的权重。则 RNN 同理，梯度消失，不擅长捕捉长期依赖。\n",
    "\n",
    "回想 梯度爆炸问题：指数级大的梯度 会导致 参数爆炸性增长，神经网络参数崩溃，会看到很多 NaN，这意味着在你的神经网络计算数值溢出的结果。\n",
    "\n",
    "总结：训练很深的神经网络时，随着层数的增加，导数会出现指数级的下降，则导致 梯度消失。或者指数级的增加，导致梯度爆炸。\n",
    "\n",
    "解决梯度爆炸问题可以使用 梯度修剪（gradient clipping）解决。观察梯度向量，如果大于某个阈值，可以适当地额缩放梯度，保证其不会太大，（通过最大值 阈值来修剪），相对具有鲁棒性。\n",
    "\n",
    "而梯度消失就比较难解决，下面介绍 GRU （Gated Recurrent Unit）门控循环单元，来解决这个问题。\n",
    "\n",
    "\n",
    "---\n",
    "## <font color=#0099ff>1.9 GRU 单元  （Gated Recurrent Unit ）   \n",
    "    \n",
    "\n",
    "以上已经了解了基础 RNN 模型的运行机制。本节介绍 GRU 。改变了 神经网络隐藏层，使其可以更好的捕捉深层连接，并改善了梯度消失问题。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180226150739302?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "$a^{<t>}=g(W_{a}[a^{<t-1>} ,x^{<t>} ]+ b_a)$ \n",
    "\n",
    "如计算在时间 t 的激活值 $a^{<t>}$ , 输入  权重 $W_a$,上一个时间步的 激活值 $a^{<t-1>} $,当前输入值 $x^{<t>}$,以及偏移量 $b_a$。激活函数 g ，比如是 tanh。 上图中 左侧部分，绘画了 输入及输出，经过 tanh  后，在经过 softmax ，再输出 $\\hat{y}^{<t>}$。\n",
    "\n",
    "讲解这个图的目的是，将使用类似的图来讲解 GRU。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180226154123228?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "\n",
    "c = memory cell  : 记忆细胞，提供记忆能力，如 主语猫是单数还是复数。\n",
    "\n",
    "$c^{<t>}=a^{<t>}$ 在 GRU 中这两个值是相同的，后面的 LSTM 中是不同的\n",
    "\n",
    "$\\tilde{c}^{<t>}==tanh (W_c [c^{<t-1>},x^{<t>}] +  b_c)$\n",
    "\n",
    "$\\tilde{c}^{<t>}=$ : 候选值，代替 $c^{<t>}$的值，tanh 激活函数计算，$W_c$ 权重，$c^{<t-1>}$ 前一个时间步的记忆值，$x^{<t>}$ 当前输入值。$b_c)$ 偏移项。\n",
    "\n",
    "重点：GRU 中的真正思想。（决定是否更新）\n",
    "\n",
    "$Γ_u = \\sigma(W_u [c^{<t-1>},x^{<t>}] +  b_u)$\n",
    "\n",
    "$Γ_u$:更新门 （符号比较像 门的边缘），0 到 1 之间的值。$\\sigma$ sigmoid 函数 （其函数图 回想下，介于 0 到 1 之间） u: update 更新 \n",
    "\n",
    "假设 cat 输出 是 $c^{<t>} =1$ （1 代表单数，0 代表复数）memory cell 在从左往右读的过程中，会一直记住 这个值，直到 到了 was ，根据 $c^{<t>} =1$ 所以决定用 was 。\n",
    "\n",
    "$Γ_u$ 决定什么时候更新这个值，比如 看到 the cat 那么这是个好的时机去更新  $c^{<t>} =1$\n",
    "\n",
    "$c^{<t>} = Γ_u * \\tilde{c}^{<t>} + (1- Γ_u) * c^{<t-1>}$\n",
    "\n",
    "$c^{<t>} $ 等于 门的值 element-wise 候选值， $(1- Γ_u)$ element-wise  旧的值。若 $Γ_u ≈1$则代表把候选值 设为新的值。 若 $Γ_u ≈0$则代表 不更新，还是用旧的值。\n",
    "\n",
    "详细的图解看上图 左侧图示。简化的 GRU 示意图。\n",
    "\n",
    "优点：从左到右扫描句子时，门可以决定在哪个时间是否更新值。指导需要使用记忆细胞的时候。\n",
    "\n",
    " $Γ_u = 0.000000001$可以缓解梯度消失的问题，即使经过了很多很多的时间步，这个值也可以很好的维持。关键所在\n",
    " \n",
    "注意细节： $c^{<t>} $ 可以是向量，比如 维度 100 ，同理 上图中所标注出来的，都是相同的维度。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180226160258111?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "\n",
    "完整 GRU :\n",
    "\n",
    "再添加一个门 $Γ_r $ ,r 可以理解为代表相关性，前一个 $c^{<t-1>$ 对于计算 $\\tilde{c}^{<t>}$ 有多大的相关性。\n",
    "\n",
    "$\\tilde{c}^{<t>}==tanh (W_c [Γ_r  * c^{<t-1>},x^{<t>}] +  b_c)$\n",
    "\n",
    "$Γ_u = \\sigma(W_u [c^{<t-1>},x^{<t>}] +  b_u)$\n",
    "\n",
    "$Γ_r = \\sigma(W_r [c^{<t-1>},x^{<t>}] +  b_r)$\n",
    "\n",
    "$c^{<t>} = Γ_u * \\tilde{c}^{<t>} + (1- Γ_u) * c^{<t-1>}$\n",
    "\n",
    "\n",
    "产生更大的影响，更好的缓解 梯度消失问题，GRU 是一个常用的版本。LSTM （Long Short Term Memory 长短时记忆网络）也是比较常用的。\n",
    "\n",
    "---\n",
    "## <font color=#0099ff>1.10 长短期记忆   Long Short Term Memory Unit （LSTM）\n",
    "  \n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180226172251822?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180226172415589?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "---\n",
    "## <font color=#0099ff>1.11 双向神经网络 \n",
    "    \n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180226173329487?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/2018022617333950?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "---\n",
    "## <font color=#0099ff>1.12 深层循环神经网络\n",
    "    \n",
    "![这里写图片描述](http://img.blog.csdn.net/20180226173347938?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)    \n",
    "    \n",
    "    \n",
    "---\n",
    " \n",
    "**PS: 欢迎扫码关注公众号：「SelfImprovementLab」！专注「深度学习」，「机器学习」，「人工智能」。以及 「早起」，「阅读」，「运动」，「英语 」「其他」不定期建群 打卡互助活动。**\n",
    "\n",
    "<center><img src=\"http://upload-images.jianshu.io/upload_images/1157146-cab5ba89dfeeec4b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
