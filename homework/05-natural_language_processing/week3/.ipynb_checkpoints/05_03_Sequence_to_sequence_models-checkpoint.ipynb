{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursera | Andrew Ng (05-week3)—序列模型和注意力机制\n",
    "\n",
    "【第 5 部分-序列模型-第一周】在吴恩达深度学习视频基础上，笔记总结，添加个人理解。- ZJ\n",
    "    \n",
    ">[Coursera 课程](https://www.coursera.org/specializations/deep-learning) |[deeplearning.ai](https://www.deeplearning.ai/) |[网易云课堂](https://mooc.study.163.com/smartSpec/detail/1001319001.htm)\n",
    "\n",
    "\n",
    "[CSDN]()：\n",
    "   \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# 序列模型和注意力机制（Sequence to sequence models）\n",
    "\n",
    "## <font color=#0099ff>3.1 基础模型 (Basic models)\n",
    "    \n",
    "Hello and welcome to this final week of this course,as well as to the final week of this sequence offive courses in Deep Learning Specialization.You're nearly at the finish line.In this week, you'll hear about sequence and sequence models which areuseful for everything from machine translation to speech recognition.The start of the basic models,and then later this week, you hear about pin search,detention model, and we'll wrap up the discussion ofmodels audio data like speech. Let's get started.Let's say you want to input a French sentence like \"Jane visite I'Afrique en septembre.\"And, you want to translate it tothe English sentence \"Jane is visiting Africa in September.\"As usual, let's use x1 through x, in this case,five to represent the words and the input sequence,and we use y1 through y6 to represent the words in the output sequence.So, how can you train a neural network to input the sequence x,and output to sequence y.Well, here's something you could do.And, the ideas I'm about to present are mainly from these two papers,due to Ilya Sutskever,Oriol Vinyals, and Quoc V. Le,and that one by Kyunghyun Cho,Bart van Merrienboer, Calgar Gulcehre,Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.First, let's have a network which we're going to call the encoder network,be built as a RNN and this could be a jiayou analysia,feeding the input French words, one word at a time.And, after ingesting the input sequence,the RNN then outputs a vector that represents the input sentence.After that, you can build a decoder network,which you might draw here, which takes as inputthe encoding output by the encoding network shown in black on the left.And then, can be trained to output the translation,one word at a time.And eventually, it opens the CD endof sequence and the sentence token upon which the decoder stops.And, as usual, we could take the generator tokens and feed themto the next set of phase of state in the sequence,like we were doing before when synthesizing texts using the language model.One of the most remarkable recent results in Deep Learning is that this model works.Given enough pairs of French and English sentences,if you train a model to inputa French sentence and output the corresponding English translation,this will actually work decently well.And, this model simply uses an encoder network whose job it is to find an encoding ofthe input French sentence and then usea decoder network to then generate the corresponding English translation.An architecture very similar to this also works for image captioning.So, given an image like the one shown here,maybe you want it to be captioned automatically as a cat sitting on a chair.So, how do you train a neural network to input an image andoutput a caption like that face up there.Here's what you can do. From the earlier course on conference,you see how you can input an image into a convolutional network,maybe a pretrained AlexNet.And have that learn an encoding,or learn a set of feature of the input image.So, this is actually the AlexNet architecture.And, if we get rid of this final Softmax unit,the pretrained AlexNet can give youa 4096 dimensional feature vector of which to represent this picture of a cat.And so, this pretrained network can be the encoder network for the imageand you now have a 4000 or 4096 initial vector that represents the image.You can then take this and feed it to an RNN,whose job with it is to generate the caption one word at a time.So, similar to what we saw with machine translation,translating from French to English,you can now input a feature vector describing the inputand then have it generate an output sequence,output set of words one word at a time.And that actually works pretty well for image captioning,especially if the caption you want to generate is not too long.As far as I know, this type of model was first proposed by Junhua Mao,Wei Xu, Yi Yang, Jiang Wang,Zhiheng Huang, and Alan Yuille.Although it turns out there are multiple groups come out with very similar modelsindependently and at about the same time.So, two other groups that had donevery similar work at about the same time and I think independently are Oriol Vinyals,Alexander Toshev, Samy Bengio,and Dumitru Erhan as well as Andrej Karpathy and Fei-Fei Li.So, you've now seen how a basic sequence,a sequence model work,so how basic image the sequence or image captioning model works.But there are some differences between how yourun a model like this to generate the sequencecompared to how you were synthesizing novel text using a language model.One of the key differences is,you don't want a randomly chosen translation.You maybe want the most likely translation.You don't want the randomly chosen caption. Maybe not.But you might want the best caption and most likely caption.So let's see in the next video how you go about generating that.    \n",
    "    \n",
    "    \n",
    "---\n",
    "## <font color=#0099ff> 3.2 选择最有可能的句子(Picking the most likely sentence)\n",
    "    \n",
    "There are some similarities between the sequence to sequence machine translation modeland the language models that you have worked within the first week of this course,but there are some significant differences as well.Let's take a look. So, you can think ofmachine translation as building a conditional language model.Here's what I mean, in language modeling,this was the network we had built in the first week.And this model allows you to estimate the probability of a sentence.That's what a language model does.And you can also use this to generate novel sentences,and sometimes when you are writing x1 and x2 here,where in this example,x2 would be equal to y1 or equal to y and one is just a feedback.But x1, x2, and so on were not important.So just to clean this up for this slide,I'm going to just cross these off.X1 could be the vector of all zeros and x2,x3 are just the previous output you are generating.So that was the language model.The machine translation model looks as follows,and I am going to use a couple different colors,green and purple, to denote respectivelythe coded network in green and the decoded network in purple.And you notice that the decoded network looks pretty muchidentical to the language model that we had up there.So what the machine translation model is,is very similar to the language model,except that instead of always starting along with the vector of all zeros,it instead has an encoded networkthat figures out some representation for the input sentence,and it takes that input sentence and starts off the decoded network withrepresentation of the input sentence rather than with the representation of all zeros.So, that's why I call this a conditional language model,and instead of modeling the probability of any sentence,it is now modeling the probability of, say,the output English translation,conditions on some input French sentence.So in other words, you're trying to estimate the probability of an English translation.Like, what's the chance that the translation is \"Jane is visiting Africa in September,\"but conditions on the input French censors like,\"Jane visite I'Afrique en septembre.\"So, this is really the probability of an English sentence conditions onan input French sentence which is why it is a conditional language model.Now, if you want to apply this model to actuallytranslate a sentence from French into English,given this input French sentence,the model might tell you what is the probabilityof difference in corresponding English translations.So, x is the French sentence,\"Jane visite l'Afrique en septembre.\"And, this now tells you what is the probability ofdifferent English translations of that French input.And, what you do not want is to sample outputs at random.If you sample words from this distribution,p of y given x, maybe one time you get a pretty good translation,\"Jane is visiting Africa in September.\"But, maybe another time you get a different translation,\"Jane is going to be visiting Africa in September. \"Which sounds a little awkward but is not a terrible translation,just not the best one.And sometimes, just by chance,you get, say, others: \"In September,Jane will visit Africa.\"And maybe, just by chance,sometimes you sample a really bad translation:\"Her African friend welcomed Jane in September.\"So, when you're using this model for machine translation,you're not trying to sample at random from this distribution.Instead, what you would like is to find the English sentence,y, that maximizes that conditional probability.So in developing a machine translation system,one of the things you need to do is come up with an algorithm that can actually findthe value of y that maximizes this term over here.The most common algorithm for doing this is called beam search,and it's something you'll see in the next video.But, before moving on to describe beam search,you might wonder, why not just use greedy search? So, what is greedy search?Well, greedy search is an algorithm from computer science which says to generatethe first word just pick whatever isthe most likely first word according to your conditional language model.Going to your machine translation model and then after having picked the first word,you then pick whatever is the second word that seems most likely,then pick the third word that seems most likely.This algorithm is called greedy search.And, what you would really like is to pick the entire sequence of words, y1, y2,up to yTy, that's there,that maximizes the joint probability of that whole thing.And it turns out that the greedy approach,where you just pick the best first word,and then, after having picked the best first word,try to pick the best second word,and then, after that,try to pick the best third word,that approach doesn't really work.To demonstrate that, let's consider the following two translations.The first one is a better translation,so hopefully, in our machine translation model,it will say that p of y given x is higher for the first sentence.It's just a better, more succinct translation of the French input.The second one is not a bad translation,it's just more verbose,it has more unnecessary words.But, if the algorithm has picked \"Jane is\" as the first two words,because \"going\" is a more common English word,probably the chance of \"Jane is going,\" given the French input,this might actually be higher than the chance of \"Jane isvisiting,\" given the French sentence.So, it's quite possible that if you just pickthe third word based on whatever maximizes the probability of just the first three words,you end up choosing option number two.But, this ultimately ends up resulting in a less optimal sentence,in a less good sentence as measured by this model for p of y givenx. I know this was may be a slightly hand-wavey argument,but, this is an example of a broader phenomenon,where if you want to find the sequence of words, y1, y2,all the way up to the final word that together maximize the probability,it's not always optimal to just pick one word at a time.And, of course, the total number of combinations ofwords in the English sentence is exponentially larger.So, if you have just 10,000 words in a dictionary and if you'recontemplating translations that are up to ten words long,then there are 10000 to the tenth possible sentences that are ten words long.Picking words from the vocabulary size,the dictionary size of 10000 words.So, this is just a huge space of possible sentences,and it's impossible to rate them all,which is why the most common thing to do is use an approximate search out of them.And, what an approximate search algorithm does,is it will try,it won't always succeed,but it will to pick the sentence, y,that maximizes that conditional probability.And, even though it's not guaranteed to find the value of y that maximizes this,it usually does a good enough job.So, to summarize, in this video,you saw how machine translation can be posed as a conditional language modeling problem.But one major difference between this andthe earlier language modeling problems is ratherthan wanting to generate a sentence at random,you may want to try to find the most likely English sentence,most likely English translation.But the set of all English sentences of a certain lengthis too large to exhaustively enumerate.So, we have to resort to a search algorithm.So, with that, let's go onto the next video whereyou'll learn about beam search algorithm.    \n",
    "    \n",
    "    \n",
    "    \n",
    "---\n",
    "## <font color=#0099ff> 3.3 定向搜索 (Beam Search)  \n",
    "    \n",
    "In this video you learn about the beam search algorithm.In the last video you remember how for machine translation, given an inputFrench sentence, you don't want to output a random English translation.You want to output the best,the most likely English translation.The same is also true for speech recognition, where given an inputaudio clip, you don't want to output a random text transcript of that audio.You want to output the best,maybe the most likely text transcript.Beam search is the most widely used algorithm to do this.And in this video, you see how to get beam search to work for yourself.Let's illustrate beam search using our running example of the French sentence[FOREIGN] hopefully, being translated into Jane visits Africa in September.Sorry, let's start this slide over again.Let's illustrate beam search using our running example of Jane visits Africa inSeptember.Let's describe beam search using our running example of the French sentence[FOREIGN] hopefully, being translated into Jane visits Africa in September.The first thing beam search is to do is try to pickthe first words of the English translation that's going to output.So here I've listed all say,10,000 words in the vocabulary.And to simplify the problem a bit, I'm going to ignore capitalizations.So I'm just listing all the words in lowercase.So in the first step of beam search,it'll use this network fragment where the encode is shown in green and the decode isshown in purple to try to evaluate what is the probability of that first word.So what's the probability of the first output y given the inputsentence x given the French input.And whereas, greedy search would pick the single most likely words andmove on, beam search, instead, tries several different alternatives.In particular, it can try three different alternatives.Here, three is a parameter of the algorithm.It's called the beam width parameter.And by setting B to a different values,you can get beam search to simultaneously consider.So whereas, greedy search will pick only the one most likely words andmove on, beam search instead can consider multiple alternatives.So the beam search algorithm has a parameter called B,which is called the beam width.And for this example, I'm going to set the beam width to be equal to 3.And what this means is beam search will consider not just one possibility butconsider three at a time.So in particular,let's say evaluating this probability over different choices the first words.It finds that the choices in jane and september are the most likelythree possibilities for the first word in the English output.Then beam search will store away in computermemory that it wants to try out all three of these words.And if the beam width was five, it would try five words.If the beam width was ten, it would try andkeep track of the ten most likely choices for the first words.Let me say that long sentence again.And if the beam width parameter was set differently, if the beam width parameterwas say ten, then it would keep track of not just three, butup to ten most likely possible choices for the first word.So to be clear,in order to perform this first stepof beam search, what you need to do is run the inputFrench sentence through this encoder network.And then this first step of the decoder network,if this is a softmax output overall 10,000 possibilities,then you would take those 10,000 possible outputs andkeep in memory which were the top three.Let's go into the second step of beam search.Having picked in jane and september as the the three most likelychoices of the first word,what beam search will do now is foreach of these three choices,consider what should be the second word.So after in, maybe the second word is a,or maybe it's aaron.I'm just listing words from the vocab here from the dictionary.Or somewhere down the list will be September.Somewhere down the list is visit.And then all the way to z, maybe the last word is zulu.And there could be an EOS character in there as well.Actually, Danielle, leave off the EOS comment please, yeah.So never mind that, okay.So to evaluate the probability of the second word,it will use this neural network fragment where there's the encoder in green.And for the decoder portion, when trying to decide what comes after in,remember the decoder first outputs y hat 1.So I'm going to set this y hat 1 to the word in and feed this back in.So there's the words in, because they'retrying to figure out that the first word was in, what is the second word?And then this will output, I guess y hat 2.And so by hard wiring y hat 1 here,really the input here, to be the first word in.This network fragment can be used to evaluate what is the probabilityof the second word given the input French sentence andthat the first word of the translation has been the word it.Now, notice that what we ultimately care about in this second step of beam searchis to find the pair of the first and second words that is most likely.So not just the second word that's most likely, but the pair of the first andsecond words that are most likely.And by the rules of conditional probability,this can be expressed as P of the first word timesP of the probability of the second word.Right, which you are getting from this neural network fragment.And so if for each of the three words you've chosen, in, jane, and september,you save away this probability.Then you can multiple them by this second probability to getthe probability of the first and second words.So now,you've seen how if the first word was in,how you can evaluate the probability of the second word.Now, if the first word was jane,you'd do the same thing.The sentence could be jane a,jane aaron, and soon down to jane is,jane visits, and so on.And you will use this neural network fragment.And let me draw this in as well where here you would hard wire y hat 1 to be jane.Shoot, I just realized I've forgotten to draw this arrow earlier.Maybe and use some video editing magic to make this horizontal arrow appear earlierin the video.All right, and so with the first word, y<1> hat,hardwired as jane,then this neural network fragment cantell you what's the probability the second word,given the input x and given that the first word was jane.And then same as above, you can multiplyit with p of y<1> to get the probability of y<1> and y<2> foreach of these 10,000 different possible choices for the second word.And then finally, you do the same thing for september.All the words from a down to zulu and use this network fragment.I'll just draw this in as well,to see if the first word was september,what are the most likely options for the second words.So for this second step of beam search,because we're continuing to use a beam width of three.And because there are 10,000 words in the vocabulary,you'd end up considering 3 times 10,000 or 30,000 possibilities,because there are 10,000 here,10,000 here, 10,000 here.So it's the beam width times the number of words in the vocabulary.[COUGH] And what you do is you evaluate all of these 30,000 options accordingto the probability of the first and second words.And then pick the top three.So really cut down these 30,000 possibilities down to three again,down to the beam width parameter again.And so let's say that these 30,000 choices, the most likely werein september, and say jane is,and jane visits.Sorry, this a bit messy, but those are the most likely three out ofthe 30,000 choices, then that's what beam search would memorize away andtake on to the next step of beam search.So notice one thing.If beam search decides that the three most likely choices for the first andsecond words are in september or jane is or jane visits,then what that means is that it is now rejecting september as a candidate forthe first word of the output English translation.So we're now down to two possibilities for the first word.But we still have a beam width of three keeping trackof three choices for pairs of y<1>, y<2>.So before going on to the third step of beam search,I just want to notice that because the beam width is equal to three,at every step you instantiate three copies of the network toevaluate these partial sentence fragments in the output.And it's because the beam width is equal to three that you have three copies ofthe network with different choices for the first word.But these three copies of the network can be very efficiently used to evaluateall 30,000 options for the second word.So just don't instantiate 30,000 copies of the network.You need only three copies of the network to very quickly evaluate all10,000 possible outputs at that softmax output, say, for y<2>.Let's just quickly illustrate one more step of beam search.Let's just quickly illustrate one more step of beam search.So we said that the most likely choices of first twowords were in september,jane is and jane visits.And for each of these pairs of words we should have savedaway in computer memory the probability of y<1> andy<2> given the input x,given the French sentence x.So similar to before we now want to consider what is the third word.So is it in september a,in september aaron,all the way down to in september zulu.And to evaluate possible choices for the third word,you use this network fragment where youhardwire the first here to be in the second word to be september.And so this network fragment allows you to evaluate what's the probability ofthe third word given the input French sentence x and given that the first twowords are in september in the English output.And then you do the same thing for the second fragment.So like so.And same thing for jane visits.And so beam search will then once again, pick the top three possibilities.Maybe it thinks in september jane is a likely outcome orjane is visiting is likely, or maybe jane visits africa is likely forthe first three words and then it keeps going.And then you go on to the fourth step of beam search to add one more word andon it goes.Let me just do that last piece again.And the outcome of this process hopefully will be that adding one word at a time.That beam search will decide that jane visits africa in september will beterminated by the end of sentence symbol if you're using that in your system,which is quite common.That it'll find that this is a likely output English sentence.And you see more details of this for yourself in this week's exercise as well,where you get to play with beam search yourself.So with a beam width of three, beam search considers three possibilities at a time.Notice that if the beam width was set to be equal to 1, so because there's only 1,then this essentially becomes the greedy search algorithm,which we had discussed in the last video.Then this essentially becomes the greedy search algorithmwhich we had discussed in the last video.But by considering multiple possibilities, say three or ten orsome other number at the same time,beam search will usually find a much better output sentence than greedy search.You've now seen how beam search works, but it turns out there's some additional tipsand tricks and refinements that help you to make beam search work even better.Let's go into the next video to take a look.    \n",
    "    \n",
    "    \n",
    "    \n",
    "---\n",
    "## <font color=#0099ff> 3.4 改进定向搜索 (Refinements to Beam Search) \n",
    "    \n",
    "WEBVTTIn the last video,you saw the basic beam search algorithm.In this video, you learned some little changes that'll make it work even better.Length normalization is a small change to the beam search algorithmthat can help you get much better results.Here's what it is.We talked about beam search as maximizing this probability.And this product here is just expressing the observation that P ofy1 up to y Ty given x canbe expressed as P of y1 given x,times P ofy2 given x and y1 times dot dot dot,up to I guess P of y, Ty given x, andy1 up to y Ty minus one.But then maybe this notation is a bit more scary, andmore intimidating than it needs to be.But this is at product of probabilities that you see previously.Now, rather than,now if you're implementing these,these probabilities are all numbers less than 1.In fact, often they are much less than 1.And multiplying a lot of numbers less than 1 will result in a tiny, tiny,tiny number,which can result in numerical underflow.Meaning that is too small forthe probability part representation in your computer to store accurately.So in practice, instead of maximizing this product, we will take logs.And if you insert a log there, then a log of a product becomes a sum of a log andmaximizing this sum of log probabilities should give you the sameresult in terms of selecting the most likely sentence y.So by taking logs,you end up with a more numerically stablealgorithm that is less prone to rounding errors,numerical rounding errors, or to really numerical underflow.And because the log function,that's the log grouping function,there's a strictly monotonically increasing function, maximizing P(y).And because the logarithmic function,here's a long function,is a strictly monotonically increasing function, we know that maximizinglog P(y) given x should give you the same result as maximizing P(y) given x,as in the same value of y that maximizes this should also maximize that.So in most implementations, you keep track of the,sum of logs of probabilities, rather than the product of the probabilities.Now, there is one other tweak to this.All right, there's another typo here.I hope you can do video editing magic.So we can look good from the start.Now there's one other change to this objective functionthat makes the machine translation algorithm work even better.Which is that if you refer to this original objective up here,if you have a very long sentence, the probability of that sentence is going tobe low because you're multiplying as many terms here plus a numbers andless than one to estimate the probability of that sentence.And so if you multiply a lot of numbers that are less than one together,you just tend to end up with a smaller probability.And so this objective function has an undesirable effect,that it maybe unnaturally tends to prefer very short translations,tends to prefer very short outputs.Because the probability of a short sentence is determined just bymultiplying fewer of these numbers less than one.And so the product will just be not quite as small.And by the way, the same thing is true for this.The log of a probability is always less than or equal to 1.You're actually in this major the lock, sothe more terms you had together, the more negative this thing becomes.So there's one other change the algorithm that makes it work better,which is instead of using this as the objective you're trying to maximize.One thing you can do is normalize this by the number of words in your translation.And so this takes the average of the log of the probability of each word andthis significantly reduces the penalty for outputting longer translations.And in practice, as a heuristic,instead of dividing by Ty, by the number of words in the output sentence.Sometimes you use the softer approach,where you have Ty to the power of alpha,where maybe alpha is equal to 0.7.So if alpha was equal to 1, then you're completely normalizing by length.If alpha was equal to 0 then well,Ty to the 0 would be 1,then you're just not normalizing at all.And this is somewhere in between full normalization and no normalization.And alpha is another parameter,alpha parameter of algorithm that you can tune to try to get the best of results.And half of it using alpha this way,this is a heuristic or this is a hack,there isn't a great theoretical justification for it butpeople have found this works well.People have found it works well in practice.So many groups will do this.And you can try out different values of alpha until, andsee which one gives you the best result.So just to wrap up how you run beam search.As you run beam search, you see a lot of sentences with length equal 1,a lot of sentences with length equal 2,a lot of sentences with lengthequals 3 and so on and maybe,you run beam search for 30 steps.You consider output sentences up to length 30, let's say.And so with beam width of 3,you'll be keeping track of the top threepossibilities for each of these possible sentence lengths.1, 2, 3, 4, and so on up to 30.Then you will look at all the, Outputsentences and score them against this score.If you're using an EOS,you could also artificially tag onthe end of sentence token to your best three choices.And so,you can take your top sentences andjust compute this objective function on the sentences that you have seenthrough the beam search process.And then finally, of all three sentences that you have added this way,you will pick the one that achieves the highest valueon this normalized log probability objective.Sometimes it's called a normalized likelihood objective.And then that will be the final translation you output.So that's how you implement beam search andyou get to play at this yourself in this week's program exercise.Finally, a few implementational details.How do you choose the beam width B?The larger B is, the more possibilities you're considering, andthus the better the sentence you probably find.But the larger B is, the more computationally expensive your algorithmis because you're also keeping a lot more possibilities around, right?So finally,let's just wrap up with some thoughts on how to choose the beam width B.So here are the pros and cons of setting B to be very large versus very small.If the B width is very large,then you consider a lot of possibilities.And so you tend to get a better resultbecause you're considering a lot of different options, but it will be slower.And the minimum requirements will also grow,it'll also be computationally slower.Whereas if you use a very small beam width, then you get a worse result.Because you're just keeping less possibilities in mindas the algorithm is running,but you get a result faster.And the memory requirements will also be lower.So in the previous video we use,in our running example,a beam width of three so we're keeping three possibilities in mind.In practice, that is on the small side.In production systems, it's not uncommon to see a beam width maybe around ten.And I think a beam width of 100 would be considered very large fora production system,depending on the application.But for research systems, where people want to squeeze out every last drop ofperformance in order to publish a paper the best possible result.It's not uncommon to see people use beam widths of 1,000 or 3,000.But this is very application,as well as domain dependent.So I would say, try out a variety of values of B and see what works foryour application.But when B gets very large,there is often diminishing returns.So for many applications, occasions,I will expect to see a huge gain as you gofrom a beam width of 1, which is basically a beam search, to 3, to maybe 10.But the gains as you go from 1,000 to 3,000 in beam width are might notbe as big and forthose of you that have taken maybe a lot of computer science courses before.If you're familiar with computer science search algorithms like BFS,Breadth-first search, or DFS, Depth-first search.The way to think about beam search is that unlike those other algorithms,which you might have learned about in a computer science algorithms course anddon't worry about it if you've not heard of these algorithms.But if you've heard of Breadth-first search and Depth-first search,then unlike those algorithms which are exact search algorithms,beam search runs much faster but is not guaranteed to find the exact maximum forthis arg max that you'd like to find.If you haven't heard of Breadth-first search or Depth-first search,don't worry about it, it's not important for our purposes.But if you have, this is how beam search release those algorithms.So that's it for beam search which is a widely used algorithm inmany production system or in many commercial systems.Now in the third course in the sequence of course of deep learning,we talk a lot about error analysis.It turns out, one of the most useful tools I found is to be able to doerror analysis on beam search.So you sometimes wonder,should I increase my beam width?Is my beam width working well enough?And there's some simple things we can compute to give youguidance on whether you need to work on improving your search algorithm.Let's talk about that in the next video.    \n",
    "    \n",
    "---\n",
    "## <font color=#0099ff> 3.5 定向搜索的误差分析 (Error analysis in beam search) \n",
    "\n",
    "In the third course of this sequence of five courses, you saw how error analysiscan help you focus your time on doing the most useful work for your project.Now, beam search is an approximate search algorithm,also called a heuristic search algorithm.And so it doesn't always output the most likely sentence.It's only keeping track of B equals 3 or 10 or 100 top possibilities.So what if beam search makes a mistake?In this video, you'll learn how error analysis interacts with beam search andhow you can figure out whether it is the beam search algorithm that's causingproblems and worth spending time on.Or whether it might be your RNN model that is causing problems andworth spending time on.Let's take a look at how to do error analysis with beam search.Let's use this example of Jane visite l'Afrique en septembre.So let's say that in your machine translation dev set,your development set,the human provided this translation andJane visits Africa in September,and I'm going to call this y*.So it is a pretty good translation written by a human.Then let's say that when you run beam search on your learned RNN model andyour learned translation model,it ends up with this translation,which we will call y-hat, Jane visited Africa last September,which is a much worse translation of the French sentence.It actually changes the meaning,so it's not a good translation.Now, your model has two main components.There is a neural network model,the sequence to sequence model.We shall just call this your RNN model.It's really an encoder and a decoder.And you have your beam search algorithm,which you're running with some beam width b.And wouldn't it be nice if you could attribute this error,this not very good translation,to one of these two components?Was it the RNN or really the neural network that is more to blame, oris it the beam search algorithm,that is more to blame?And what you saw in the third course of the sequence is thatit's always tempting to collect more training data that never hurts.So in similar way, it's always tempting to increase the beam width that neverhurts or pretty much never hurts.But just as getting more training data by itself might notget you to the level of performance you want.In the same way,increasing the beam width by itself might not get you to where you want to go.But how do you decide whether ornot improving the search algorithm is a good use of your time?So just how you can break the problem down andfigure out what's actually a good use of your time.Now, the RNN, the neural network,what was called RNN really means the encoder and the decoder.It computes P(y given x).So for example, for a sentence, Jane visits Africain September,you plug in Jane visits Africa.Again, I'm ignoring upper versus lowercase now, right, and so on.And this computes P(y given x).So it turns out that the most useful thing foryou to do at this point is to compute using this model to computeP(y* given x) as well as to compute P(y-hat given x) using your RNN model.And then to see which of these two is bigger.So it's possible that the left side is bigger than the right hand side.It's also possible that P(y*) is less than P(y-hat) actually, or less than orequal to, right?Depending on which of these two cases hold true, you'd be able to moreclearly ascribe this particular error,this particular bad translationto one of the RNN or the beam search algorithm being had greater fault.So let's take out the logic behind this.Here are the two sentences from the previous slide.And remember,we're going to compute P(y* given x) andP(y-hat given x) and see which of these two is bigger.So there are going to be two cases.In case 1,P(y* given x) as output by the RNNmodel is greater than P(y-hat given x).What does this mean?Well, the beam search algorithm chose y-hat, right?The way you got y-hat was you had an RNN that was computing P(y given x).And beam search's job was to try to find a value of y that gives that arg max.But in this case,y* actually attains a higher value forP(y given x) than the y-hat.So what this allows you to conclude is beam search is failing to actually giveyou the value of y that maximizes P(y given x) because the onejob that beam search had was to find the value of y that makes this really big.But it chose y-hat,the y* actually gets a much bigger value.So in this case, you could conclude that beam search is at fault.Now, how about the other case?In case 2, P(y* given x) is less than orequal to P(y-hat given x), right?And then either this or this has gotta be true.So either case 1 or case 2 has to hold true.What do you conclude under case 2?Well, in our example,y* is a better translation than y-hat.But according to the RNN,P(y*) is less than P(y-hat),so saying that y* is a less likely output than y-hat.So in this case, it seems that the RNN model isat fault and it might be worth spending more time working on the RNN.There's some subtleties here pertaining tolength normalizations that I'm glossing over.There's some subtleties pertaining to length normalizations that I'mglossing over.And if you are using some sort of length normalization,instead of evaluating these probabilities,you should be evaluating the optimizationobjective that takes into account length normalization.But ignoring that complication for now,in this case, what this tells you is thateven though y* is a better translation,the RNN ascribed y* in lower probability than the inferior translation.So in this case,I will say the RNN model is at fault.So the error analysis process looks as follows.You go through the development set andfind the mistakes that the algorithm made in the development set.And so in this example, let's say that P(y* given x) was 2 x 10 to the -10,whereas, P(y-hat given x) was 1 x 10 to the -10.Using the logic from the previous slide,in this case, we see thatbeam search actually chose y-hat,which has a lower probability than y*.So I will say beam search is at fault.So I'll abbreviate that B.And then you go through a second mistake orsecond bad output by the algorithm,look at these probabilities.And maybe for the second example,you think the model is at fault.I'm going to abbreviate the RNN model with R.And you go through more examples.And sometimes the beam search is at fault,sometimes the model is at fault,and so on.And through this process, you can then carry out error analysis to figure outwhat fraction of errors are due to beam search versus the RNN model.And with an error analysis process like this, for every example in your dev sets,where the algorithm gives a much worse output than the human translation,you can try to ascribe the error to either the search algorithm orto the objective function, or to the RNN model that generatesthe objective function that beam search is supposed to be maximizing.And through this, you can try to figure out which of these two components isresponsible for more errors.And only if you find that beam search is responsible for a lot of errors,then maybe is we're working hard to increase the beam width.Whereas in contrast, if you find that the RNN model is at fault,then you could do a deeper layer of analysis to try to figure out if you wantto add regularization, or get more training data, ortry a different network architecture,or something else.And so a lot of the techniques that you saw in the third course inthe sequence will be applicable there.So that's it for error analysis using beam search.I found this particular error analysis process very useful whenever you havean approximate optimization algorithm,such as beam searchthat is working to optimize some sort of objective, some sort of cost functionthat is output by a learning algorithm,such as a sequence-to-sequence model ora sequence-to-sequence RNN that we've been discussing in these lectures.So with that, I hope that you'll be more efficient at making these types of modelswork well for your applications.\n",
    "\n",
    "    \n",
    "---\n",
    "## <font color=#0099ff> 3.6 Bleu 得分 (Bleu Score) \n",
    " \n",
    " \n",
    "One of the challenges of machine translation is that given a Frenchsentence, there could be multiple English translations that are equally goodtranslations to that French sentence.So how do you evaluate a machine translation system if there are multipleequally good answers,unlike say image recognition,where there's one right answer.You just measure accuracy.If there are multiple great answers,how do you measure accuracy?The way this is done conventionally is with something called the Bleu score.So in this optional video, I want to share with you,I want to give you a sense of how the Bleu score works.Let's say you are given this French sentence,le chat est sur la tapis,and you are given a reference,human generated translation of this,which is the cat is on the mat, butthere are multiple pretty good translations of this, so different human,different person might translate it as there is a cat on the mat.And both of these are actually just fine,perfectly fine translations of the French sentence.What the Bleu score does is,given a machine generated translation,it allows you to automatic compute a score then measures how goodis that machine translation.And the intuition is, so long as the machine generated translationis pretty close to any of the references provided by humans,then it would get a high Bleu score.Bleu, by the way, stands for bilingual evaluation,Understudy.So in the theater world,an understudy is someone that learns the role of a more senior actor sothey can take over the role of the more senior actor if necessary.And motivation for Bleu is that,whereas you could ask human evaluators to evaluate the machine translation system,the Bleu score is an understudy, it could be a substitute forhaving humans evaluate every output of a machine translation system.So the Bleu score was due to Kishore Papineni,Salim Roukos, Todd Ward, and Wei-Jing Zhu.This paper has been incredibly influential and is actuallyquite a readable paper so I encourage you to take a look if you have time.So the intuition behind the Bleu score is,we're going to look at the machine generated output and see if the types ofwords it generates appear in at least one of the human generated references.And so these human generated references would be providedas part of the depvset or as part of the test set.Now, let's look at some of the extreme examples.Let's say that the machine translation system abbreviatingmachine translation is MT, so the machine translation of the MT outputs is the, the,the, the, the, the, the, so this is clearly a pretty terrible translation.So one way to measure how good the machine translation output is,is to look at each of the words in the output andsee if it appears in the references.And so this we call the precision of the machine translation output.And in this case there are seven words in the machine translation output.And every one of these seven words appears in either Reference 1 or Reference 2.Right, so the word the appears in both references so each of these words lookslike a pretty good word to include, so this will have a precision of 7 over 7.It looks like it's a great precision.So this is why the basic precision measure of what fractionof the words in the MT output also appear in the references.This is not a particularly useful measure because it seems to imply that thisMT output has very high precision.So instead, what we're going to use is a modified precision measure,in which we would give each word credit only up tothe maximum number of times it appears in the reference sentences.So in Reference one, the word the appears twice, in reference two,the word the appears just once.So two is bigger than one, and so we're going to say that the word the gets creditup to twice, so with a modified precision,we will say that it gets a score of two out of seven,because out of seven words will give it a two credits for appearing.So here the denominator is the count of the number of times the wordthe appears,there are seven words in total.And the numerator is the count of the number of times the word the appears,but we clip this count, we take a max, we clip this count at two.So this gives us the modified precision measure.Now, so far we've been looking at words in isolation.In the Bleu score, you don't want to just look at isolated words,you maybe want to look at pairs of words as well.And solet's define a portion of the Bleu score on bigrams, andbigrams just means pairs of words appearing next to each other.Continuing our example from before,let's now use a slightly more sophisticated machine translation output.Let's say this time the MT machine translation, sonow let's see how we could use bigrams to define the Bleu score,and this would just be a portion of the final Bleu score.And we'll take unigrams or single words as well as bigrams,which means pairs of words, into account as well as maybe even longersequences worth such as trigrams which means three words paired together.So, Let's continue our example from before.We have the same Reference 1 and Reference 2, but now let's say the machinetranslation or the MT system has a slightly better output.The cat the cat on the mat.Still not a great translation, but maybe better than the last one.So here the possible bigrams are,well, there's the cat,we'll ignore case, and then there's cat the, that's another bigram.And then there's the cat again, but we've already had that so let's skip that.And then cat on is the next one.And then on the and the mat.So these are the bigrams in the machine translation output.And so let's count up how many times each of these bigrams appear.The cat appears twice, cat the appears once, and the others all appear just once.And then finally,let's define the clipped count.So, count and then subscript clip.And to define that,let's take this column of numbers butgive our algorithm credit only up to the maximum number of times that that bigramappears in either Reference one or Reference two.So the cat appears a maximum of once in either of the references,so we'll clip that count to one.Cat the, well, doesn't appear in Reference 1 or reference 2 so clip that to zero.Cat on, yeah, that appears once,we give it credit for once.On the appears once, we give it credit for once and the mat appears once.So these are the clipped counts,we're taking all the counts andclipping them, reducing them to be no more than the number of times thatbigram appears in at least one of the references.And then finally,our modified bigram precision will be the sum of the count clips.So that's one, two, three, four,divided by the total number of bigrams.That's two, three, four, five, six.So four out of six or two thirds is the modified precision on bigrams.So let's just formalize this a little bit further.With what we had developed with on unigrams we defined thismodified precision computed on unigrams as p substrate 1, sop stands for precision and the substrate 1 here means we're referring to unigrams.But that is defined as sum over the unigrams sothat just means sum over the words that appear in the machine translation output.So this is called y hat ofcount clip of that unigram.Divided by sum of raw unigrams inthe machine translation output of numberof counts of that unigram, right.And so this is what we had gotten as I guess,as 2 out of 7, two slides back.So, the one here refers to unigram,meaning we're looking at single words in isolation.You can also define p n as the n-gram version.Instead of a unigram for n-gram.So this would be sum over the n-gram inthe machine translation output of countclip of that n-gram divided by sum overn-grams of the count of that n-gram.And so these precisions, or these modifiedprecision scores, measured on unigrams, oron bigrams,which we did on a previous slide,or on trigrams,which are triples of words,or even higher values of n for other n-grams.This allows you to measure the degree to whichthe machine translation output is similar ormaybe overlaps with the references.And one thing that you could probably convince yourself of is,if the MT of output is exactly the same as either Reference 1 or Reference 2,then all of these values, P1, and P2 and so on, they'll all be equal to 1.0.So to get a precision or a modified precision of 1.0,you just have to be exactly equal to one of the references.And sometimes it's possible to achieve this even if you aren'texactly the same as any other references but kind of combined themin a way that hopefully still results in a good translation.Finally, let's put this together to form the final Bleu score.P subscript n is the Bleu score computed on n-grams only,also the modified precision,computed on n-grams onlyBy convention, Andby convention, to compute one number,you compute p1, p2,p3 and p4 and combine them together using the following formula.It's going to be the average, so sum from n equals 1 to 4 of pn anddivide that by 4, so basically taking the average.By convention the Bleu school is defined as E to the this.And exponentiation is a strictly monotonically increasingoperation and then we actually adjust this with one more factor calledthe BP penalty.So BP Stands forbrevity penalty.The details maybe aren't super important,but just give you a sense,it turns out that if you output very short translations,it's easier to get high precision because probably most of the words yououtput appear in your references.So, but we don't want translations that are very short.So the BP, or the brevity penalty,is an adjustment factor that penalizestranslation systems that output translations that are too short.So the formula for the brevity penalty is the following.It's equal to one if your machine translation system actually outputsthings that are longer than the human generatedreference outputs and otherwise,it's some formula like that that overallpenalizes shorter translations.So in the details,you can find in this paper.So once again,earlier in this set of clauses,you saw the importance of having a single row number evaluation metric becauseit allows you to try out two ideas, see which one achieves the higher score andthen try to stick with the one that have achieves the highest score.So the reason the Blue score was revolutionary formachine translation was because this gave a pretty good,by no means perfect, but pretty good single real number evaluation matrix andso that accelerated the progress of the entire field of machine translation.I hope this video gave you a sense of how the Bleu score works.In practice, few people would implement the Bleu score from scratch.There are open source implementations you can download andjust use to evaluate your own system.But today, BLEU score is used to evaluate many systems that generate text,such as machine translation systems as well as the example I showed brieflyearlier of image captioning systems,where you would have a system,have a neural network generated image caption and then use the Bleu score to seehow much that overlaps with maybe a reference caption ormultiple reference captions that were generated by people.So the Bleu score is a useful, single,real number evaluation measure to use whenever you want your algorithm togenerate a piece of text and you want to see whether it has similar meaningas a reference piece of text generated by humans.This is not used forspeech recognition because in speech recognition there's usually, you have oneground you just use other measures to see if you got the speech transcription onpretty much exactly word for word correct but for things like image captioning andmultiple captions for a picture it could be about equally good orfor machine translation there are multiple translations about equally good.The Bleu score gives you a way to evaluate that automatically andtherefore speed up your algorithm development.So with that, I hope you have a sense of how the Bleu score works. \n",
    "    \n",
    "    \n",
    "---\n",
    "## <font color=#0099ff> 3.7 注意力模型直观理解 (Attention Model Intuition) \n",
    "\n",
    "For most of this week,you've been working with the sequence to sequence model,where an encoder RNN reads in a sentence and then a decoder RNN outputsa new sentence, maybe a translation.One of the most significant ideas recently in deep learning has beenthe idea of attention models which makes this encoder-decoder.For most of this week,you've been using a encoder-decoder architecture for machine translation,where one RNN reads in a sentence and then different one outputs a sentence.There's a modification to this calledthe attention model that makes all this work much better.The attention algorithm, the attention ideahas been one of the most influential ideas in deep learning.Let's take a look at how that works.A huge weakness ofthis basic encoder-decoder architecture is that it's not great at long sentences.So, given very long french sentence like this,what we are asking this green encoder neural network to do is to read inthe whole sentence and then memorize the whole sentences and storeit in the activations conveyed here.And then for the purple that wrote the decoder network tothen generate the English translation.Jane went to Africa last September and enjoyed the culture and met many wonderful people,she came back raving about how wonderful her trip was and is tempting me to go too.Now, the way a human translatorwould translate the sentence is not to first readthe whole phrase sentence and then memorize the whole thingand then regurgitate an English sentence from scratch.Instead, what a human translator would do is read the first part of it,maybe generate part of the translation,you'll look at the second part,generate a few more words,look at a few more words, generate a few more words and so on.You kind of work part by part through this sentence because it's justreally difficult to memorize the whole long sentence like that.And so, what you see forthe encoder-decoder architecture above is that it works quite well for short sentences.So it might achieve a relatively high bleu score.But for very long sentences,maybe longer than 30 or 40 words now the performance comes down.So the blue score might look like this asthe sentence length varies and short sentences are just hard to translate,hard to get all the words right and long sentences,it doesn't do well on because it's just difficult to getthe neural network to memorize a super long sentence.So in this and the next video,you'll see the attention model,which translates maybe a bit more like humans might looking atpart of a sentence at a time and with the attention model,machine translation systems performance can look likethis because by working one part of the sentence at a time,you don't see this huge dip which isreally measuring the ability of a neural network to memorizea long sentence which maybe isn't what we most badly need a neural network to do.So in this video,I want to just give you some intuitionabout how attention works and then we'll fetch all the details in the next video.The attention model was due to Dzmitry Bahdanau,Kyunghyun Cho, and Yoshua Bengio andeven though it was obviously developed for machine translation,it spread to many other application areas as well.And this is really a very influential anda very seminal paper in the deep learning literature.So let's illustrate this with a short sentence.Even though these ideas were maybe developed more forlong sentences but it will be easier to illustrate his ideas through a simple example.We have a usual sentence,Jane visite L'Afrique en septembre and let's say that we use a RNN and in this case,I'm going to use a bi-directional RNN in order to compute some set offeatures for each of the input words andhere I've drawn a standard bi-directional RNN with outputs y1,y2, y3, and so on up to y5 but we're not doing a word for word translation.So, let me get rid of the ys on top.By using a bi-directional RNN,what we've done is for each of the words,really for each of the five positions into a sentence,you can compute a very rich set of featuresabout the words in the sentence and maybe surrounding words in every position.Now, let's go ahead and generate the English translation.We're going to use another RNN to generate the English translations.So, here is my RNN node as usual and instead of using \"a\" to denote the activation,in order to avoid confusion with the activations down here,I'm just going to use a different notation.I'm going to use \"s\" to denote the hidden state in this RNN up here.So instead of writing a1,I'm going to write s1.And so, we hope in this model that the first word it generates will be Jane, right?To generate Jane visits Africa in September.Now, the question is when you're trying to generate this first word,this output, what parts of the input French sentence she should be looking at?Seems like you should be looking primarily at this first word.Maybe a few other words close by butyou don't need to be looking way at the end of the sentence.So what the attention model would be computing is a set ofattention waits and we're going to usealpha 1,1 to denote when you're generating the first words,how much should you be paying attention tothis first piece of information here and then we'll also come upwith a second that's called the attention waitalpha 1,2 which tells us what we're trying to compute the first word hopefully Jane,how much attention should we pay tothis second word from the inputs and so on and the alpha 1,3 and so on.And together, this will tell us what is exactlythe context which want to know the C that we should be payingattention to and that is input to this RNN unit to then try to generate the first words.So that's one step of the RNN.I will flesh on all these details in the next video.For the second step of this RNN,we're going to havea new hidden state s2 and we're going to have a new set of attention waits.So we're going to have alpha 2,1 to tell us where we're generating the second word.I guess this would be visits maybe [inaudible] label.How much should we paying attention to the first word in the French input andalso alpha 2,2 and so on?How much should we be paying attention to the word visits?How much should we pay attention to L'Afrique? And so on.And of course, the first we generated Jane is also an inputto this and there were some contexts that we're paying attention to.The second step, there's also an input and that together,would generate the second word,and that leads us to the third step s3,where this is an input and we have some new contextC that depends on the various alpha 3 for the different time steps.It tells us how much should we be paying attention to the different words fromthe input French sentence and so on.So, some things I haven't specified yet,but that won't go further into detail in the next videois how exactly does context definedand the goal of the context is for the third word is reallyshould capture that maybe we should be looking around this part of the sentence.And the formula you use to do that will deferto next video as well as how do you compute these attention waits.And you'll see in the next video that alpha 3,t which is when you try to generate the third word,I guess this be Africa,just getting the right output,the amounts that this RNN step should be paying attention to the French word at time t,that depends on the activations of the bi-directional RNN at time t. So,I guess it will depend on the forward activations and backward activationsat time t and it will depend on the state from the previous steps,so it will depend on s2 and these things together will influencehow much you pay attention to a specific word in the input French sentence.But we will flesh out all these details in the next video.But the key intuition to take away is that this way the RNN matchesforward generating one word at a time until eventually it generates maybe the EOS,and at every step,there are these attention waits alpha t,t prime that tells it when you're trying to generate the English word,how much should you be paying attention to the t prime French word.And this allows it on every timestep to look only maybe within a local window ofthe French sentence to pay attention to when generating a specific English word.So I hope this video conveys some intuition aboutthe attention model that you know I have a rough sense of maybe how the algorithm works.Let's go on to the next video to flesh all the details of the attention model.\n",
    "\n",
    "    \n",
    "    \n",
    "---\n",
    "## <font color=#0099ff> 3.8 注意力模型 (Attention Model)  \n",
    "\n",
    "\n",
    "In the last video, you saw how the attention model allowsa neural network to pay attention toonly part of an input sentence while it's generating a translation,much like a human translator might.Let's now formalize that intuition intothe exact details of how you would implement an attention model.So same as in the previous video,let's assume you have an input sentence and you use a bidirectional RNN,or bidirectional GRU, or bidirectional LSTM to compute features on every word.In practice, GRUs and LSTMs are often used for this,with maybe LSTMs be more common.And so for the forward occurrence,you have a forward occurrence first time step.Activation backward occurrence, first time step.Activation forward occurrence, second time step.Activation backward and so on.For all of them in just a forward fifth time step a backwards fifth time step.We had a zero here technically we can alsohave I guess a backwards sixth as a factor of all zero,actually that's a factor of all zeroes.And then to simplify the notation going forwards at every time step,even though you have the features computed fromthe forward occurrence and from the backward occurrence in the bidirectional RNN.I'm just going to use a of t to represent both of these concatenated together.So a of t is going to be a feature vector fortime step t. Although to be consistent with notation,we're using second, I'm going to call this t_prime.Actually, I'm going to use t_prime to index into the words in the French sentence.Next, we have our forward only,so it's a single direction RNN with state s to generate the translation.And so the first time step,it should generate y1 and just will have as input some contextC. And if you want to index it with time I guess youcould write a C1 but sometimes I just right C without the superscript one.And this will depend on the attention parameters so alpha_11,alpha_12 and so on tells us how much attention.And so these alpha parameters tells us how much the context would dependon the features we're getting or the activations we'regetting from the different time steps.And so the way we define the context is actually be a way to some ofthe features from the different time steps waited by these attention waits.So more formally the attention waits will satisfy this that they are all be non-negative,so it will be a zero positive and they'll sum to one.We'll see later how to make sure this is true.And we will have the context or the context at time oneoften drop that superscript that's going to be sum over t_prime,all the values of t_prime of this waitedsum of these activations.So this term here are the attention waits and this term here comes from here.So alpha(t_prime) is the amount of attention that'syt should pay to a of t_prime.So in other words,when you're generating the t of the output words,how much you should be paying attention to the t_primeth input to word.So that's one step of generating the output and then at the next time step,you generate the second output and is again done some ofwhere now you have a new set of attention waits on they to find a new way to sum.That generates a new context.This is also input and that allows you to generate the second word.Only now just this way to sum becomes the context ofthe second time step is sum over t_prime alpha(2, t_prime).So using these context vectors.C1 right there back,C2, and so on.This network up here looks like a pretty standard RNN sequencewith the context vectors as output and wecan just generate the translation one word at a time.We have also define how to compute the context vectors in terms ofthese attention ways and those features of the input sentence.So the only remaining thing to do is todefine how to actually compute these attention waits.Let's do that on the next slide.So just to recap, alpha(t,t_prime) is the amount of attention you should paid toa(t_prime ) when you're trying to generate the t th words in the output translation.So let me just write down the formula and we talk of how this works.This is formula you could use the compute alpha(t,t_prime) which is going to compute these terms e(t,t_prime) and then use essentially a soft pass to make sure thatthese waits sum to one if you sum over t_prime.So for every fix value of t,these things sum to one if you're summing over t_prime.And using this soft max prioritization,just ensures this properly sums to one.Now how do we compute these factors e. Well,one way to do so is to use a small neural network as follows.So s t minus one was the neural network state from the previous time step.So here is the network we have.If you're trying to generate yt then st minus one was the hidden state fromthe previous step that just fell into stand that's one input to very small neural network.Usually, one hidden layer in neural network because you need to compute these a lot.And then a(t_prime) the features from time step t_prime is the other inputs.And the intuition is,if you want to decide how much attention to pay to the activation of t_prime.Well, the things that seems like it should depend the most onis what is your own hidden state activation from the previous time step.You don't have the current state activation yetbecause of context feeds into this so you haven't computed that.But look at whatever you're hidden stages of this RNN generatingthe upper translation and then for each of the positions,each of the words look at their features.So it seems pretty natural that alpha(t,t_prime) and e(t, t_prime) should depend on these two quantities.But we don't know what the function is.So one thing you could do is just train a very small neural networkto learn whatever this function should be.And trust that obligation trust wait and descent to learn the right function.And it turns out that if you implementedthis whole model and train it with gradient descent,the whole thing actually works.This little neural network does a pretty decent job tellingyou how much attention yt should pay toa(t_prime) and this formula makes sure thatthe attention waits sum to one and then as you chug along generating one word at a time,this neural network actually pays attention to the right parts ofthe input sentence that learns all this automatically using gradient descent.Now, one downside to this algorithm is thatit does take quadratic time or quadratic cost to run this algorithm.If you have tx words in the input and ty words inthe output then the total number ofthese attention parameters are going to be tx times ty.And so this algorithm runs in quadratic cost.Although in machine translation applications whereneither input nor output sentences isusually that long maybe quadratic cost is actually acceptable.Although, there is some research work on trying to reduce costs as well.Now, so far up in describing the attention idea in the context of machine translation.Without going too much into detail this idea has been applied to other problems as well.So just image captioning.So in the image capturing problem the task is tolook at the picture and write a caption for that picture.So in this paper set to the bottom by Kevin Chu,Jimmy Barr, Ryan Kiros, Kelvin Shaw, Aaron Korver,Russell Zarkutnov, Virta Zemo,and Andrew Benjo they also showed that you could have a very similar architecture.Look at the picture and pay attention only to partsof the picture at a time while you're writing a caption for a picture.So if you're interested, then I encourage you to take a look at that paper as well.And you get to play with all this and more in the programming exercise.Whereas machine translation is a very complicated problem in the prior exercise youget to implement and play of the attention while youyourself for the date normalization problem.So the problem inputting a date like this.This actually has a date of the Apollo Moon landing and normalizing it intostandard formats or a date like this and having a neural network a sequence,sequence model normalize it to this format.This by the way is the birthday of William Shakespeare.Also it's believed to be.And what you see in prior exercises as you can traina neural network to input dates in any ofthese formats and have it use an attention modelto generate a normalized format for these dates.One other thing that sometimes fun to do isto look at the visualizations of the attention waits.So here's a machine translation example and here were plotted in different colors.the magnitude of the different attention waits.I don't want to spend too much time on this but you find thatthe corresponding input and output wordsyou find that the attention waits will tend to be high.Thus, suggesting that when it's generating a specific word in output is,usually paying attention to the correct words in the input and all this includinglearning where to pay attention when was alllearned using propagation with an attention model.So that's it for the attention modelreally one of the most powerful ideas in deep learning.I hope you enjoy implementing and playing withthese ideas yourself later in this week's programming exercises.\n",
    "\n",
    "\n",
    "---\n",
    "## <font color=#0099ff> 3.9 语音辨识 (Speech recognition)\n",
    "    \n",
    "---\n",
    "## <font color=#0099ff> 3.10 触发字检测 (Trigger word detection)\n",
    "    \n",
    "    \n",
    "    \n",
    "---\n",
    "## <font color=#0099ff> 3.11 结论和致谢 (Summary and thank you)  \n",
    "    \n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "---\n",
    " \n",
    "**PS: 欢迎扫码关注公众号：「SelfImprovementLab」！专注「深度学习」，「机器学习」，「人工智能」。以及 「早起」，「阅读」，「运动」，「英语 」「其他」不定期建群 打卡互助活动。**\n",
    "\n",
    "<center><img src=\"http://upload-images.jianshu.io/upload_images/1157146-cab5ba89dfeeec4b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
