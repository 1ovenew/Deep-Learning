{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursera | Andrew Ng (05-week2)—自然语言处理与词嵌入\n",
    "\n",
    "【第 5 部分-序列模型-第 2 周】在吴恩达深度学习视频基础上，笔记总结，添加个人理解，如有理解描述错误，请多加批评指教。- ZJ\n",
    "    \n",
    ">[Coursera 课程](https://www.coursera.org/specializations/deep-learning) |[deeplearning.ai](https://www.deeplearning.ai/) |[网易云课堂](https://mooc.study.163.com/smartSpec/detail/1001319001.htm)\n",
    "\n",
    "\n",
    "[CSDN]()：\n",
    "   \n",
    "\n",
    "---\n",
    "\n",
    "# 自然语言处理与词嵌入 （NLP and Word Embeddings）\n",
    "\n",
    "\n",
    "## <font color=#0099ff> 2.1 词汇表征 （Word representation）\n",
    "\n",
    "上周的学习了 RNNs GRUs and LSTMs，这一部分，我们学习将那些运用在 **自然语言处理中 （NLP）**。\n",
    "\n",
    "深度学习已经对这一领域带来了革命性的变革。其中很关键的一个概念是 **词嵌入 （Word Embeddings）**,是语言表示的一种方法，可以让算法自动理解一些类似的词，比如 男人对女人 （man is to woman）,国王对皇后（King is to queen）等类似的例子。\n",
    "\n",
    "通过词嵌入的概念，就可以构建 NLP 应用了。即使你的模型中，标记的训练值较小。这周课的最后，我们会学习如何消除词嵌入的偏差，就是去除不想要的特性。或者学习算法有时候会学到的其他类型的偏差。\n",
    "\n",
    "**单词的表示：**\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180302135029652?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "- 我们一直使用词汇表（Vocabulary）来表示词，如 | V |=10000 .\n",
    "- One-hot Vector ,如上图所示，使用 $O_{5391}$ 表示 Man ,O 代表 One -hot \n",
    "\n",
    "**缺点：**\n",
    "\n",
    "- 这种表示方法的一大缺点就是，把每个词都孤立起来了，导致算法对相关词的**泛化能力**不强。\n",
    "\n",
    "**举个栗子 （Example）:**\n",
    "\n",
    " 假如你已经学习到了一种语言模型。\n",
    "\n",
    "比如有下面这句话：    \n",
    "    \n",
    "  “I want a glass of orange ________”\n",
    "  \n",
    "可能想到的就是 “juice”\n",
    "\n",
    "但是看到了另一句话时，比如：\n",
    "\n",
    "“I want a glass of apple _________”\n",
    "\n",
    "如果算法不知道苹果和橙子的关系，就很难，从学习过的 橙子果汁，这样常见的东西或词语句子，推理出苹果果汁也是常见的东西或词语句子。\n",
    "\n",
    "**Why ？:** 因为 上图中 Apple  和 Orange 两个 One -hot 向量的的内积是 0,所以没有什么意义，也就无法知道 **橙子和苹果**  比 **橙子和国王**的 **更相似**。\n",
    "\n",
    "**换一种表示方式：词汇的特性**（Featurized representation）\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180302140631127?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "- 使用 **特征化**（Featurized representation） 表示。来表示 苹果 国王等词的 特征，数值化。\n",
    "\n",
    "单词与单词之间是有很多共性的，或在某一特性上相近，比如 “苹果” 和 “橙子” 都是水果；或者在某一特性上相反，比如 “父亲” 在性别上是男性，“母亲” 在性别上是女性，通过构建他们其中的联系可以将在一个单词学习到的内容应用到其他的单词上来提高模型的学习的效率。\n",
    "\n",
    "如上图所示，特征 比如 性别（Gender），高贵(Royal)，年龄(Age)，食物(Food)......等等,然后需要表示的 词，相对这些特征给出 数值化的度量。可以看到不同的词语对应着不同的特性有不同的系数值，代表着这个词语与当前特性的关系。在实际的应用中，特性的数量可能有几百种，甚至更多。\n",
    "\n",
    "比如，Man 是一个 300 维度的向量，如上图所示，用 $e_{5391}$ 来表示，其余同理。\n",
    "\n",
    "对于单词 “orange” 和 “apple” 来说他们会共享很多的特性，比如都是水果，都是圆形，都可以吃，也有些不同的特性比如颜色不同，味道不同，但因为这些特性让 RNN 模型理解了他们的关系，也就增加了通过学习一个单词去预测另一个的可能性。（学习过 Orange juice 就可以更好的明白 Apple juice ）\n",
    "\n",
    "\n",
    "**可视化词嵌入 （Visualizing word embeddings ）：**\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180302142453652?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "把一个 300 维的向量，嵌入到一个 二维空间当中，这样实现可视化。\n",
    "\n",
    "- 常用的可视化算法：**t-SNE 算法**\n",
    "\n",
    "上图中圈起来的，表示聚集程度较高。相对而言，就可以看做成一个整体。特性相似，概念相似，等等，最终可以映射为相似的特征向量。\n",
    "\n",
    "因为词性表本身是一个很高维度的空间，通过这个算法压缩到二维的可视化平面上，每一个单词 嵌入 属于自己的一个位置，相似的单词离的近，没有共性的单词离得远，这个“嵌入”的概念就是下一节的内容 词嵌 (word embeddings)。\n",
    "\n",
    "上图中对于 “嵌入” （embeddings） 的理解，想象一个 300 维的空间，Orange 这个词的向量表示，就相当于嵌入到了 300 维空间的某个点上。为了 可视化，t-SEN 算法，将高维的空间映射到了低维空间。\n",
    "\n",
    "    \n",
    "---\n",
    "## <font color=#0099ff>2.2 使用词嵌入 （Using word embeddings）\n",
    "\n",
    "\n",
    "学习目的：上节学习了不同单词的特征化表示，这节学习如何应用到 NLP 中。\n",
    "\n",
    "举个栗子：（命名实体识别）\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180302144016136?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "如上图所示，从 句子中，找出 Sally Johnson 这个名字，由 orange farmer 可知，Sally Johnson  是个人名，而非公司名。若是用特征化表示方法表示嵌入的向量，用词嵌入作为输入训练好的模型，就能更容易的知道，因为 orange farmer and apple farmer 是相似的，所以，Robert Lin 也是个人名。\n",
    "    \n",
    "**Transfer learning and word embeddings:**\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180302145728694?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "使用词嵌主要通过以下三步：（使用迁移学习）\n",
    "\n",
    "1. 获得词嵌：获得的方式可以通过训练大的文本集或者下载很多开源的词嵌库\n",
    "2. 应用词嵌：将获得的词嵌应用在我们的训练任务中\n",
    "3. 可选：通过我们的训练任务更新词嵌库（如果训练量很小就不要更新了）\n",
    "    \n",
    "- 当你的训练集数据较小时，词嵌入的作用最明显，所以在 NLP 应用很广泛。    \n",
    "    \n",
    "![这里写图片描述](http://img.blog.csdn.net/2018030215005479?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "**词嵌入和人脸编码中有奇妙的关系。**\n",
    "\n",
    "在人脸识别领域，人们喜欢用 encoding 编码结果 来指代 向量$f(x^{(i)})$ 。\n",
    "\n",
    "- 人脸识别，就是 将一张图片，放到神经网络中，计算得出编码结果。\n",
    "\n",
    "-  词嵌入，就是有一个词汇表 | V | = 10000,通过，学习算法，网络，我们学习到 $e_1........e_{10000}$ 学习到一个固定的编码。\n",
    "\n",
    "---\n",
    "## <font color=#0099ff>2.3 词嵌入的特性 （Properties of word embeddings）\n",
    "\n",
    "之前学到了 词嵌入是如何帮助构建 NLP 应用的。\n",
    "\n",
    "此外，另一个迷人的特性是，能够帮助实现，**类比推理**。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180302150958565?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)    \n",
    "\n",
    "提问： Man 对应 Woman ,那么 King 对应什么？  \n",
    "\n",
    "答案： Queen \n",
    "\n",
    "So,能否有一种算法，可以自动推导出这种关系？\n",
    "\n",
    "**实现方法：**\n",
    "\n",
    "- 典型的向量表示 是 50 到 1000 维，这里我们用简单的 4 维表示上图词汇向量，简单表示为 $e_{man},e_{woman}......$\n",
    "\n",
    "- 向量之间做相减运算：\n",
    "$$e_{man}-e_{woman} ≈\\left[ \\begin{array}{l} -2\\\\0\\\\0\\\\ 0 \\end{array} \\right] $$\n",
    "\n",
    " $$e_{king}-e_{queen}≈\\left[ \\begin{array}{l} -2\\\\0\\\\0\\\\0 \\end{array} \\right] $$\n",
    "\n",
    "以上可以看出来，两两之间相互的差别主要都是在 Gender 上。\n",
    "\n",
    "所以对于上面的问题，算法所做的就是：找一个向量，使得公式两边结果相近。\n",
    "\n",
    "$$e_{man}-e_{woman} ≈e_{king}-e_{?}$$\n",
    "\n",
    "正式探讨，将思想写成算法：\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180302151008984?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "\n",
    "如上图所示：在 300 D 的空间中， man  to woman and king to queen ,两个 向量 （箭头）在 Gender  这一特性中的差值相似。\n",
    "\n",
    "Find word w : arg $max_w  $  sim($e_w,e_{king}- e_{man}+e_{woman} $)\n",
    "\n",
    "注意：\n",
    "\n",
    "- t-SEN 算法 将 300D 映射成 2D 可视化数据，但映射关系是 复杂的且非线性，所以不能总期望等式成立的关系，会像左边那样形成平行四边形，在 t-SEN 映射中都会失去原貌。\n",
    "\n",
    "**余弦相似度（Cosine similarity）**\n",
    "\n",
    "最常用的相似度函数，余弦相似度。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180302151016798?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "$$sim(u,v) = \\frac{u^Tv}{||u||_2||v||_2}$$\n",
    "\n",
    "- $u^Tv$:u 和 v 的内积。\n",
    "\n",
    "- 计算两个向量夹角 $\\phi$ 的余弦。\n",
    "\n",
    "如上图所示，夹角是 0 时，相似度就是 1 ，夹角是 90 度时，相似度就是 0 。180 度时 是 -1。因此，这就是余弦相似度 对于这种类比工作可以起到非常好的效果。\n",
    "\n",
    "只要有足够大的语料库，就可以自主的发现上图右侧中所示例子中的模式。\n",
    "\n",
    "    \n",
    "---\n",
    "## <font color=#0099ff>2.4 嵌入矩阵  （Embedding matrix）  \n",
    "\n",
    "接下来将学习**词嵌入** 这个问题具体化，当你使用算法来学习词嵌入时，实际上是**学习一个嵌入矩阵**。\n",
    "    \n",
    "![这里写图片描述](http://img.blog.csdn.net/20180302161729802?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "- 词汇表： a     aron  ...  orange  ......zulu .....`<UNK>`\n",
    "  \n",
    "- 如上图所示，词嵌入向量构建 嵌入矩阵 E，（300, 10000）维度，其中 orange 是 6257 个，其向量表示如上所示  One -hot vector  $O_{6257}$,(10000, 1) 维度\n",
    "\n",
    "- 用 E 表示矩阵， $E * o_{6257}=\\matrix[]=e_{6257}$  是个 （300，1 ）维度的。\n",
    "\n",
    " $E * o_{j}=e_{j}$ = embedding for word j ---- 单词  j 的嵌入向量.\n",
    "\n",
    "**此节的目标就是：学习一个嵌入矩阵 E 。** E * One -hot vector 会得到 嵌入向量 （embedding  vector）\n",
    "\n",
    "下节讲述，随机化初始矩阵 E ，然后使用梯度下降法，来学习 300 * 10000 维度中的各个参数。\n",
    "    \n",
    "- 其中有一点需注意，实践中，不会直接矩阵 和向量相乘，因为存在 大量的 0 ,会单独有一个函数，查找矩阵 E 中的某列，这样不至于计算太过缓慢。\n",
    "\n",
    "\n",
    "---\n",
    "## <font color=#0099ff>2.5 学习词嵌入 （Learning word embeddings）    \n",
    "\n",
    "学习目标： 学习一些具体的算法，来学习词嵌入。\n",
    "\n",
    "算法讲解方式，从复杂型的算法，一步步简化到简单算法，但是能达到或者有更好的效果。目前流行使用的都是简单的算法。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180302161743302?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "- 首先，构建了一个语言模型，并用深度学习实现该模型。来预测如上图所示中，预测一段序列中的下一个单词。\n",
    "- 用 One-hot  向量表示词汇，如 $I$ : $o_{4343}$ (10000 维度向量)\n",
    "- 然后，生成参数矩阵 $E$.\n",
    "- $E$乘以 $o$ 得到嵌入向量 $e_{4343}$ （每一个是 300 维度），其余单词同理。 6 个单词 叠加后是 6 * 300  =1800 维度的输入\n",
    "- 全部放到神经网络中 （参数 $w^{[1]}, b^{[1]}$），再经过 softmax 分类器 (参数 $w^{[2]}, b^{[2]}$)，最后得出 10000 个可能的输出，\n",
    "-更常见的是，有一个固定的历史窗口，比如，你总是想预测给定的 4 个单词后的下一个单词，也就是图上所示，之前下面 4 个单词，而上面 2个不看，则从 1800 变为 1200。\n",
    "-意味着，你可以使用任意长度的语句，但是输入的 总是前 4 个单词。\n",
    "\n",
    "以上是 **早期**的学习词嵌入，学习 矩阵 E 的算法之一，接下来，我们来概括这个算法，从而推导出更简单的算法。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180302161752509?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "- 假设训练集中有一个更复杂的句子，juice 作为 target word （目标单词），\n",
    "- 上下文 （Context）选择 （根据模型的学习目标）\n",
    "\t- 由前 4 个单词推导，\n",
    "\t- 左边和右边各 4 个\n",
    "\t- 前 1 个单词\n",
    "\t- 附近的一个单词 （Skip Gram 模型的思想）\n",
    "\n",
    "总结：本节学习了 语言模型问题，模型提出了一个机器学习问题，即 输入一些上下文，比如目标词的 前 4 个单词，然后预测出目标词。学习了提出这些问题，是怎样帮助学习词嵌入的。\n",
    "\n",
    "下节学习，更简单的上下文，可更简单的算法来做预测。\n",
    "    \n",
    "---\n",
    "## <font color=#0099ff>2.6 Word2Vec    \n",
    "\n",
    "上节见到了，如何学习一个神经语言模型，来得到更好的词嵌入。\n",
    "\n",
    "学习内容：Word2Vec  算法，一种简单且计算更加高效的算法。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180305085731295?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)    \n",
    "\n",
    "回顾 Skip-grams ：\n",
    "\n",
    "- 抽取上下文和目标词配对，来构建监督学习问题。如上图所示，选取 orange 作为 Context ，目标单词为 juice 或 glass 或 my。\n",
    "- 也就是给定上下文，然后选取在这个词正负 10 个词距或 5 个词距，随机选取的某个目标词。\n",
    "- 构建这个监督学习问题的目标并不是监督学习本身，而是通过视同这个学习问题，来学到一个更好的词嵌入模型。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180305085739895?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "模型细节：\n",
    "- 继续假设，词汇表是 10000 （有时会超过一百万）\n",
    "- 我们要解决的基本的监督问题是，学习一种映射关系，如上图所示，从上下文 Context c 到——> 某个目标 target t 。\n",
    "- $o_c$ One-hot 向量 ——>嵌入矩阵 E (乘以$o_c$  )  ——>  得到嵌入向量$e_c$  ——>softmax——>$\\hat{y}$\n",
    "\n",
    "detail：\n",
    "\n",
    "- $Softmax : p( t | c)=\\frac{e^{\\theta_t^{T}e_c}}{\\sum_{j=1}^{10000}e^{\\theta_j^{T}e_c}}$\n",
    "\t- $\\theta_t$ 输出 t 有关的参数 t.\n",
    "- $L(\\hat{y},y)=-\\sum_{i=1}^{10000}y_ilog\\hat{y}_i$\n",
    "\t- y 如上图所示，某个位置有 一个 1 的 向量，$\\hat{y}$ 是从 softmax 输出的 10000 维的向量，是所有可能目标词的概率\n",
    "\n",
    "总结： 绿色框，框起来的部分是，一个可以找到词嵌入的简化模型和神经网络。\n",
    "\n",
    "- 矩阵 E 有对应所有嵌入向量 $e_c$ 的参数，\n",
    "- softmax 单元有 $\\theta_t$ 的参数\n",
    "- 优化损失函数 L 可以得到一个很好的嵌入向量集。\n",
    "\n",
    "以上是 Word2Vec 的 skip-grams 模型。\n",
    "\n",
    "视频中一直没有给 Word2Vec 下一个明确的定义，我们再次下一个非正式定义便于理解  word2vec 是指将词语word 变成向量 vector 的过程，这一过程通常通过浅层的神经网络完成例如 CBOW 或者skip gram，这一过程同样可以视为构建词嵌表 E  的过程”。\n",
    "\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180305085748135?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "遇到的问题：\n",
    "\n",
    "- 首要问题是，计算速度。\n",
    "\n",
    "解决方法：\n",
    "\n",
    "- 使用分级的 softmax 分类器。实际中其二叉树的形状，如上图最右侧部分，较常见的 词汇在 树的相对靠上的位置，而并不常见的词汇会在更深的位置。\n",
    "- 其他更有效的解决方法。\n",
    "    \n",
    "在skip gram中有一个不足是softmax作为激活函数需要的运算量太大，在上限为10000个单词的词库中就已经比较\n",
    "慢了。一种补救的办法是用一个它的变种“Hierachical Softmax”，通过类似二叉树的方法提高训练的效率。\n",
    "\n",
    "下节中讲到的负采样，对于加速 softmax 和解决对上图公式分母中，整个词汇表求和的问题。\n",
    "\n",
    "**怎么对上下文 c 进行采样？**\n",
    "\n",
    "实际上 P(C) 的分布，并不是单纯的再训练集语料库上均匀且随机采样得到的，而是采用了不同的启发来平衡更常见的词。\n",
    "    \n",
    "---\n",
    "## <font color=#0099ff>2.7 负采样 （Negative sampling） \n",
    "    \n",
    "上节学到了 skip-gram 模型如何构建一个监督学习任务，把上下文映射到目标词上，如何让你学到一个实用的词嵌入。但是缺点在于 softmax 计算起来很慢。\n",
    "\n",
    "本节将会看到一个改善过的学习问题叫做 **负采样 （Negative sampling）**,运用了更有效的学习算法。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180305104731923?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "在这个算法中，我们要做的就是构建一个新的监督学习问题。\n",
    "\n",
    "- 问题：给定一对单词，如 orange  and juice ,去预测 这是否是 上下文-目标词 （context-target）pair。\n",
    "\n",
    "如上图所示，从序列中，选取一定词距的词汇，序列顺序正确的作为 正样本，其余的 （ K 个）在字典或词汇表中，随机选取的，没有任何联系的作为负样本。\n",
    "\n",
    "orange —— juice —— 1 (正样本)\n",
    "orange —— King —— 0 (负样本)\n",
    "\n",
    "- 构建监督学习问题， 学习算法输入这对词 x ,要去预测的目标的标签，输出 y 。\n",
    "- 这个算法，就是判断，这两种不同的采样方式，1.在上下文词距中，选词 2.随机在词汇表中选词。\n",
    "- k = 5~20（smaller datasets）  k = 2~5 (laeger datasets)\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180305104741914?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "如上图所示，x 输入， y 输出，c 上下文，t 目标词。\n",
    "\n",
    "- 定义一个 logistic regression model \n",
    " $$P(y=1 | c,t )= \\sigma(\\theta_t^{T}e_c)$$\n",
    "\n",
    "- 参数与之前的参数相同，对于每一个可能的目标词，有一个参数向量 $\\theta$,以及另一个参数向量，即每一个可能上下文词的嵌入向量。\n",
    "- 用这个公式估计， y =1 的概率。正负样本 1/K ，每一个 正样本，都对应 K 个负样本。（本例中 k = 4）\n",
    "\n",
    "上图的下半部分为模型的神经网络结构。\n",
    "\n",
    "- 看做是 10000 个二分类 logistic 回归分类器，但不是每次迭代都训练 10000 个，而是训练其中的 5 个，训练对应真正目标词的那一个分类器，再训练随机选取的 4 个负样本。\n",
    "\n",
    "总结：\n",
    "\n",
    "- 所以相对于 softmax ，图片最上方的公式，去计算巨大的 10000维度的 softmax ,计算成本很高。\n",
    "- 转化为 10000 个 二分类问题，每一个都很容易计算，每一次迭代只需要计算其中 5 个，k +1 个样本。因此计算成本低，只需要更新 k+1 个logistic 单元。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180305104754653?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "重要细节：**如何选取负样本？**\n",
    "\n",
    "$$P(w_i)= \\frac{f(w_i)^{3/4}}{\\sum_{j=1}^{10000}f(w_j)^{3/4}}$$\n",
    "\n",
    "经验值：对词频的 3/4 次方 除以整体的值，进行采样。\n",
    "\n",
    "- $f(w_i)$ 是观测到的再语料库中某个英文词的词频，对词频的 3/4 次方 处理后，可以让它处于两个极端中间，即 处于 **完全独立分布**，和**训练集的观测分布**之间。\n",
    "\n",
    "- 网上有开源的 训练好的 词向量，在授权允许的情况下，可以加快 NLP 问题的进展。\n",
    "\n",
    "下节，介绍一个更简单的算法。GloVe \n",
    "\n",
    "---\n",
    "## <font color=#0099ff>2.8 GloVe 词向量 (GloVe word Vectors)     \n",
    "\n",
    "前面已经了解了几个关于计算词嵌入的算法，另一个在 NLP 领域有一定势头的的算法是 GloVe 算法。（这个虽然没有 Word2Vec 和 skip gram 用的多）但是也有人热衷于用它。可能是因为更简单。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180305104841766?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)    \n",
    "\n",
    "- $X_{ij}$ 是单词 i 在单词 j 上下文中出现的次数，这里 i ,j 和 t,c 的功能一样，\n",
    "- 如果全局的遍历，可以 得出 $X_{ij}=X_{ji}$ 类似的结论，但是如果规定了 词距 或前后顺序，结果可能会不一致。\n",
    "- $X_{ij}$ 是一个能够获取单词 i 和单词 j 出现位置相近时，或是彼此接近的频率的计数器。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180305104851440?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "GloVe model ：\n",
    "\n",
    "$$minimize \\sum\\limits_{i=1}^{10000}\\sum\\limits_{j=1}^{10000}f(X_{ij})(\\theta_i^Te_j+b_i+b^'_j -logX_ij)^2 $$\n",
    "\n",
    "- 两个单词之间有多少联系，也就是同时出现的频率是多少。由 $logX_ij$决定。\n",
    "- 解决 $\\theta_i^Te_j$ 的问题，用梯度下降来最小化\n",
    "- 添加一个额外的加权项（weighting term），\n",
    "\t-  $f(X_{ij})=0 $ if $X_ij=0$,约定 \"0log0\" =0,来 解决 log 0 是无穷大的这个问题，\n",
    "\t- 解决另一个问题，常出现的单词如 this,is,of a,an ......这些叫做停止词（stop words），\n",
    "\t- 对于  $f(X_{ij})$ 有着启发性的原则，既不给常出现的词过分的权重，也不给不常用的词 durian 榴莲 太小的权重。\n",
    "\n",
    "$\\theta_i$ 和 $e_j $ 是对称的\n",
    "\n",
    "- 训练算法的一个方法是，一致的初始化 $\\theta$ 和 $e$ ，然后用梯度下降来最小化输出。当每个词都处理完后，取平均值。\n",
    "\t- $e_w^{(final)}=\\frac{e_w+\\theta_w}{2}$\n",
    "\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180305104900835?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "- 你不能保证嵌入向量的组成部分，是能够理解的。很难单独看，这个矩阵的单独部分，然后解释它的意思，如上图右图所示，其关系可能是非正交的。\n",
    "- 尽管有这种类型的线性变换，这个平行四边形也说明了我们解决了这个问题，当你类比其他问题时，这个方法也是行得通的，因此尽管存在潜在的特征量的任意线性变换，最终还是能学习解决类似问题的平行四边形映射。\n",
    "\n",
    "    \n",
    "---\n",
    "## <font color=#0099ff>2.9 情绪分类 \n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180305104919143?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70) \n",
    " \n",
    "![这里写图片描述](http://img.blog.csdn.net/201803051049296?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "  \n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180305104937654?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "## <font color=#0099ff>2.10 词嵌入除偏 \n",
    "    \n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180305104948725?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "    \n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180305104957200?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    " 因为RNN通常是通过大量的网络数据文本集进行训练得到的，所以很多时候文本集中的偏见会反映在词嵌以及最终\n",
    "的结果中，例如\n",
    "如果：“男人“ 对应 “医生”， 那么“女人” 对应 什么？ RNN: “女人” 对应 “护士”。\n",
    "这种带有偏见的结果是应该尽力避免的，这类偏见大量存在于网络数据文本中，包括 性别偏见，种族偏见，年龄偏\n",
    "见，等等...\n",
    "给词嵌去偏见主要分三步（在词嵌的高维空间中完成）：\n",
    "1. 找到偏见的方向（确定偏见的x，y轴）\n",
    "2. 将非定义化的词平移到x=0（父亲，母亲这类词就是定义化的词，本身就带有了性别的暗示）\n",
    "3. 使定义化的词据离移动的词距离相等\n",
    "上述的描述有些抽象，在作业中会有专门针对这一部分的练习。 \n",
    "    \n",
    "    \n",
    "---\n",
    " \n",
    "**PS: 欢迎扫码关注公众号：「SelfImprovementLab」！专注「深度学习」，「机器学习」，「人工智能」。以及 「早起」，「阅读」，「运动」，「英语 」「其他」不定期建群 打卡互助活动。**\n",
    "\n",
    "<center><img src=\"http://upload-images.jianshu.io/upload_images/1157146-cab5ba89dfeeec4b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\"></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
