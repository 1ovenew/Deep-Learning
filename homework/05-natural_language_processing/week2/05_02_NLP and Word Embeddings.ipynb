{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursera | Andrew Ng (05-week2)—自然语言处理与词嵌入\n",
    "\n",
    "【第 5 部分-序列模型-第 2 周】在吴恩达深度学习视频基础上，笔记总结，添加个人理解，如有理解描述错误，请多加批评指教。- ZJ\n",
    "    \n",
    ">[Coursera 课程](https://www.coursera.org/specializations/deep-learning) |[deeplearning.ai](https://www.deeplearning.ai/) |[网易云课堂](https://mooc.study.163.com/smartSpec/detail/1001319001.htm)\n",
    "\n",
    "\n",
    "[CSDN]()：\n",
    "   \n",
    "\n",
    "---\n",
    "\n",
    "# 自然语言处理与词嵌入 （NLP and Word Embeddings）\n",
    "\n",
    "\n",
    "## <font color=#0099ff> 2.1 词汇表征 （Word representation）\n",
    "\n",
    "上周的学习了 RNNs GRUs and LSTMs，这一部分，我们学习将那些运用在 **自然语言处理中 （NLP）**。\n",
    "\n",
    "深度学习已经对这一领域带来了革命性的变革。其中很关键的一个概念是 **词嵌入 （Word Embeddings）**,是语言表示的一种方法，可以让算法自动理解一些类似的词，比如 男人对女人 （man is to woman）,国王对皇后（King is to queen）等类似的例子。\n",
    "\n",
    "通过词嵌入的概念，就可以构建 NLP 应用了。即使你的模型中，标记的训练值较小。这周课的最后，我们会学习如何消除词嵌入的偏差，就是去除不想要的特性。或者学习算法有时候会学到的其他类型的偏差。\n",
    "\n",
    "**单词的表示：**\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180302135029652?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "- 我们一直使用词汇表（Vocabulary）来表示词，如 | V |=10000 .\n",
    "- One-hot Vector ,如上图所示，使用 $O_{5391}$ 表示 Man ,O 代表 One -hot \n",
    "\n",
    "**缺点：**\n",
    "\n",
    "- 这种表示方法的一大缺点就是，把每个词都孤立起来了，导致算法对相关词的**泛化能力**不强。\n",
    "\n",
    "**举个栗子 （Example）:**\n",
    "\n",
    " 假如你已经学习到了一种语言模型。\n",
    "\n",
    "比如有下面这句话：    \n",
    "    \n",
    "  “I want a glass of orange ________”\n",
    "  \n",
    "可能想到的就是 “juice”\n",
    "\n",
    "但是看到了另一句话时，比如：\n",
    "\n",
    "“I want a glass of apple _________”\n",
    "\n",
    "如果算法不知道苹果和橙子的关系，就很难，从学习过的 橙子果汁，这样常见的东西或词语句子，推理出苹果果汁也是常见的东西或词语句子。\n",
    "\n",
    "**Why ？:** 因为 上图中 Apple  和 Orange 两个 One -hot 向量的的内积是 0,所以没有什么意义，也就无法知道 **橙子和苹果**  比 **橙子和国王**的 **更相似**。\n",
    "\n",
    "**换一种表示方式：词汇的特性**（Featurized representation）\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180302140631127?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "使用 **特征化**（Featurized representation） 表示。来表示 苹果 国王等词的 特征，数值化。\n",
    "\n",
    "单词与单词之间是有很多共性的，或在某一特性上相近，比如 “苹果” 和 “橙子” 都是水果；或者在某一特性上相反，比如 “父亲” 在性别上是男性，“母亲” 在性别上是女性，通过构建他们其中的联系可以将在一个单词学习到的内容应用到其他的单词上来提高模型的学习的效率。\n",
    "\n",
    "如上图所示，特征 比如 性别（Gender），高贵(Royal)，年龄(Age)，食物(Food)......等等,然后需要表示的 词，相对这些特征给出 数值化的度量。可以看到不同的词语对应着不同的特性有不同的系数值，代表着这个词语与当前特性的关系。在实际的应用中，特性的数量可能有几百种，甚至更多。\n",
    "\n",
    "比如，Man 是一个 300 维度的向量，如上图所示，用 $e_{5391}$ 来表示，其余同理。\n",
    "\n",
    "对于单词 “orange” 和 “apple” 来说他们会共享很多的特性，比如都是水果，都是圆形，都可以吃，也有些不同的特性比如颜色不同，味道不同，但因为这些特性让 RNN 模型理解了他们的关系，也就增加了通过学习一个单词去预测另一个的可能性。（学习过 Orange juice 就可以更好的明白 Apple juice ）\n",
    "\n",
    "\n",
    "**可视化词嵌入 （Visualizing word embeddings ）：**\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180302142453652?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "\n",
    "\n",
    "这里还介绍了一个 t-SNE 算法，因为词性表本身是一个很高维度的空间，通过这个算法压缩到二维的可视化平面上，每一个单词 嵌入 属于自己的一个位置，相似的单词离的近，没有共性的单词离得远，这个“嵌入”的概念就是下一节的内容 词嵌(word embeddings)\n",
    "\n",
    "    \n",
    "---\n",
    "## <font color=#0099ff>2.2 使用词嵌入\n",
    "    \n",
    " 先下一个非正规定义 “词嵌 - 描述了词性特征的总量，也是在高维词性空间中嵌入的位置，拥有越多共性的词，词嵌离得越近，反之则越远”。值得注意的是，表达这个“位置”，需要使用所有设定的词性特征，假如有300个特征（性别，颜色，...），那么词嵌的空间维度就是300.\n",
    "\n",
    "使用词嵌主要通过以下三步：\n",
    "\n",
    "1. 获得词嵌：获得的方式可以通过训练大的文本集或者下载很多开源的词嵌库\n",
    "2. 应用词嵌：将获得的词嵌应用在我们的训练任务中\n",
    "3. 可选：通过我们的训练任务更新词嵌库（如果训练量很小就不要更新了）\n",
    "    \n",
    "    \n",
    "    \n",
    "---\n",
    "## <font color=#0099ff>2.3 词嵌入的特性   \n",
    "    \n",
    " 假设有如下的问题：\n",
    "\"Man\" -> \"Woman\" 那么 \"King\" -> ？\n",
    "这个问题被称作词汇的类比问题，通过研究词嵌的特征可以解决这样的问题。\n",
    "    \n",
    "数学的表达式为：\n",
    "\n",
    "\n",
    "这个公式相当于在算两个向量(vector)的cos相似度\n",
    "    \n",
    "---\n",
    "## <font color=#0099ff>2.4 嵌入矩阵    \n",
    "    \n",
    "\n",
    "这一节中主要讲了词嵌矩阵的shape，如果词嵌（词性特征的总量）是300，独热编码的长度是10000，那么词嵌\n",
    "矩阵的的shape就是 300 * 10000 。所以就有了下面的式子：\n",
    "词嵌矩阵 * 单词的独热编码 = 单词的词嵌\n",
    "(300, 10000) * (10000, 1) = (300, 1)\n",
    "\n",
    "    \n",
    "---\n",
    "## <font color=#0099ff>2.5 学习词嵌入     \n",
    "\n",
    "可以通过训练神经网络的方式构建词嵌表 E ，这次换个方式，先放图\n",
    "    \n",
    "\n",
    "在这个训练模式中，是通过全部的单词去预测最后一个单词然后反向传播更新词嵌表E\n",
    "假设要预测的单词为W，词嵌表仍然为E，需要注意的是训练词嵌表和预测W是两个不同的任务。\n",
    "如果任务是预测W，最佳方案是使用W前面n个单词构建语境。\n",
    "如果任务是训练E，除了使用W前全部单词还可以通过：前后各4个单词、前面单独的一个词、前面语境中随机的一个词（这个方式也叫做 Skip Gram 算法），这些方法都能提供很好的结果。\n",
    "    \n",
    "---\n",
    "## <font color=#0099ff>2.6 Word2Vec    \n",
    "    \n",
    "视频中一直没有给 Word2Vec 下一个明确的定义，我们再次下一个非正式定义便于理解 “word2vec 是指将词语\n",
    "word 变成向量vector 的过程，这一过程通常通过浅层的神经网络完成例如CBOW或者skip gram，这一过程同样\n",
    "可以视为构建词嵌表E的过程”。\n",
    "这里着重介绍了skip gram model，上一节介绍过这是一个用一个随机词预测其他词的方法。比如下面这句话中\n",
    "“I want a glass of orange juice.”\n",
    "我们可以选orange作为随机词 c(Context)，通过设置窗口值例如前后5个单词以监督学习的方式去预测其中的词\n",
    "t(Target) 例如“juice, glass, a, of” 但需要注意的是，这个过程仍然是为了搭建（更新）词嵌表 E 而不是为了真正\n",
    "的去预测，所以如果预测效果不好并不用担心，表达式：\n",
    "    \n",
    "在skip gram中有一个不足是softmax作为激活函数需要的运算量太大，在上限为10000个单词的词库中就已经比较\n",
    "慢了。一种补救的办法是用一个它的变种“Hierachical Softmax”，通过类似二叉树的方法提高训练的效率。\n",
    "    \n",
    "---\n",
    "## <font color=#0099ff>2.7 负采样    \n",
    "    \n",
    "对于skip gram model而言，还要解决的一个问题是如何取样（选择）有效的随机词 c 和目标词 t 呢？如果真的按\n",
    "照自然随机分布的方式去选择，可能会大量重复的选择到出现次数频率很高的单词比如说“the, of, a, it, I, ...” 重复的\n",
    "训练这样的单词没有特别大的意义。\n",
    "如何有效的去训练选定的词如 orange 呢？在设置训练集时可以通过“负取样”的方法, 下表中第一行是通过和上面一\n",
    "样的窗口法得到的“正”（1）结果，其他三行是从字典中随机得到的词语，结果为“负”（0）。通过这样的负取样法\n",
    "可以更有效地去训练skip gram model.\n",
    "    \n",
    "负取样的个数 k 由数据量的大小而定，上述例子中为3. 实际中数据量大则 k = 2 ~ 5，数据量小则可以相对大一些\n",
    "k = 5 ~ 20\n",
    "通过负取样，我们的神经网络训练从softmax预测每个词出现的频率变成了经典binary logistic regression问题，\n",
    "概率公式用 sigmoid 代替 softmax从而大大提高了速度。\n",
    "    \n",
    "\n",
    "最后我们通过一个并没有被理论验证但是实际效果很好的方式来确定每个被负选样选中的概率为：\n",
    "\n",
    "\n",
    "---\n",
    "## <font color=#0099ff>2.8 GloVe 词向量     \n",
    "    \n",
    "GloVe(global vectors for word representation) 作为自然语言处理中的算法，没有像work2vec中的模型那么流\n",
    "行，但它仍然有自己独特的优点 - 简单。\n",
    "= 词汇 i 在语境 j 中出现的次数，i 相当于之前的 t， j 相当于之前的 c，这个模型的意图是最小化下面这个公\n",
    "式：    \n",
    "\n",
    "\n",
    "存在是为了防止当 时上面的式子等于0而不是无穷大。\n",
    "在接下来的两节中将会介绍用到了这些技术的应用。\n",
    "    \n",
    "---\n",
    "## <font color=#0099ff>2.9 情绪分类 \n",
    "    \n",
    "情绪分类是指将一段文字要表达的情绪通过RNN识别出来。比如说我们开了一家餐厅，在微博上有很多人给我们留\n",
    "言，现在的任务是将这些留言以分数的形式量化(1 ～ 5分),以便做更好的改良，在RNN的帮助下可以做到这一点\n",
    "\n",
    "---\n",
    "## <font color=#0099ff>2.10 词嵌入除偏 \n",
    "    \n",
    "    \n",
    " 因为RNN通常是通过大量的网络数据文本集进行训练得到的，所以很多时候文本集中的偏见会反映在词嵌以及最终\n",
    "的结果中，例如\n",
    "如果：“男人“ 对应 “医生”， 那么“女人” 对应 什么？ RNN: “女人” 对应 “护士”。\n",
    "这种带有偏见的结果是应该尽力避免的，这类偏见大量存在于网络数据文本中，包括 性别偏见，种族偏见，年龄偏\n",
    "见，等等...\n",
    "给词嵌去偏见主要分三步（在词嵌的高维空间中完成）：\n",
    "1. 找到偏见的方向（确定偏见的x，y轴）\n",
    "2. 将非定义化的词平移到x=0（父亲，母亲这类词就是定义化的词，本身就带有了性别的暗示）\n",
    "3. 使定义化的词据离移动的词距离相等\n",
    "上述的描述有些抽象，在作业中会有专门针对这一部分的练习。 \n",
    "    \n",
    "    \n",
    "---\n",
    " \n",
    "**PS: 欢迎扫码关注公众号：「SelfImprovementLab」！专注「深度学习」，「机器学习」，「人工智能」。以及 「早起」，「阅读」，「运动」，「英语 」「其他」不定期建群 打卡互助活动。**\n",
    "\n",
    "<center><img src=\"http://upload-images.jianshu.io/upload_images/1157146-cab5ba89dfeeec4b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\"></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
