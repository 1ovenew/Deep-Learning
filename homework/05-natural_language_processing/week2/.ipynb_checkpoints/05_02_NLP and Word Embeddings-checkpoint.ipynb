{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursera | Andrew Ng (05-week2)—自然语言处理与词嵌入\n",
    "\n",
    "【第 5 部分-序列模型-第 2 周】在吴恩达深度学习视频基础上，笔记总结，添加个人理解，如有理解描述错误，请多加批评指教。- ZJ\n",
    "    \n",
    ">[Coursera 课程](https://www.coursera.org/specializations/deep-learning) |[deeplearning.ai](https://www.deeplearning.ai/) |[网易云课堂](https://mooc.study.163.com/smartSpec/detail/1001319001.htm)\n",
    "\n",
    "\n",
    "[CSDN]()：\n",
    "   \n",
    "\n",
    "---\n",
    "\n",
    "# 自然语言处理与词嵌入 （NLP and Word Embeddings）\n",
    "\n",
    "\n",
    "## <font color=#0099ff> 2.1 词汇表征 （Word representation）\n",
    "\n",
    "上周的学习了 RNNs GRUs and LSTMs，这一部分，我们学习将那些运用在 **自然语言处理中 （NLP）**。\n",
    "\n",
    "深度学习已经对这一领域带来了革命性的变革。其中很关键的一个概念是 **词嵌入 （Word Embeddings）**,是语言表示的一种方法，可以让算法自动理解一些类似的词，比如 男人对女人 （man is to woman）,国王对皇后（King is to queen）等类似的例子。\n",
    "\n",
    "通过词嵌入的概念，就可以构建 NLP 应用了。即使你的模型中，标记的训练值较小。这周课的最后，我们会学习如何消除词嵌入的偏差，就是去除不想要的特性。或者学习算法有时候会学到的其他类型的偏差。\n",
    "\n",
    "**单词的表示：**\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180302135029652?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "- 我们一直使用词汇表（Vocabulary）来表示词，如 | V |=10000 .\n",
    "- One-hot Vector ,如上图所示，使用 $O_{5391}$ 表示 Man ,O 代表 One -hot \n",
    "\n",
    "**缺点：**\n",
    "\n",
    "- 这种表示方法的一大缺点就是，把每个词都孤立起来了，导致算法对相关词的**泛化能力**不强。\n",
    "\n",
    "**举个栗子 （Example）:**\n",
    "\n",
    " 假如你已经学习到了一种语言模型。\n",
    "\n",
    "比如有下面这句话：    \n",
    "    \n",
    "  “I want a glass of orange ________”\n",
    "  \n",
    "可能想到的就是 “juice”\n",
    "\n",
    "但是看到了另一句话时，比如：\n",
    "\n",
    "“I want a glass of apple _________”\n",
    "\n",
    "如果算法不知道苹果和橙子的关系，就很难，从学习过的 橙子果汁，这样常见的东西或词语句子，推理出苹果果汁也是常见的东西或词语句子。\n",
    "\n",
    "**Why ？:** 因为 上图中 Apple  和 Orange 两个 One -hot 向量的的内积是 0,所以没有什么意义，也就无法知道 **橙子和苹果**  比 **橙子和国王**的 **更相似**。\n",
    "\n",
    "**换一种表示方式：词汇的特性**（Featurized representation）\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180302140631127?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "- 使用 **特征化**（Featurized representation） 表示。来表示 苹果 国王等词的 特征，数值化。\n",
    "\n",
    "单词与单词之间是有很多共性的，或在某一特性上相近，比如 “苹果” 和 “橙子” 都是水果；或者在某一特性上相反，比如 “父亲” 在性别上是男性，“母亲” 在性别上是女性，通过构建他们其中的联系可以将在一个单词学习到的内容应用到其他的单词上来提高模型的学习的效率。\n",
    "\n",
    "如上图所示，特征 比如 性别（Gender），高贵(Royal)，年龄(Age)，食物(Food)......等等,然后需要表示的 词，相对这些特征给出 数值化的度量。可以看到不同的词语对应着不同的特性有不同的系数值，代表着这个词语与当前特性的关系。在实际的应用中，特性的数量可能有几百种，甚至更多。\n",
    "\n",
    "比如，Man 是一个 300 维度的向量，如上图所示，用 $e_{5391}$ 来表示，其余同理。\n",
    "\n",
    "对于单词 “orange” 和 “apple” 来说他们会共享很多的特性，比如都是水果，都是圆形，都可以吃，也有些不同的特性比如颜色不同，味道不同，但因为这些特性让 RNN 模型理解了他们的关系，也就增加了通过学习一个单词去预测另一个的可能性。（学习过 Orange juice 就可以更好的明白 Apple juice ）\n",
    "\n",
    "\n",
    "**可视化词嵌入 （Visualizing word embeddings ）：**\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180302142453652?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "把一个 300 维的向量，嵌入到一个 二维空间当中，这样实现可视化。\n",
    "\n",
    "- 常用的可视化算法：**t-SNE 算法**\n",
    "\n",
    "上图中圈起来的，表示聚集程度较高。相对而言，就可以看做成一个整体。特性相似，概念相似，等等，最终可以映射为相似的特征向量。\n",
    "\n",
    "因为词性表本身是一个很高维度的空间，通过这个算法压缩到二维的可视化平面上，每一个单词 嵌入 属于自己的一个位置，相似的单词离的近，没有共性的单词离得远，这个“嵌入”的概念就是下一节的内容 词嵌 (word embeddings)。\n",
    "\n",
    "上图中对于 “嵌入” （embeddings） 的理解，想象一个 300 维的空间，Orange 这个词的向量表示，就相当于嵌入到了 300 维空间的某个点上。为了 可视化，t-SEN 算法，将高维的空间映射到了低维空间。\n",
    "\n",
    "    \n",
    "---\n",
    "## <font color=#0099ff>2.2 使用词嵌入 （Using word embeddings）\n",
    "\n",
    "\n",
    "学习目的：上节学习了不同单词的特征化表示，这节学习如何应用到 NLP 中。\n",
    "\n",
    "举个栗子：（命名实体识别）\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180302144016136?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "如上图所示，从 句子中，找出 Sally Johnson 这个名字，由 orange farmer 可知，Sally Johnson  是个人名，而非公司名。若是用特征化表示方法表示嵌入的向量，用词嵌入作为输入训练好的模型，就能更容易的知道，因为 orange farmer and apple farmer 是相似的，所以，Robert Lin 也是个人名。\n",
    "    \n",
    "**Transfer learning and word embeddings:**\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180302145728694?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "使用词嵌主要通过以下三步：（使用迁移学习）\n",
    "\n",
    "1. 获得词嵌：获得的方式可以通过训练大的文本集或者下载很多开源的词嵌库\n",
    "2. 应用词嵌：将获得的词嵌应用在我们的训练任务中\n",
    "3. 可选：通过我们的训练任务更新词嵌库（如果训练量很小就不要更新了）\n",
    "    \n",
    "- 当你的训练集数据较小时，词嵌入的作用最明显，所以在 NLP 应用很广泛。    \n",
    "    \n",
    "![这里写图片描述](http://img.blog.csdn.net/2018030215005479?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "**词嵌入和人脸编码中有奇妙的关系。**\n",
    "\n",
    "在人脸识别领域，人们喜欢用 encoding 编码结果 来指代 向量$f(x^{(i)})$ 。\n",
    "\n",
    "- 人脸识别，就是 将一张图片，放到神经网络中，计算得出编码结果。\n",
    "\n",
    "-  词嵌入，就是有一个词汇表 | V | = 10000,通过，学习算法，网络，我们学习到 $e_1........e_{10000}$ 学习到一个固定的编码。\n",
    "\n",
    "---\n",
    "## <font color=#0099ff>2.3 词嵌入的特性 （Properties of word embeddings）\n",
    "\n",
    "之前学到了 词嵌入是如何帮助构建 NLP 应用的。\n",
    "\n",
    "此外，另一个迷人的特性是，能够帮助实现，**类比推理**。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180302150958565?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)    \n",
    "\n",
    "提问： Man 对应 Woman ,那么 King 对应什么？  \n",
    "\n",
    "答案： Queen \n",
    "\n",
    "So,能否有一种算法，可以自动推导出这种关系？\n",
    "\n",
    "**实现方法：**\n",
    "\n",
    "- 典型的向量表示 是 50 到 1000 维，这里我们用简单的 4 维表示上图词汇向量，简单表示为 $e_{man},e_{woman}......$\n",
    "\n",
    "- 向量之间做相减运算：\n",
    "$$e_{man}-e_{woman} ≈\\left[ \\begin{array}{l} -2\\\\0\\\\0\\\\ 0 \\end{array} \\right] $$\n",
    "\n",
    " $$e_{king}-e_{queen}≈\\left[ \\begin{array}{l} -2\\\\0\\\\0\\\\0 \\end{array} \\right] $$\n",
    "\n",
    "以上可以看出来，两两之间相互的差别主要都是在 Gender 上。\n",
    "\n",
    "所以对于上面的问题，算法所做的就是：找一个向量，使得公式两边结果相近。\n",
    "\n",
    "$$e_{man}-e_{woman} ≈e_{king}-e_{?}$$\n",
    "\n",
    "正式探讨，将思想写成算法：\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180302151008984?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "\n",
    "如上图所示：在 300 D 的空间中， man  to woman and king to queen ,两个 向量 （箭头）在 Gender  这一特性中的差值相似。\n",
    "\n",
    "Find word w : arg $max_w  $  sim($e_w,e_{king}- e_{man}+e_{woman} $)\n",
    "\n",
    "注意：\n",
    "\n",
    "- t-SEN 算法 将 300D 映射成 2D 可视化数据，但映射关系是 复杂的且非线性，所以不能总期望等式成立的关系，会像左边那样形成平行四边形，在 t-SEN 映射中都会失去原貌。\n",
    "\n",
    "**余弦相似度（Cosine similarity）**\n",
    "\n",
    "最常用的相似度函数，余弦相似度。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180302151016798?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "$$sim(u,v) = \\frac{u^Tv}{||u||_2||v||_2}$$\n",
    "\n",
    "- $u^Tv$:u 和 v 的内积。\n",
    "\n",
    "- 计算两个向量夹角 $\\phi$ 的余弦。\n",
    "\n",
    "如上图所示，夹角是 0 时，相似度就是 1 ，夹角是 90 度时，相似度就是 0 。180 度时 是 -1。因此，这就是余弦相似度 对于这种类比工作可以起到非常好的效果。\n",
    "\n",
    "只要有足够大的语料库，就可以自主的发现上图右侧中所示例子中的模式。\n",
    "\n",
    "    \n",
    "---\n",
    "## <font color=#0099ff>2.4 嵌入矩阵  （Embedding matrix）  \n",
    "\n",
    "接下来将学习**词嵌入** 这个问题具体化，当你使用算法来学习词嵌入时，实际上是**学习一个嵌入矩阵**。\n",
    "    \n",
    "![这里写图片描述](http://img.blog.csdn.net/20180302161729802?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "- 词汇表： a     aron  ...  orange  ......zulu .....`<UNK>`\n",
    "  \n",
    "- 如上图所示，词嵌入向量构建 嵌入矩阵 E，（300, 10000）维度，其中 orange 是 6257 个，其向量表示如上所示  One -hot vector  $O_{6257}$,(10000, 1) 维度\n",
    "\n",
    "- 用 E 表示矩阵， $E * o_{6257}=\\matrix[]=e_{6257}$  是个 （300，1 ）维度的。\n",
    "\n",
    " $E * o_{j}=e_{j}$ = embedding for word j ---- 单词  j 的嵌入向量.\n",
    "\n",
    "**此节的目标就是：学习一个嵌入矩阵 E 。** E * One -hot vector 会得到 嵌入向量 （embedding  vector）\n",
    "\n",
    "下节讲述，随机化初始矩阵 E ，然后使用梯度下降法，来学习 300 * 10000 维度中的各个参数。\n",
    "    \n",
    "- 其中有一点需注意，实践中，不会直接矩阵 和向量相乘，因为存在 大量的 0 ,会单独有一个函数，查找矩阵 E 中的某列，这样不至于计算太过缓慢。\n",
    "\n",
    "\n",
    "---\n",
    "## <font color=#0099ff>2.5 学习词嵌入 （Learning word embeddings）    \n",
    "\n",
    "学习目标： 学习一些具体的算法，来学习词嵌入。\n",
    "\n",
    "算法讲解方式，从复杂型的算法，一步步简化到简单算法，但是能达到或者有更好的效果。目前流行使用的都是简单的算法。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180302161743302?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "- 首先，构建了一个语言模型，并用深度学习实现该模型。来预测如上图所示中，预测一段序列中的下一个单词。\n",
    "- 用 One-hot  向量表示词汇，如 $I$ : $o_{4343}$ (10000 维度向量)\n",
    "- 然后，生成参数矩阵 $E$.\n",
    "- $E$乘以 $o$ 得到嵌入向量 $e_{4343}$ （每一个是 300 维度），其余单词同理。 6 个单词 叠加后是 6 * 300  =1800 维度的输入\n",
    "- 全部放到神经网络中 （参数 $w^{[1]}, b^{[1]}$），再经过 softmax 分类器 (参数 $w^{[2]}, b^{[2]}$)，最后得出 10000 个可能的输出，\n",
    "-更常见的是，有一个固定的历史窗口，比如，你总是想预测给定的 4 个单词后的下一个单词，也就是图上所示，之前下面 4 个单词，而上面 2个不看，则从 1800 变为 1200。\n",
    "-意味着，你可以使用任意长度的语句，但是输入的 总是前 4 个单词。\n",
    "\n",
    "以上是 **早期**的学习词嵌入，学习 矩阵 E 的算法之一，接下来，我们来概括这个算法，从而推导出更简单的算法。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180302161752509?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "- 假设训练集中有一个更复杂的句子，juice 作为 target word （目标单词），\n",
    "- 上下文 （Context）选择 （根据模型的学习目标）\n",
    "\t- 由前 4 个单词推导，\n",
    "\t- 左边和右边各 4 个\n",
    "\t- 前 1 个单词\n",
    "\t- 附近的一个单词 （Skip Gram 模型的思想）\n",
    "\n",
    "总结：本节学习了 语言模型问题，模型提出了一个机器学习问题，即 输入一些上下文，比如目标词的 前 4 个单词，然后预测出目标词。学习了提出这些问题，是怎样帮助学习词嵌入的。\n",
    "\n",
    "下节学习，更简单的上下文，可更简单的算法来做预测。\n",
    "    \n",
    "---\n",
    "## <font color=#0099ff>2.6 Word2Vec    \n",
    "\n",
    "上节见到了，如何学习一个神经语言模型，来得到更好的词嵌入。\n",
    "\n",
    "学习内容：Word2Vec  算法，一种简单且计算更加高效的算法。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180305085731295?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)    \n",
    "\n",
    "回顾 Skip-grams ：\n",
    "\n",
    "- 抽取上下文和目标词配对，来构建监督学习问题。如上图所示，选取 orange 作为 Context ，目标单词为 juice 或 glass 或 my。\n",
    "- 也就是给定上下文，然后选取在这个词正负 10 个词距或 5 个词距，随机选取的某个目标词。\n",
    "- 构建这个监督学习问题的目标并不是监督学习本身，而是通过视同这个学习问题，来学到一个更好的词嵌入模型。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180305085739895?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "模型细节：\n",
    "- 继续假设，词汇表是 10000 （有时会超过一百万）\n",
    "- 我们要解决的基本的监督问题是，学习一种映射关系，如上图所示，从上下文 Context c 到——> 某个目标 target t 。\n",
    "- $o_c$ One-hot 向量 ——>嵌入矩阵 E (乘以$o_c$  )  ——>  得到嵌入向量$e_c$  ——>softmax——>$\\hat{y}$\n",
    "\n",
    "detail：\n",
    "\n",
    "- $Softmax : p( t | c)=\\frac{e^{\\theta_t^{T}e_c}}{\\sum_{j=1}^{10000}e^{\\theta_j^{T}e_c}}$\n",
    "\t- $\\theta_t$ 输出 t 有关的参数 t.\n",
    "- $L(\\hat{y},y)=-\\sum_{i=1}^{10000}y_ilog\\hat{y}_i$\n",
    "\t- y 如上图所示，某个位置有 一个 1 的 向量，$\\hat{y}$ 是从 softmax 输出的 10000 维的向量，是所有可能目标词的概率\n",
    "\n",
    "总结： 绿色框，框起来的部分是，一个可以找到词嵌入的简化模型和神经网络。\n",
    "\n",
    "- 矩阵 E 有对应所有嵌入向量 $e_c$ 的参数，\n",
    "- softmax 单元有 $\\theta_t$ 的参数\n",
    "- 优化损失函数 L 可以得到一个很好的嵌入向量集。\n",
    "\n",
    "以上是 Word2Vec 的 skip-grams 模型。\n",
    "\n",
    "视频中一直没有给 Word2Vec 下一个明确的定义，我们再次下一个非正式定义便于理解  word2vec 是指将词语word 变成向量 vector 的过程，这一过程通常通过浅层的神经网络完成例如 CBOW 或者skip gram，这一过程同样可以视为构建词嵌表 E  的过程”。\n",
    "\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180305085748135?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "遇到的问题：\n",
    "\n",
    "- 首要问题是，计算速度。\n",
    "\n",
    "解决方法：\n",
    "\n",
    "- 使用分级的 softmax 分类器。实际中其二叉树的形状，如上图最右侧部分，较常见的 词汇在 树的相对靠上的位置，而并不常见的词汇会在更深的位置。\n",
    "- 其他更有效的解决方法。\n",
    "    \n",
    "在skip gram中有一个不足是softmax作为激活函数需要的运算量太大，在上限为10000个单词的词库中就已经比较\n",
    "慢了。一种补救的办法是用一个它的变种“Hierachical Softmax”，通过类似二叉树的方法提高训练的效率。\n",
    "\n",
    "下节中讲到的负采样，对于加速 softmax 和解决对上图公式分母中，整个词汇表求和的问题。\n",
    "\n",
    "**怎么对上下文 c 进行采样？**\n",
    "\n",
    "实际上 P(C) 的分布，并不是单纯的再训练集语料库上均匀且随机采样得到的，而是采用了不同的启发来平衡更常见的词。\n",
    "    \n",
    "---\n",
    "## <font color=#0099ff>2.7 负采样 （Negative sampling） \n",
    "    \n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180305104731923?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180305104741914?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180305104754653?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "\n",
    "对于skip gram model而言，还要解决的一个问题是如何取样（选择）有效的随机词 c 和目标词 t 呢？如果真的按\n",
    "照自然随机分布的方式去选择，可能会大量重复的选择到出现次数频率很高的单词比如说“the, of, a, it, I, ...” 重复的\n",
    "训练这样的单词没有特别大的意义。\n",
    "如何有效的去训练选定的词如 orange 呢？在设置训练集时可以通过“负取样”的方法, 下表中第一行是通过和上面一\n",
    "样的窗口法得到的“正”（1）结果，其他三行是从字典中随机得到的词语，结果为“负”（0）。通过这样的负取样法\n",
    "可以更有效地去训练skip gram model.\n",
    "    \n",
    "负取样的个数 k 由数据量的大小而定，上述例子中为3. 实际中数据量大则 k = 2 ~ 5，数据量小则可以相对大一些\n",
    "k = 5 ~ 20\n",
    "通过负取样，我们的神经网络训练从softmax预测每个词出现的频率变成了经典binary logistic regression问题，\n",
    "概率公式用 sigmoid 代替 softmax从而大大提高了速度。\n",
    "    \n",
    "\n",
    "最后我们通过一个并没有被理论验证但是实际效果很好的方式来确定每个被负选样选中的概率为：\n",
    "\n",
    "\n",
    "---\n",
    "## <font color=#0099ff>2.8 GloVe 词向量     \n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180305104841766?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)    \n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180305104851440?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180305104900835?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "GloVe(global vectors for word representation) 作为自然语言处理中的算法，没有像work2vec中的模型那么流\n",
    "行，但它仍然有自己独特的优点 - 简单。\n",
    "= 词汇 i 在语境 j 中出现的次数，i 相当于之前的 t， j 相当于之前的 c，这个模型的意图是最小化下面这个公\n",
    "式：    \n",
    "\n",
    "\n",
    "存在是为了防止当 时上面的式子等于0而不是无穷大。\n",
    "在接下来的两节中将会介绍用到了这些技术的应用。\n",
    "    \n",
    "---\n",
    "## <font color=#0099ff>2.9 情绪分类 \n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180305104919143?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70) \n",
    " \n",
    "![这里写图片描述](http://img.blog.csdn.net/201803051049296?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "  \n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180305104937654?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "情绪分类是指将一段文字要表达的情绪通过RNN识别出来。比如说我们开了一家餐厅，在微博上有很多人给我们留\n",
    "言，现在的任务是将这些留言以分数的形式量化(1 ～ 5分),以便做更好的改良，在RNN的帮助下可以做到这一点\n",
    "\n",
    "---\n",
    "## <font color=#0099ff>2.10 词嵌入除偏 \n",
    "    \n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180305104948725?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "    \n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180305104957200?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    " 因为RNN通常是通过大量的网络数据文本集进行训练得到的，所以很多时候文本集中的偏见会反映在词嵌以及最终\n",
    "的结果中，例如\n",
    "如果：“男人“ 对应 “医生”， 那么“女人” 对应 什么？ RNN: “女人” 对应 “护士”。\n",
    "这种带有偏见的结果是应该尽力避免的，这类偏见大量存在于网络数据文本中，包括 性别偏见，种族偏见，年龄偏\n",
    "见，等等...\n",
    "给词嵌去偏见主要分三步（在词嵌的高维空间中完成）：\n",
    "1. 找到偏见的方向（确定偏见的x，y轴）\n",
    "2. 将非定义化的词平移到x=0（父亲，母亲这类词就是定义化的词，本身就带有了性别的暗示）\n",
    "3. 使定义化的词据离移动的词距离相等\n",
    "上述的描述有些抽象，在作业中会有专门针对这一部分的练习。 \n",
    "    \n",
    "    \n",
    "---\n",
    " \n",
    "**PS: 欢迎扫码关注公众号：「SelfImprovementLab」！专注「深度学习」，「机器学习」，「人工智能」。以及 「早起」，「阅读」，「运动」，「英语 」「其他」不定期建群 打卡互助活动。**\n",
    "\n",
    "<center><img src=\"http://upload-images.jianshu.io/upload_images/1157146-cab5ba89dfeeec4b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\"></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
