{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursera | Andrew Ng (05-week2)—自然语言处理与词嵌入\n",
    "\n",
    "【第 5 部分-序列模型-第 2 周】在吴恩达深度学习视频基础上，笔记总结，添加个人理解，如有理解描述错误，请多加批评指教。- ZJ\n",
    "    \n",
    ">[Coursera 课程](https://www.coursera.org/specializations/deep-learning) |[deeplearning.ai](https://www.deeplearning.ai/) |[网易云课堂](https://mooc.study.163.com/smartSpec/detail/1001319001.htm)\n",
    "\n",
    "\n",
    "[CSDN]()：\n",
    "   \n",
    "\n",
    "---\n",
    "\n",
    "# 自然语言处理与词嵌入 （NLP and Word Embeddings）\n",
    "\n",
    "\n",
    "## <font color=#0099ff> 2.1 词汇表征 （Word representation）\n",
    "\n",
    "上周的学习了 RNNs GRUs and LSTMs，这一部分，我们学习将那些运用在 **自然语言处理中 （NLP）**。\n",
    "\n",
    "深度学习已经对这一领域带来了革命性的变革。其中很关键的一个概念是 **词嵌入 （Word Embeddings）**,是语言表示的一种方法，可以让算法自动理解一些类似的词，比如 男人对女人 （man is to woman）,国王对皇后（King is to queen）等类似的例子。\n",
    "\n",
    "通过词嵌入的概念，就可以构建 NLP 应用了。即使你的模型中，标记的训练值较小。这周课的最后，我们会学习如何消除词嵌入的偏差，就是去除不想要的特性。或者学习算法有时候会学到的其他类型的偏差。\n",
    "\n",
    "**单词的表示：**\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180302135029652?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "- 我们一直使用词汇表（Vocabulary）来表示词，如 | V |=10000 .\n",
    "- One-hot Vector ,如上图所示，使用 $O_{5391}$ 表示 Man ,O 代表 One -hot \n",
    "\n",
    "**缺点：**\n",
    "\n",
    "- 这种表示方法的一大缺点就是，把每个词都孤立起来了，导致算法对相关词的**泛化能力**不强。\n",
    "\n",
    "**举个栗子 （Example）:**\n",
    "\n",
    " 假如你已经学习到了一种语言模型。\n",
    "\n",
    "比如有下面这句话：    \n",
    "    \n",
    "  “I want a glass of orange ________”\n",
    "  \n",
    "可能想到的就是 “juice”\n",
    "\n",
    "但是看到了另一句话时，比如：\n",
    "\n",
    "“I want a glass of apple _________”\n",
    "\n",
    "如果算法不知道苹果和橙子的关系，就很难，从学习过的 橙子果汁，这样常见的东西或词语句子，推理出苹果果汁也是常见的东西或词语句子。\n",
    "\n",
    "**Why ？:** 因为 上图中 Apple  和 Orange 两个 One -hot 向量的的内积是 0,所以没有什么意义，也就无法知道 **橙子和苹果**  比 **橙子和国王**的 **更相似**。\n",
    "\n",
    "**换一种表示方式：词汇的特性**（Featurized representation）\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180302140631127?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "- 使用 **特征化**（Featurized representation） 表示。来表示 苹果 国王等词的 特征，数值化。\n",
    "\n",
    "单词与单词之间是有很多共性的，或在某一特性上相近，比如 “苹果” 和 “橙子” 都是水果；或者在某一特性上相反，比如 “父亲” 在性别上是男性，“母亲” 在性别上是女性，通过构建他们其中的联系可以将在一个单词学习到的内容应用到其他的单词上来提高模型的学习的效率。\n",
    "\n",
    "如上图所示，特征 比如 性别（Gender），高贵(Royal)，年龄(Age)，食物(Food)......等等,然后需要表示的 词，相对这些特征给出 数值化的度量。可以看到不同的词语对应着不同的特性有不同的系数值，代表着这个词语与当前特性的关系。在实际的应用中，特性的数量可能有几百种，甚至更多。\n",
    "\n",
    "比如，Man 是一个 300 维度的向量，如上图所示，用 $e_{5391}$ 来表示，其余同理。\n",
    "\n",
    "对于单词 “orange” 和 “apple” 来说他们会共享很多的特性，比如都是水果，都是圆形，都可以吃，也有些不同的特性比如颜色不同，味道不同，但因为这些特性让 RNN 模型理解了他们的关系，也就增加了通过学习一个单词去预测另一个的可能性。（学习过 Orange juice 就可以更好的明白 Apple juice ）\n",
    "\n",
    "\n",
    "**可视化词嵌入 （Visualizing word embeddings ）：**\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180302142453652?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "把一个 300 维的向量，嵌入到一个 二维空间当中，这样实现可视化。\n",
    "\n",
    "- 常用的可视化算法：**t-SNE 算法**\n",
    "\n",
    "上图中圈起来的，表示聚集程度较高。相对而言，就可以看做成一个整体。特性相似，概念相似，等等，最终可以映射为相似的特征向量。\n",
    "\n",
    "因为词性表本身是一个很高维度的空间，通过这个算法压缩到二维的可视化平面上，每一个单词 嵌入 属于自己的一个位置，相似的单词离的近，没有共性的单词离得远，这个“嵌入”的概念就是下一节的内容 词嵌 (word embeddings)。\n",
    "\n",
    "上图中对于 “嵌入” （embeddings） 的理解，想象一个 300 维的空间，Orange 这个词的向量表示，就相当于嵌入到了 300 维空间的某个点上。为了 可视化，t-SEN 算法，将高维的空间映射到了低维空间。\n",
    "\n",
    "    \n",
    "---\n",
    "## <font color=#0099ff>2.2 使用词嵌入 （Using word embeddings）\n",
    "\n",
    "\n",
    "学习目的：上节学习了不同单词的特征化表示，这节学习如何应用到 NLP 中。\n",
    "\n",
    "举个栗子：（命名实体识别）\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180302144016136?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "如上图所示，从 句子中，找出 Sally Johnson 这个名字，由 orange farmer 可知，Sally Johnson  是个人名，而非公司名。若是用特征化表示方法表示嵌入的向量，用词嵌入作为输入训练好的模型，就能更容易的知道，因为 orange farmer and apple farmer 是相似的，所以，Robert Lin 也是个人名。\n",
    "    \n",
    "**Transfer learning and word embeddings:**\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180302145728694?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "使用词嵌主要通过以下三步：（使用迁移学习）\n",
    "\n",
    "1. 获得词嵌：获得的方式可以通过训练大的文本集或者下载很多开源的词嵌库\n",
    "2. 应用词嵌：将获得的词嵌应用在我们的训练任务中\n",
    "3. 可选：通过我们的训练任务更新词嵌库（如果训练量很小就不要更新了）\n",
    "    \n",
    "- 当你的训练集数据较小时，词嵌入的作用最明显，所以在 NLP 应用很广泛。    \n",
    "    \n",
    "![这里写图片描述](http://img.blog.csdn.net/2018030215005479?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "**词嵌入和人脸编码中有奇妙的关系。**\n",
    "\n",
    "在人脸识别领域，人们喜欢用 encoding 编码结果 来指代 向量$f(x^{(i)})$ 。\n",
    "\n",
    "- 人脸识别，就是 将一张图片，放到神经网络中，计算得出编码结果。\n",
    "\n",
    "-  词嵌入，就是有一个词汇表 | V | = 10000,通过，学习算法，网络，我们学习到 $e_1........e_{10000}$ 学习到一个固定的编码。\n",
    "\n",
    "---\n",
    "## <font color=#0099ff>2.3 词嵌入的特性 （Properties of word embeddings）\n",
    "\n",
    "之前学到了 词嵌入是如何帮助构建 NLP 应用的。\n",
    "\n",
    "此外，另一个迷人的特性是，能够帮助实现，**类比推理**。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180302150958565?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)    \n",
    "\n",
    "提问： Man 对应 Woman ,那么 King 对应什么？  \n",
    "\n",
    "答案： Queen \n",
    "\n",
    "So,能否有一种算法，可以自动推导出这种关系？\n",
    "\n",
    "**实现方法：**\n",
    "\n",
    "- 典型的向量表示 是 50 到 1000 维，这里我们用简单的 4 维表示上图词汇向量，简单表示为 $e_{man},e_{woman}......$\n",
    "\n",
    "- 向量之间做相减运算：\n",
    "$$e_{man}-e_{woman} ≈\\left[ \\begin{array}{l} -2\\\\0\\\\0\\\\ 0 \\end{array} \\right] $$\n",
    "\n",
    " $$e_{king}-e_{queen}≈\\left[ \\begin{array}{l} -2\\\\0\\\\0\\\\0 \\end{array} \\right] $$\n",
    "\n",
    "以上可以看出来，两两之间相互的差别主要都是在 Gender 上。\n",
    "\n",
    "所以对于上面的问题，算法所做的就是：找一个向量，使得公式两边结果相近。\n",
    "\n",
    "$$e_{man}-e_{woman} ≈e_{king}-e_{?}$$\n",
    "\n",
    "正式探讨，将思想写成算法：\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180302151008984?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "\n",
    "如上图所示：在 300 D 的空间中， man  to woman and king to queen ,两个 向量 （箭头）在 Gender  这一特性中的差值相似。\n",
    "\n",
    "Find word w : arg $max_w  $  sim($e_w,e_{king}- e_{man}+e_{woman} $)\n",
    "\n",
    "注意：\n",
    "\n",
    "- t-SEN 算法 将 300D 映射成 2D 可视化数据，但映射关系是 复杂的且非线性，所以不能总期望等式成立的关系，会像左边那样形成平行四边形，在 t-SEN 映射中都会失去原貌。\n",
    "\n",
    "**余弦相似度（Cosine similarity）**\n",
    "\n",
    "最常用的相似度函数，余弦相似度。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180302151016798?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "$$sim(u,v) = \\frac{u^Tv}{||u||_2||v||_2}$$\n",
    "\n",
    "- $u^Tv$:u 和 v 的内积。\n",
    "\n",
    "- 计算两个向量夹角 $\\phi$ 的余弦。\n",
    "\n",
    "如上图所示，夹角是 0 时，相似度就是 1 ，夹角是 90 度时，相似度就是 0 。180 度时 是 -1。因此，这就是余弦相似度 对于这种类比工作可以起到非常好的效果。\n",
    "\n",
    "只要有足够大的语料库，就可以自主的发现上图右侧中所示例子中的模式。\n",
    "\n",
    "    \n",
    "---\n",
    "## <font color=#0099ff>2.4 嵌入矩阵  （Embedding matrix）  \n",
    "\n",
    "接下来将学习**词嵌入** 这个问题具体化，当你使用算法来学习词嵌入时，实际上是**学习一个嵌入矩阵**。\n",
    "    \n",
    "![这里写图片描述](http://img.blog.csdn.net/20180302161729802?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "- 词汇表： a     aron  ...  orange  ......zulu .....`<UNK>`\n",
    "  \n",
    "- 如上图所示，词嵌入向量构建 嵌入矩阵 E，（300, 10000）维度，其中 orange 是 6257 个，其向量表示如上所示  One -hot vector  $O_{6257}$,(10000, 1) 维度\n",
    "\n",
    "- 用 E 表示矩阵， $E * o_{6257}=\\matrix[]=e_{6257}$  是个 （300，1 ）维度的。\n",
    "\n",
    " $E * o_{j}=e_{j}$ = embedding for word j ---- 单词  j 的嵌入向量.\n",
    "\n",
    "**此节的目标就是：学习一个嵌入矩阵 E 。** E * One -hot vector 会得到 嵌入向量 （embedding  vector）\n",
    "\n",
    "下节讲述，随机化初始矩阵 E ，然后使用梯度下降法，来学习 300 * 10000 维度中的各个参数。\n",
    "    \n",
    "- 其中有一点需注意，实践中，不会直接矩阵 和向量相乘，因为存在 大量的 0 ,会单独有一个函数，查找矩阵 E 中的某列，这样不至于计算太过缓慢。\n",
    "\n",
    "\n",
    "---\n",
    "## <font color=#0099ff>2.5 学习词嵌入 （Learning word embeddings）    \n",
    "\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180302161743302?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180302161752509?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "可以通过训练神经网络的方式构建词嵌表 E ，这次换个方式，先放图\n",
    "    \n",
    "\n",
    "在这个训练模式中，是通过全部的单词去预测最后一个单词然后反向传播更新词嵌表E\n",
    "假设要预测的单词为W，词嵌表仍然为E，需要注意的是训练词嵌表和预测W是两个不同的任务。\n",
    "如果任务是预测W，最佳方案是使用W前面n个单词构建语境。\n",
    "如果任务是训练E，除了使用W前全部单词还可以通过：前后各4个单词、前面单独的一个词、前面语境中随机的一个词（这个方式也叫做 Skip Gram 算法），这些方法都能提供很好的结果。\n",
    "    \n",
    "---\n",
    "## <font color=#0099ff>2.6 Word2Vec    \n",
    "    \n",
    "视频中一直没有给 Word2Vec 下一个明确的定义，我们再次下一个非正式定义便于理解 “word2vec 是指将词语\n",
    "word 变成向量vector 的过程，这一过程通常通过浅层的神经网络完成例如CBOW或者skip gram，这一过程同样\n",
    "可以视为构建词嵌表E的过程”。\n",
    "这里着重介绍了skip gram model，上一节介绍过这是一个用一个随机词预测其他词的方法。比如下面这句话中\n",
    "“I want a glass of orange juice.”\n",
    "我们可以选orange作为随机词 c(Context)，通过设置窗口值例如前后5个单词以监督学习的方式去预测其中的词\n",
    "t(Target) 例如“juice, glass, a, of” 但需要注意的是，这个过程仍然是为了搭建（更新）词嵌表 E 而不是为了真正\n",
    "的去预测，所以如果预测效果不好并不用担心，表达式：\n",
    "    \n",
    "在skip gram中有一个不足是softmax作为激活函数需要的运算量太大，在上限为10000个单词的词库中就已经比较\n",
    "慢了。一种补救的办法是用一个它的变种“Hierachical Softmax”，通过类似二叉树的方法提高训练的效率。\n",
    "    \n",
    "---\n",
    "## <font color=#0099ff>2.7 负采样    \n",
    "    \n",
    "对于skip gram model而言，还要解决的一个问题是如何取样（选择）有效的随机词 c 和目标词 t 呢？如果真的按\n",
    "照自然随机分布的方式去选择，可能会大量重复的选择到出现次数频率很高的单词比如说“the, of, a, it, I, ...” 重复的\n",
    "训练这样的单词没有特别大的意义。\n",
    "如何有效的去训练选定的词如 orange 呢？在设置训练集时可以通过“负取样”的方法, 下表中第一行是通过和上面一\n",
    "样的窗口法得到的“正”（1）结果，其他三行是从字典中随机得到的词语，结果为“负”（0）。通过这样的负取样法\n",
    "可以更有效地去训练skip gram model.\n",
    "    \n",
    "负取样的个数 k 由数据量的大小而定，上述例子中为3. 实际中数据量大则 k = 2 ~ 5，数据量小则可以相对大一些\n",
    "k = 5 ~ 20\n",
    "通过负取样，我们的神经网络训练从softmax预测每个词出现的频率变成了经典binary logistic regression问题，\n",
    "概率公式用 sigmoid 代替 softmax从而大大提高了速度。\n",
    "    \n",
    "\n",
    "最后我们通过一个并没有被理论验证但是实际效果很好的方式来确定每个被负选样选中的概率为：\n",
    "\n",
    "\n",
    "---\n",
    "## <font color=#0099ff>2.8 GloVe 词向量     \n",
    "    \n",
    "GloVe(global vectors for word representation) 作为自然语言处理中的算法，没有像work2vec中的模型那么流\n",
    "行，但它仍然有自己独特的优点 - 简单。\n",
    "= 词汇 i 在语境 j 中出现的次数，i 相当于之前的 t， j 相当于之前的 c，这个模型的意图是最小化下面这个公\n",
    "式：    \n",
    "\n",
    "\n",
    "存在是为了防止当 时上面的式子等于0而不是无穷大。\n",
    "在接下来的两节中将会介绍用到了这些技术的应用。\n",
    "    \n",
    "---\n",
    "## <font color=#0099ff>2.9 情绪分类 \n",
    "    \n",
    "情绪分类是指将一段文字要表达的情绪通过RNN识别出来。比如说我们开了一家餐厅，在微博上有很多人给我们留\n",
    "言，现在的任务是将这些留言以分数的形式量化(1 ～ 5分),以便做更好的改良，在RNN的帮助下可以做到这一点\n",
    "\n",
    "---\n",
    "## <font color=#0099ff>2.10 词嵌入除偏 \n",
    "    \n",
    "    \n",
    " 因为RNN通常是通过大量的网络数据文本集进行训练得到的，所以很多时候文本集中的偏见会反映在词嵌以及最终\n",
    "的结果中，例如\n",
    "如果：“男人“ 对应 “医生”， 那么“女人” 对应 什么？ RNN: “女人” 对应 “护士”。\n",
    "这种带有偏见的结果是应该尽力避免的，这类偏见大量存在于网络数据文本中，包括 性别偏见，种族偏见，年龄偏\n",
    "见，等等...\n",
    "给词嵌去偏见主要分三步（在词嵌的高维空间中完成）：\n",
    "1. 找到偏见的方向（确定偏见的x，y轴）\n",
    "2. 将非定义化的词平移到x=0（父亲，母亲这类词就是定义化的词，本身就带有了性别的暗示）\n",
    "3. 使定义化的词据离移动的词距离相等\n",
    "上述的描述有些抽象，在作业中会有专门针对这一部分的练习。 \n",
    "    \n",
    "    \n",
    "---\n",
    " \n",
    "**PS: 欢迎扫码关注公众号：「SelfImprovementLab」！专注「深度学习」，「机器学习」，「人工智能」。以及 「早起」，「阅读」，「运动」，「英语 」「其他」不定期建群 打卡互助活动。**\n",
    "\n",
    "<center><img src=\"http://upload-images.jianshu.io/upload_images/1157146-cab5ba89dfeeec4b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\"></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
