{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursera | Andrew Ng (02-week-1-1.6)—Dropout regularization\n",
    "\n",
    "该系列仅在原课程基础上部分知识点添加个人学习笔记，或相关推导补充等。如有错误，还请批评指教。在学习了 Andrew Ng 课程的基础上，为了更方便的查阅复习，将其整理成文字。因本人一直在学习英语，所以该系列以英文为主，同时也建议读者以英文为主，中文辅助，以便后期进阶时，为学习相关领域的学术论文做铺垫。- ZJ\n",
    "    \n",
    ">[Coursera 课程](https://www.coursera.org/specializations/deep-learning) |[deeplearning.ai](https://www.deeplearning.ai/) |[网易云课堂](https://mooc.study.163.com/smartSpec/detail/1001319001.htm)\n",
    "\n",
    "---\n",
    "   **转载请注明作者和出处：ZJ 微信公众号-「SelfImprovementLab」**\n",
    "   \n",
    "   [知乎](https://zhuanlan.zhihu.com/c_147249273)：https://zhuanlan.zhihu.com/c_147249273\n",
    "   \n",
    "   [CSDN](http://blog.csdn.net/JUNJUN_ZHAO/article/details/79066602)：http://blog.csdn.net/JUNJUN_ZHAO/article/details/79066602\n",
    "   \n",
    "\n",
    "---\n",
    "\n",
    "**1.6  Dropout regularization (Dropout 正则化)**\n",
    "\n",
    "(字幕来源：网易云课堂)\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180116072059390?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "In addition to L2 regularization,another very powerful regularization techniques is called \"dropout.\"Let's see how that works.Let's say you train a neural network like the one on the left and there's over-fitting.Here's what you do with dropout.Let me make a copy of the neural network.With dropout, what we're going to do is go through each of the layers of the network,and set some probability of eliminating a node in neural network.So let's say that for each of these layers,we're going to- for each node, toss a coinand have a 0.5 chance of keeping each node and 0.5 chance of removing each node.So, after the coin tosses,maybe we'll decide to eliminate those nodes,\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180116072156863?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "除了 L2 正则化，还有一个非常实用的正则化方法——“dropout（随机失活）”，我们来看看它的工作原理，假设你在训练左图这样的神经网络  它存在过拟合，这就是 dropout 所要处理的，我们复制这个神经网络，dropout 会遍历网络的每一层，并设置消除神经网络中节点的概率，假设  网络中的每一层，每个节点都以抛硬币的方式设置概率，**每个节点得以保留和消除的概率都是 0.5**，设置完节点概率，我们会消除一些节点，\n",
    "\n",
    "then what you do is actually remove all the ingoing outgoing things from that node as well.So you end up with a much smaller, really much diminished network.And then you do back propagation training.There's one example on this much diminished network.And then on different examples, you would toss a set of coins againand keep a different set of nodes and then dropout or eliminate different set of nodes.And so for each training example,you would train it using one of these neural reduced networks.So, maybe it seems like a slightly crazy technique.They just go around coding those are random,but this actually works.But you can imagine that because you're training a much smaller network on each example,or maybe just give a sense for why you end up able to regularize the network,because these much smaller networks are being trained.\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180116072211635?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "然后删掉从该节点进出的连线，**最后得到一个节点更少  规模更小的网络**，然后用 backprop 方法进行训练。这是网络节点精简后的一个样本，对于其它样本  我们照旧以抛硬币的方式设置概率，保留一类节点集合  删除其它类型的节点集合，对于每个训练样本，**我们都将采用一个精简后的神经网络来训练它**，这种方法似乎有点怪，单纯遍历节点  编码也是随机的，可它真的有效，不过可想而知  我们针对每个样本训练规模极小的网络，最后你可能会认识到为什么要正则化网络，因为我们在训练极小的网络，\n",
    "\n",
    "Let's look at how you implement dropout.There are a few ways of implementing dropout.I'm going to show you the most common one,which is technique called **inverted dropout**.For the sake of completeness,let's say we want to illustrate this with layer l=3.So, in the code, I'm going to write - there will be a bunch of 3s here.I'm just illustrating how to implement dropout in a single layer.So, what we are going to do is set a vector dand $d^3$ is going to be the dropout vector for the layer 3.That's what the 3 is to be np.random.rand And this is going to be the same shape as a3. And when I see if this is less than some number,which I'm going to call keep.prob.  And so, keep-prob is a number, it was 0.5 on the previous time,and maybe now I'll use 0.8 in this example,and there will be the probability that a given hidden unit will be kept.So keep.prob = 0.8,then this means that there's a 0.2 chance of eliminating any hidden unit.So, what it does is it generates a random matrix.And this works as well if you have factorized.So d3 will be a matrix,where for each example and for each hidden unit,there's a 0.8 chance that the corresponding d3 will be one,and a 20% chance there will be zero.So, this random numbers being less than 0.8, it has a 0.8 chance of being one or be true,and 20% or 0.2 chance of being false of being zero.\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180116084539655?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "如何实施 dropout 呢，方法有几种，接下来我要讲的是最常用的方法，即 **inverted dropout（反向随机失活）**，出于完整性考虑，我们用一个三层（l=3）网络来举例说明，编码中会有很多涉及到3的地方，我只举例说明如何在某一层中实施 dropout，首先要定义向量d，$d^3 $表示一个三层的 dropout 向量，$d^3$=`np.random.rand`，参数都是 $a^3$`.shape`，然后看它是否小于某数，我们称之为 **keep-prob**，**keep-prob 是一个具体数字**  上个示例中它是 0.5，而本例中它是 0.8，**它表示保留某个隐藏单元的概率**，此处 keep-prob 等于0.8，它意味着消除任意一个隐藏单元的概率是0.2，**它的作用就是生成随机矩阵**，如果对 $a^3$ 进行因子分解 效果也是一样的，d3 是一个矩阵，每个样本和每个隐藏单元，其在 d3 中的对应值为 1 的概率都是 0.8，其对应值为 0 的概率是 20%，随机数字小于 0.8  它等于 1 的概率是 0.8，等于 0 的概率是 0.2。\n",
    "\n",
    "And then what you are going to do is take your activations from the third layer,let me just call it a3 in this low example.So, a3 has the activations you compute.And you can set a3 to be equal to the old a3 times, there is element wise multiplication.Or you can also write this as a3* = d3.But what this does is for every element of d3 that's equal to zero.And there was a 20% chance of each of the elements being zero,just multiply operation ends up zeroing out, the corresponding element of d3.If you do this in python,technically d3 will be a boolean array where value is true and false,rather than one and zero.But the multiply operation works and will interpret the true and false values as one and zero.If you try this yourself in python, you'll see.\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180116084539655?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "接下来要做的就是**从第三层中获取激活函数**，这里我们叫它 a3，a3 含有要计算的激活函数，a3 等于上面的 a3 乘以 d3，这里是元素相乘，也可以写成 a3*=d3，**它的作用就是过滤 d3 中所有等于 0 的元素**，而各个元素等于 0 的概率只有20%，乘法运算最终把 d3 中相应元素归零，如果用 python 实现该算法的话，**d3 则是一个布尔型数组  值为 true 和 false**，而不是 1 和 0，**乘法运算依然有效  ,phyton 会把 tru e和 false 翻译为 1 和 0**，大家可以用 python 尝试一下，最后  我们向外扩展 a3，用它除以 0.8  或者除以 keep-prob 参数。\n",
    "\n",
    "```\n",
    "keep_prob = 0.8  # 设置神经元保留概率\n",
    "d3 = np.random.rand(a3.shape[0], a3.shape[1]) < keep_prob\n",
    "a3 = np.multiply(a3, d3)\n",
    "a3 /= keep_prob\n",
    "```\n",
    "\n",
    "Then finally, we're going to take a3 and scale it up by dividing by 0.8 or really dividing by our keep.prob parameter.So, let me explain what this final step is doing.Let's say for the sake of argument that you have 50 unitsor 50 neurons in the third hidden layer.So maybe a3 is 50 by one dimensional or if you factorization maybe it's 50 by m dimensional.So, if you have a 80% chance of keeping them and 20% chance of eliminating them,this means that on average,you end up with 10 units shut off or 10 units zeroed out.And so now, if you look at the value of $z^4$,$z^4$ is going to be equal to $w^4$ * $a^3$ + $b^4$.And so, on expectation,this will be reduced by 20%.By which I mean that 20% of the elements of a3 will be zeroed out.So, in order to not reduce the expected value of $z^4$,what you do is you need to take this, and divide it by 0.8,because this will correct or just a bump that back up by the roughly 20% that you need.So it's not changed the expected value of a3.And, so this line here is what's called the inverted dropout technique.And its effect is that,no matter what you set to keep.prob to whether it's 0.8 or 0.9 or even one,if it's set to one, then there's no dropout,because it's keeping everything or 0.5 or whatever,this inverted dropout technique by dividing by the keep.prop,it ensures that the expected value of a3 remains the same.And it turns out that at test time,when you trying to evaluate a neural network,which we'll talk about on the next slide.this inverted dropout technique,there is line to are due to the green box at dropping out.This makes test time easier, because you have less of a scaling problem.\n",
    "\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180116084102250?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "下面我解释一下为什么要这么做，方便起见  我假设第三隐层上有 50 个单元，或 50 个神经元，在一维上 a3 是 50，我们通过因子分解将它拆分成 50Xm 维的，保留和删除它们的概率分别是 80% 和 20%，这意味着，最后被删除或归零的单元平均有 10 个，现在我们看下 $z^{[4]}$，它等于$w^{[4]}* a^{[3]} + b^{[4]}$，我们的预期是，$a^{[3]}$减少 20%，也就是说 $a^{[3]}$中有 20% 的元素被归零，为了不影响$z^{[4]}$的期望值，我们需要用 $w^{[4]}*a^{[3]}$ 除以 0.8，它将会修正或弥补我们所需的那 20%，a3 的期望值不会变，划线部分就是所谓的 dropout 方法，它的功能是，不论 keep-prob 的值是多少，0.8  0.9  甚至是 1，如果 keep-prob 设置为 1  那么就不存在dropout，因为它会保留所有节点，**反向随机失活（inverted dropout）** 方法通过**除以 keep-prob，确保 a3 的期望值不变**，事实证明  在测试阶段，当我们评估一个神经网络时，inverted dropout，也就是用绿线框标注的反向随机失活方法，使测试阶段变得更容易，因为它的数据扩展问题变少 我们将在下节课讨论。\n",
    "\n",
    "By far the most common implementation of dropouts today as far as I know is inverted dropouts.I recommend you just implement this.But there were some early iterations of dropout that missed this divide by keep.prob line,and so at test time, the average becomes more and more complicated.But again, people tend not to use those other versions.So, what you do is you use the d vector,and you'll notice that for different training examples,you zero out different hidden units.And in fact, if you make multiple passes through the same training set,then on different pauses through the training set,you should randomly zero out different hidden units.So, it's not that for one example,you should keep zeroing out the different same hidden units is that,on iteration one of gradient descent,you might zero out some hidden units.And on the second iteration of great descent where you go through the training set the second time,maybe you'll zero out a different pattern of hidden units.And the vector d or d3, for the third layer, is used to decide what to zero out,both in foreprop as well as in that backprop.We are just showing foreprop here.\n",
    "\n",
    "\n",
    "据我了解  **目前实施 dropout 最常用的方法就是，Inverted dropout**，建议大家动手实践一下，dropout 早期的迭代版本都没有除以keep-prob，所以在测试阶段  平均值会变得越来越复杂，不过那些版本已经不再使用了，现在你使用的是 d 向量，你会发现  不同的训练样本，清除不同的隐藏单元也不同，实际上  如果你通过相同训练集多次传递数据，每次训练数据的梯度不同，则随机对不同隐藏单元归零，有时却并非如此  比如，需要将相同隐藏单元归零，第一次迭代梯度下降时，把一些隐藏单元归零，第二次迭代梯度下跌时  也就是第二次遍历训练集时，对不同类型的隐层单元归零，向量 d 或 d3 用来决定第三层中哪些单元归零，无论用 foreprop 还是 backprop，这里我们只介绍了 foreprob。\n",
    "\n",
    "Now, having trained the algorithm at test time, here's what you would do.At test time, you're given some x or which you want to make a prediction.And using our standard notation,I'm going to use a^0,the activations of the zeroes layer to denote just test example x.So what we're going to do is not to use dropout at test time,in particular which is in a sense Z^1= w^1.a^0 + b^1.a^1 = g^1(z^1).Z^2 = w^2.a^1 + b^2.a^2 =...and so on, until you get to the last layer and that you make a prediction $\\hat{y}$.But notice that at the test time you're not using dropout explicitly and that you're not tossing coins at random,you're not flipping coins to decide which hidden units to eliminate.And that's because when you are making predictions at the test time,you don't really want your output to be random.If you are implementing dropout at test time, that just add noise to your predictions.\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180116092009161?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "如何在测试阶段训练算法，在测试阶段  我们已经给出了 x  或是想预测的变量，用的是标准计数法，我用 $a^{[0]}$，第 0 层的激活函数标注为测试样本 x，**我们在测试阶段不使用 dropout函数**，尤其是像下列情况，$Z^{[1]}= w^{[1]}*a^{[0]} + b^{[1]}$，$a^{[1]} = g^{[1]}(z^{[1]})，Z^{[2]} = w^{[2]}*a^{[1]} + b^{[2]}$，$a^{[2]}$=...，以此类推直到最后一层  预测值为 $\\hat{y}$，显然在测试阶段  我们并未使用 dropout，自然也就不用抛硬币来决定失活概率，以及要消除哪些隐藏单元了，**因为在测试阶段进行预测时，我们不期望输出结果是随机的，如果测试阶段应用 dropout 函数  预测会受到干扰**。\n",
    "\n",
    "In theory, one thing you could do is run a prediction process many times with different hidden units randomly dropped out and have it across them.But that's computationally inefficient and will give you roughly the same result,very, very similar results to this different procedure as well.And just to mention, the inverted dropout thing,remember the step on the previous line when we divided by the keep.prob.The effect of that was to ensure that even when you don't implement dropout at test time to the scaling,the expected value of these activations don't change.So, you don't need to add in an extra funny scaling parameter at test time.That's different than when you had at training time.So that's dropouts.And when you implement this in this week's programming exercise,you gain more first hand experience with it as well.But why does it really work?What I want to do in the next video is give yousome better intuition about what dropout really is doing.Let's go on to the next video.\n",
    "\n",
    "理论上  你只需要多次运行预测处理过程，每一次  不同的隐藏单元会被随机归零  预测处理遍历它们，但是计算效率低  得出的结果也几乎相同，与这个不同程序产生的结果极为相似，inverted dropout 函数，在除以 keep-prob 时可以记住上一步的操作，目的是确保即使在测试阶段不执行 dropout 来调整数值范围，激活函数的预期结果也不会发生变化，所以没必要在测试阶段额外添加尺度参数，这与训练阶段不同，这就是 dropout，大家可以通过本周的编程练习来执行这个函数，亲身实践一下，为什么 dropout 会起作用呢，下节课我们将，更加直观地了解 dropout 的具体功能，下节课见。\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### <font color=#0099ff>重点总结：</font>\n",
    "\n",
    "#### Dropout 正则化\n",
    " \n",
    "Dropout（随机失活）就是在神经网络的 Dropout 层，为每个神经元结点设置一个随机消除的概率，对于保留下来的神经元，我们得到一个节点较少，规模较小的网络进行训练。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180116072211635?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "**实现Dropout的方法：反向随机失活（Inverted dropout）**\n",
    "\n",
    "首先假设对 layer 3 进行dropout：\n",
    "\n",
    "```\n",
    "keep_prob = 0.8  # 设置神经元保留概率\n",
    "d3 = np.random.rand(a3.shape[0], a3.shape[1]) < keep_prob\n",
    "a3 = np.multiply(a3, d3)\n",
    "a3 /= keep_prob\n",
    "```\n",
    "\n",
    "这里解释下为什么要有最后一步：`a3 /= keep_prob`\n",
    "\n",
    "依照例子中的 `keep_prob = 0.8 `，那么就有大约 20% 的神经元被删除了，也就是说 $a^{[3]}$ 中有 20% 的元素被归零了，在下一层的计算中有 $Z^{[4]}=W^{[4]}\\cdot a^{[3]}+b^{[4]}$，所以为了不影响 $Z^{[4]}$ 的期望值，所以需要 $W^{[4]}\\cdot a^{[3]}$ 的部分除以一个 `keep_prob`。\n",
    "\n",
    "Inverted dropout 通过对“`a3 /= keep_prob`”,则保证无论 keep_prob 设置为多少，都不会对 $Z^{[4]}$ 的期望值产生影响。\n",
    "\n",
    "**Notation**：在测试阶段不要用 dropout，因为那样会使得预测结果变得随机。\n",
    "\n",
    "\n",
    "\n",
    "参考文献：\n",
    "\n",
    "[1]. 大树先生.[吴恩达Coursera深度学习课程 DeepLearning.ai 提炼笔记（2-1）-- 深度学习的实践方面](http://blog.csdn.net/koala_tree/article/details/78125697)\n",
    "\n",
    "\n",
    "---\n",
    " \n",
    "**PS: 欢迎扫码关注公众号：「SelfImprovementLab」！专注「深度学习」，「机器学习」，「人工智能」。以及 「早起」，「阅读」，「运动」，「英语 」「其他」不定期建群 打卡互助活动。**\n",
    "\n",
    "<center><img src=\"http://upload-images.jianshu.io/upload_images/1157146-cab5ba89dfeeec4b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\"></center>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
