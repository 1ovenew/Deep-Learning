{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursera | Andrew Ng (01-week4)—深层神经网络\n",
    "\n",
    "在吴恩达深度学习视频以及大树先生的博客提炼笔记基础上添加个人理解，原大树先生博客可查看该链接地址[大树先生的博客](http://blog.csdn.net/koala_tree)- ZJ\n",
    "    \n",
    ">[Coursera 课程](https://www.coursera.org/specializations/deep-learning) |[deeplearning.ai](https://www.deeplearning.ai/) |[网易云课堂](https://mooc.study.163.com/smartSpec/detail/1001319001.htm)\n",
    "\n",
    "   [CSDN]()：\n",
    "\n",
    "---\n",
    "\n",
    "## <font color=#0099ff>4.1 Deep Neural Network （深层神经网络 ）\n",
    "\n",
    "**神经网络层数表示**：只算 隐含层 和 输出层，不算 输入层。\n",
    "\n",
    "\n",
    "**符号约定:**\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180109152249729?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "L = 4 (# layers)\n",
    "\n",
    "$n^{[l]}$= # units in layer $l$ \n",
    "\n",
    "**hidden  layer** : $n^{[1]}=5,n^{[2]}=5,n^{[3]}=3$ 隐含层\n",
    "\n",
    "**output layer** : $n^{[4]}=n^{[L]}=1$   输出层\n",
    "\n",
    "**input layer**: $n^{[0]}=n_x=3$  输入层\n",
    "\n",
    "$a^{[l]}$=activations in layer $l$   激活函数层\n",
    "\n",
    "$a^{[l]}=g^{[l]}(Z^{[l]})$\n",
    "\n",
    "$ W^{[l]}=$ weights for  $Z^{[l]}$\n",
    "\n",
    "$b^{[l]} $is used to compute $z^{[l]}$\n",
    "\n",
    "$x=a^{[0]}$  输入特征 $x$ 也是第 0 层 激活函数\n",
    "\n",
    "$\\hat{y}=a^{[L]}$ 预测输出也是激活函数第 $l$ 层\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## <font color=#0099ff> 4.2 Forward and Backward Propagation (前向和反向传播)\n",
    "    \n",
    "   \n",
    "**构成深度神经网络的基本模块：每一层都有前向传播的步骤，以及一个相对的反向传播步骤。**\n",
    "\n",
    "**前向传播 Forward  Propogation for  layer $l$:**\n",
    "\n",
    "$Input :  a^{[l-1]}$ `# a 符号代表激活函数层 ，当 $l$= 1 时，则 $a^{[0]}$ 为特征 $x$`\n",
    "\n",
    "$Output: a^{[l]} ,cache (z^{[l]}),   w^{[l]} ,  z^{[l]}$\n",
    "\n",
    "对比看：($z=w^{T}x +b$)\n",
    "\n",
    "$z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}$\n",
    "\n",
    "对比看：（$a=\\sigma{（z）}$）\n",
    "\n",
    "$a^{[l]}=g^{[l]}(z^{[l]})$\n",
    "\n",
    "**Vectorized  将以上向量化：**\n",
    "\n",
    "$Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}$\n",
    "\n",
    "$A^{[l]}=g^{[l]}(Z^{[l]})$\n",
    "\n",
    "\n",
    "$a^{[0]}$ ：是对于**一个训练样本**的输入特征。\n",
    "\n",
    "$A^{[0]}$：是针对**一整个训练集**的话输入特征。\n",
    "\n",
    "\n",
    "**反向传播（Backward propagation）** \n",
    "\n",
    "$Input：da^{[l]}$ \n",
    "\n",
    "$Output：  da^{[l-1]}，dW^{[l]}，db^{[l]}$\n",
    "\n",
    "- 公式：\n",
    "\n",
    " <center>$dz^{[l]}=da^{[l]} * g^{[l]}{'}(z^{[l]})\\\\dW^{[l]}=dz^{[l]}\\cdot a^{[l-1]}\\\\db^{[l]}=dz^{[l]}\\\\da^{[l-1]}=W^{[l]}{^T}\\cdot dz^{[l]}$</center>\n",
    "\n",
    "$da^{[l-1]}代入dz^{[l]}$ ，有： \n",
    "\n",
    "<center>$dz^{[l]}=W^{[l+1]}{^T}\\cdot dz^{[l+1]}* g^{[l]}{'}(z^{[l]})$</center>\n",
    "\n",
    "- **向量化：**\n",
    "\n",
    "<center>$dZ^{[l]}=dA^{[l]} * g^{[l]}{'}(Z^{[l]})\\\\dW^{[l]}=\\dfrac{1}{m}dZ^{[l]}\\cdot A^{[l-1]}\\\\db^{[l]}=\\dfrac{1}{m}np.sum(dZ^{[l]},axis=1,keepdims = True)\\\\dA^{[l-1]}=W^{[l]}{^T}\\cdot dZ^{[l]}$</center>\n",
    "\n",
    "上面这部分可以结合 [Coursera | Andrew Ng (01-week-3-3.9)—神经网络的梯度下降法](http://blog.csdn.net/junjun_zhao/article/details/79002219)  以及 [Coursera | Andrew Ng (01-week-3-3.10)—直观理解反向传播](http://blog.csdn.net/junjun_zhao/article/details/79003068) 回忆一下 中间省略的求导过程。\n",
    "\n",
    "大致提一下：\n",
    "\n",
    "$a=\\sigma{(z)}=\\dfrac{1}{1+e^{-z}}$\n",
    "\n",
    "$\\sigma{(z)}{'}=a(1-a)$\n",
    "\n",
    "又因为 $da=\\dfrac{-y}{a}+\\dfrac{(1-y)}{1-a}$ \n",
    "\n",
    "且 $dz=\\dfrac{dL}{da} \\cdot \\dfrac{da}{dz}=da \\cdot g(z)^{'}$\n",
    "\n",
    "剩下的再根据上面两个链接去推导就可以了。\n",
    "\n",
    "反向传播辅助理解：已知输入 $da^{[l]}$, 最终要求得的是$da^{[l-1]}$，但是中间需要先求出 $dz^{[l]}$,然后才可以得出$dw^{[l]}$,$db^{[l]}$,最终求得$da^{[l-1]}$\n",
    "\n",
    "---\n",
    "## <font color=#0099ff>4.3 Forward propagation  in a  deep  neural network 深层网络中的前向传播\n",
    "\n",
    "\n",
    " $x=a^{[0]}$\n",
    "\n",
    "<center>$z^{[1]}=w^{[1]}a^{[0]}+b^{[1]}\\\\a^{[1]}=g^{[1]}(z^{[1]})\\\\z^{[2]}=w^{[2]}a^{[1]}+b^{[2]}\\\\a^{[2]}=g^{[2]}(z^{[2]})\\\\ .\\\\.\\\\.\\\\z^{[4]}=w^{[4]}a^{[3]}+b^{[4]}\\\\a^{[4]}=g^{[4]}(z^{[4]})=\\hat{y}$</center>\n",
    "\n",
    "基本规律：\n",
    "\n",
    "<center>$z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}\\\\a^{[l]}=g^{[l]}(z^{[l]})$</center>\n",
    "\n",
    "**向量化：**\n",
    "\n",
    " $X=A^{[0]}$\n",
    "\n",
    "<center>$Z^{[1]}=W^{[1]}A^{[0]}+b^{[1]}\\\\A^{[1]}=g^{[1]}(Z^{[1]})\\\\Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}\\\\A^{[2]}=g^{[2]}(Z^{[2]})\\\\ .\\\\.\\\\.\\\\Z^{[4]}=W^{[4]}A^{[3]}+b^{[4]}\\\\A^{[4]}=g^{[4]}(Z^{[4]})=\\hat{Y}$</center>\n",
    "\n",
    "基本规律：\n",
    "\n",
    "<center>$Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}\\\\A^{[l]}=g^{[l]}(Z^{[l]})$</center>\n",
    "\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180110170500845?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "---\n",
    "## <font color=#0099ff>4.4  Getting your matrix dimensions  right (核对矩阵的维数)\n",
    "    \n",
    "![这里写图片描述](http://img.blog.csdn.net/20180111133846722?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "<center>$z^{[1]}=w^{[1]}\\cdot X+b^{[1]}\\\\(3,1) ←(3, 2) * (2,1)\\\\(n^{[1]},1)←(n^{[1]},n^{[0]}) (n^{[0]},1)$</center>\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180111135316334?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "center>$w^{[1]}:(n^{[1]},n^{[0]})\\\\w^{[2]}:(5,3)   , (n^{[2]},n^{[1]})\\\\z^{[2]}=w^{[2]}\\cdot a^{[1]}+b^{[2]}\\\\(5,1)←(5,3)(3,1)$</center>\n",
    "\n",
    "<center>$w^{[3]}:(4,5)\\\\w^{[4]}:(2,4)\\\\w^{[5]}:(1,2)$</center>\n",
    "\n",
    "**Summary：**\n",
    "\n",
    "<center>$w^{[l]}:(n^{[l]},n^{[l-1]})$</center>\n",
    "\n",
    "<center>$b^{[1]}:(3,1) (n^{[1]},1)\\\\b^{[2]}:(5,1) (n^{[2]},1)$</center>\n",
    "\n",
    "**Summary：**\n",
    "\n",
    "<center>$b^{[l]}:(n^{[l]},1)$</center>\n",
    "\n",
    "\n",
    "<center>$dw^{[l]}:(n^{[l]},n^{[l-1]})\\\\db^{[l]}:(n^{[l]},1)$</center>\n",
    "\n",
    "$z^{[l]}=g^{[l]}(a^{[l]})$    $z$ 和 $a$ 的维度应该相等\n",
    "\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180111142849278?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "<center>$z^{[1]}=w^{[1]}\\cdot x+ b^{[1]}\\\\(n^{[1]},1)←(n^{[1]},n^{[0]})(n^{[0]},1) (n^{[1]},1)$</center>\n",
    "\n",
    "**向量化：**\n",
    "\n",
    "<center>$Z^{[1]}=W^{[1]}\\cdot X+ b^{[1]}\\\\(n^{[1]},m)←(n^{[1]},n^{[0]})(n^{[0]},m) (n^{[1]},1)$</center>\n",
    "\n",
    "其中 $b^{[1]}:(n^{[1]},1)$ 经 Python broadcasting  →$(n^{[1]},m)$\n",
    "\n",
    "\n",
    "<center>${center}z^{[l]},a^{[l]}:(n^{[l]},1)\\\\Vectoried:\\\\Z^{[l]},A^{[l]}:(n^{[l]},m)\\\\l=0  A^{[0]}=X=(n^{[0]},m)\\\\dZ,dA:(n^{[l]},m)$</center>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---    \n",
    "## <font color=#0099ff>4.5 Why deep representations? 为什么使用深层表示\n",
    "\n",
    "**为什么使用深层表示**\n",
    "\n",
    "\n",
    "人脸识别和语音识别：\n",
    "\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180111152308404?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "对于人脸识别，神经网络的第一层从原始图片中提取人脸的轮廓和边缘，每个神经元学习到不同边缘的信息；网络的第二层将第一层学得的边缘信息组合起来，形成人脸的一些局部的特征，例如眼睛、嘴巴等；后面的几层逐步将上一层的特征组合起来，形成人脸的模样。随着神经网络层数的增加，特征也从原来的边缘逐步扩展为人脸的整体，由整体到局部，由简单到复杂。层数越多，那么模型学习的效果也就越精确。\n",
    "\n",
    "对于语音识别，第一层神经网络可以学习到语言发音的一些音调，后面更深层次的网络可以检测到基本的音素，再到单词信息，逐渐加深可以学到短语、句子。\n",
    "\n",
    "所以从上面的两个例子可以看出随着神经网络的深度加深，模型能学习到更加复杂的问题，功能也更加强大。\n",
    "\n",
    "**电路逻辑计算：**\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180111161144110?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "假定计算异或逻辑输出：\n",
    "\n",
    "<center>$y=x_{1}\\oplus x_{2}\\oplus x_{3}\\oplus \\cdots\\oplus x_{n}$</center>\n",
    "\n",
    "\n",
    "对于该运算，若果使用深度神经网络，每层将前一层的相邻的两单元进行异或，最后到一个输出，此时整个网络的层数为一个树形的形状，网络的深度为 $O(log2(n))$，共使用的神经元的个数为：\n",
    "\n",
    "<center>$1+2+\\cdot+2^{\\log_{2}(n)-1}=1\\cdot \\dfrac{1-2^{\\log_{2}(n)}}{1-2}=2^{\\log_{2}(n)}-1=n-1$</center>\n",
    "\n",
    "即输入个数为 n，输出个数为 n-1。\n",
    "\n",
    "但是如果不适用深层网络，仅仅使用单隐层的网络（如右图所示），需要的神经元个数为 $2^{n−1}$ 个 。同样的问题，但是深层网络要比浅层网络所需要的神经元个数要少得多。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---     \n",
    "## <font color=#0099ff>4.6 Building blocks of deep neural networks 搭建深层神经网络块\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180112092418049?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "$w^{[l]}:=w^{[l]}-\\alpha \\cdot dw^{[l]}\\\\b^{[l]}:=b^{[l]}-\\alpha \\cdot  db^{[l]}$\n",
    "\n",
    "\n",
    "---\n",
    "## <font color=#0099ff>4.7 Parameters vs Hyperparameters (参数 VS 超参数)\n",
    "    \n",
    "\n",
    "**参数：**\n",
    "\n",
    "参数即是我们在过程中想要模型学习到的信息，$W^{[l]}，b^{[l]}$。\n",
    "\n",
    "**超参数：**\n",
    "\n",
    "超参数即为控制参数的输出值的一些网络信息，也就是超参数的改变会导致最终得到的参数 W^{[l]}，b^{[l]} 的改变。\n",
    "\n",
    "**举例：**\n",
    "\n",
    "学习速率：$α$\n",
    "\n",
    "迭代次数：$N$\n",
    "\n",
    "隐藏层的层数：$L$\n",
    "\n",
    "每一层的神经元个数：$n^{[1]}，n^{[2]},⋯$\n",
    "\n",
    "激活函数 $g(z)$ 的选择\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180112103255811?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "参考文献：\n",
    "\n",
    "[1]. 大树先生.[吴恩达Coursera深度学习课程 DeepLearning.ai 提炼笔记（1-4）-- 浅层神经网络](http://blog.csdn.net/koala_tree/article/details/78087711)\n",
    "\n",
    "---\n",
    " \n",
    "**PS: 欢迎扫码关注公众号：「SelfImprovementLab」！专注「深度学习」，「机器学习」，「人工智能」。以及 「早起」，「阅读」，「运动」，「英语 」「其他」不定期建群 打卡互助活动。**\n",
    "\n",
    "<center><img src=\"http://upload-images.jianshu.io/upload_images/1157146-cab5ba89dfeeec4b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
