{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2： Logistic Regression with a Neural Network mindset\n",
    "\n",
    "**原作者：Koala_Tree** [吴恩达Coursera深度学习课程 DeepLearning.ai 编程作业（1-2）](http://blog.csdn.net/koala_tree/article/details/78057033 )\n",
    " \n",
    " /* 本人在对原作者文章进行学习，文章中的部分中文解释，只是在重难点部分加深印象，稍微解释。*/\n",
    "\n",
    "### You will learn to: \n",
    "\n",
    "- Build the general architecture of a learning algorithm, including: 建立学习算法的总体架构\n",
    "    - Initializing parameters 初始化参数，数据\n",
    "    - Calculating the cost function and its gradient  计算代价函数，还有它的 梯度\\导数\n",
    "    - Using an optimization algorithm (gradient descent)  使用最优化算法，梯度下降\n",
    "    - Gather all three functions above into a main model function, in the right order. 将上面三个函数，使用正确的顺序整合到一个主函数中。\n",
    "\n",
    "## 1 - Packages\n",
    "\n",
    "First, let’s run the cell below to import all the packages that you will need during this assignment. \n",
    "\n",
    "- [numpy](http://www.numpy.org/) is the fundamental package for scientific computing with Python. numpy 是用来科学计算基本的库\n",
    "- [h5py](http://www.h5py.org/) is a common package to interact with a dataset that is stored on an H5 file. \n",
    "- [matplotlib](https://matplotlib.org/) is a famous library to plot graphs in Python. 绘画\n",
    "- [PIL](http://www.pythonware.com/products/pil/) and [scipy](https://www.scipy.org/) are used here to test your model with your own picture at the end. 最后的时候用自己的图片来测试模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_imaging'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-05f9803d8eb1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mndimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mlr_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program files\\python36\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;31m# Also note that Image.core is not a publicly documented interface,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;31m# and should be considered private and subject to change.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_imaging\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mPILLOW_VERSION\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'PILLOW_VERSION'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         raise ImportError(\"The _imaging extension was built for another \"\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '_imaging'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from lr_utils import load_dataset\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Overview of the Problem set\n",
    "\n",
    "**Problem Statement:** You are given a dataset (“data.h5”) containing: \n",
    "- a training set of m_train images labeled as cat (y=1) or non-cat (y=0) 一个训练集，有 m 个训练数据，并有标签 cat or  non-cat\n",
    "- a test set of m_test images labeled as cat or non-cat  一个测试集，images 对应的标签分类 cat or  non-cat\n",
    "- each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB). Thus, each image is square (height = num_px) and (width = num_px).\n",
    "\n",
    "You will build a simple image-recognition algorithm that can correctly classify pictures as cat or non-cat.\n",
    "你将创建一个简单的图像识别算法，可以正确分类图片 是猫或非猫\n",
    "\n",
    "Let’s get more familiar with the dataset. Load the data by running the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_dataset():\n",
    "    train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n",
    "    print(\"train_dataset:\",np.shape(train_dataset))\n",
    "    # train_dataset: (3,)\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features 训练集\n",
    "   \n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels 训练集对应的 标签集\n",
    "   \n",
    "    test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "    \n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes 测试集中 \n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset: (3,)\n",
      "train_set_x_orig: (209, 64, 64, 3)\n",
      "train_set_y: (1, 209)\n",
      "classes:\n",
      " [b'non-cat' b'cat']\n"
     ]
    }
   ],
   "source": [
    "# Loading the data (cat/non-cat)\n",
    "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()\n",
    "print(\"train_set_x_orig:\",np.shape(train_set_x_orig))\n",
    "print(\"train_set_y:\",np.shape(train_set_y))\n",
    "# print(\"train_set_x_orig:\\n\",train_set_x_orig,\"\\ntrain_set_y:\\n\",train_set_y)\n",
    "print(\"classes:\\n\",classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We added “_orig” at the end of image datasets (train and test) because we are going to preprocess 预处理 them. After preprocessing, we will end up with train_set_x and test_set_x (the labels train_set_y and test_set_y don’t need any preprocessing).\n",
    "\n",
    "Each line of your train_set_x_orig and test_set_x_orig is an array representing an image.是一个数组，代表图像 You can visualize an example by running the following code. Feel free also to change the `index` value and re-run to see other images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngYiFHwhMIGLhBwITiP8fjgdmcWXGD6AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25babff2748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_y[:, index]: [1]\n",
      "np.squeeze(1): 1\n",
      "classes[1]: b'cat'\n",
      "y = [1], it's a 'cat' picture.\n"
     ]
    }
   ],
   "source": [
    "# Example of a picture\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab \n",
    "index = 25\n",
    "# plt.imshow 绘制\n",
    "plt.imshow(train_set_x_orig[index])\n",
    "# 加上这句话 才显示图片\n",
    "pylab.show()\n",
    "# index 为 25 的训练数据，是否是 cat ,在 标签集中 str(train_set_y[:, index]) load_dataset() 方法中，已经使用reshape()将 y，label 标签集 转化为\n",
    "# （1，m）行向量，classes  中 存储的是易于理解的 文字显示 [b'non-cat' b'cat'] ，0 代表 non-cat  ，1 代表 cat\n",
    "\n",
    "print(\"train_set_y[:, index]:\",train_set_y[:, index])\n",
    "# train_set_y[:, index]: [1] \n",
    "print(\"np.squeeze(1):\",np.squeeze(1))\n",
    "# np.squeeze(1): 1\n",
    "print(\"classes[1]:\",classes[1])\n",
    "# classes[1]: b'cat'\n",
    "print (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"' picture.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape : (1, 3, 1)\n",
      "np.squeeze(x).shape : (3,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([[[0], [1], [2]]])  \n",
    "\n",
    "print(\"x.shape :\",x.shape)\n",
    "# x.shape : (1, 3, 1) 1个 数组 里面有 三行 一列的 数组 \n",
    "\n",
    "print(\"np.squeeze(x).shape :\",np.squeeze(x).shape)\n",
    "# np.squeeze(x).shape : (3,) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many software bugs in deep learning come from having matrix/vector dimensions that don’t fit.(在 DL 中很多 bug 都是因为矩阵或向量的维度不匹配， ) If you can keep your matrix/vector dimensions straight you will go a long way toward eliminating many bugs.（如果你能保证你的矩阵或向量的维度是正确的，就可以避免或消除很多BUG）\n",
    "\n",
    "**Exercise:** Find the values for: \n",
    "    - m_train (number of training examples) \n",
    "    - m_test (number of test examples) \n",
    "    - num_px (= height = width of a training image) \n",
    "Remember that `train_set_x_orig` is a numpy-array of shape (m_train, num_px, num_px, 3). For instance, you can access `m_train` by writing `train_set_x_orig.shape[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: m_train = 209\n",
      "Number of testing examples: m_test = 50\n",
      "Height/Width of each image: num_px = 64\n",
      "Each image is of size: (64, 64, 3)\n",
      "train_set_x shape: (209, 64, 64, 3)\n",
      "train_set_y shape: (1, 209)\n",
      "test_set_x shape: (50, 64, 64, 3)\n",
      "test_set_y shape: (1, 50)\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ### (≈ 3 lines of code)\n",
    "m_train = train_set_x_orig.shape[0] # 训练集 中的 数据 是从 原始数据中 .shape[0] 获取的，个数 行数 209\n",
    "m_test = test_set_x_orig.shape[0] # 50 个\n",
    "num_px = train_set_x_orig.shape[1] # 训练集 中的 数据 是从 原始数据中 .shape[1] 获取的 64* 64 像素\n",
    "### END CODE HERE ###\n",
    "\n",
    "print (\"Number of training examples: m_train = \" + str(m_train)) #训练样本个数\n",
    "print (\"Number of testing examples: m_test = \" + str(m_test)) #测试样本个数 \n",
    "print (\"Height/Width of each image: num_px = \" + str(num_px)) # 图片高宽像素\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\") # (64, 64, 3) 每一个图片都是 64*64 像素 3 代表 RGB\n",
    "print (\"train_set_x shape: \" + str(train_set_x_orig.shape)) #(209, 64, 64, 3) 一共 209 个样本，每个样本 都是 64*64 像素 3 代表 RGB\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape)) # train_set_y shape: (1, 209) 训练集对应标签\n",
    "print (\"test_set_x shape: \" + str(test_set_x_orig.shape)) # (50, 64, 64, 3)\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape)) # (1, 50) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, you should now reshape images of shape (num_px, num_px, 3) in a numpy-array of shape (num_px ∗ num_px ∗ 3, 1). （为了方便，你应该将 图片 的形状 (num_px, num_px, 3) 转化为一个 形状为 shape (num_px ∗ num_px ∗ 3, 1) 的numpy 数组 ，）After this, our training (and test) dataset is a numpy-array where each column represents a flattened image. There should be m_train (respectively m_test) columns.\n",
    "\n",
    "**Exercise:** Reshape the training and test data sets so that images of size (num_px, num_px, 3) are flattened into single vectors of shape (num_px ∗ num_px ∗ 3, 1).将 training and  test data 转化为 列向量\n",
    "\n",
    "A trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b∗c∗d, a) is to use:\n",
    "一个技巧，当你想把形状为(a,b,c,d) 准话为 X_flatten X 为基准的 平面 矩阵，使用  (b∗c∗d, a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_flatten = X.reshape(X.shape[0], -1).T      # X.T is the transpose of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x_flatten shape: (12288, 209)\n",
      "train_set_y shape: (1, 209)\n",
      "test_set_x_flatten shape: (12288, 50)\n",
      "test_set_y shape: (1, 50)\n",
      "sanity check after reshaping: [17 31 56 22 33]\n"
     ]
    }
   ],
   "source": [
    "# Reshape the training and test examples\n",
    "\n",
    "### START CODE HERE ### (≈ 2 lines of code)\n",
    "# reshape(m_train, -1) m_train 行，列数无限制 ，-1 代表不指定大小 \n",
    "train_set_x_flatten = train_set_x_orig.reshape(m_train, -1).T\n",
    "test_set_x_flatten = test_set_x_orig.reshape(m_test, -1).T\n",
    "### END CODE HERE ###\n",
    "\n",
    "print (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape)) # (12288, 209) \n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))# (12288, 50)\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))\n",
    "print (\"sanity check after reshaping: \" + str(train_set_x_flatten[0:5,0])) # [17 31 56 22 33] 第 0 列， 0 到 5 不包含5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To represent color images,为了代表图像的颜色 the red, green and blue channels (RGB) must be specified for each pixel,指明每一个像素 and so the pixel value is actually a vector of three numbers ranging from 0 to 255. 所以像素实际上是 三个 0 to 255 的数字\n",
    "\n",
    "One common preprocessing step in machine learning is to center and standardize your dataset, （在ML 中常用的一种预处理过程，就是标准化数据集）meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. （求平均值，然后除以标准差）But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel).对于图片数据集来说，更简单方便和有效的是每行除以255\n",
    "\n",
    "Let’s standardize our dataset. 标准化数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x: [[ 0.06666667  0.76862745  0.32156863 ...,  0.56078431  0.08627451\n",
      "   0.03137255]\n",
      " [ 0.12156863  0.75294118  0.27843137 ...,  0.60784314  0.09411765\n",
      "   0.10980392]\n",
      " [ 0.21960784  0.74509804  0.26666667 ...,  0.64705882  0.09019608\n",
      "   0.20784314]\n",
      " ..., \n",
      " [ 0.          0.32156863  0.54117647 ...,  0.33333333  0.01568627  0.        ]\n",
      " [ 0.          0.31372549  0.55294118 ...,  0.41960784  0.01960784  0.        ]\n",
      " [ 0.          0.31764706  0.55686275 ...,  0.58431373  0.          0.        ]]\n",
      "test_set_x: [[ 0.61960784  0.45098039  1.         ...,  0.16078431  0.07058824\n",
      "   0.52156863]\n",
      " [ 0.40784314  0.43137255  0.99215686 ...,  0.18431373  0.07058824\n",
      "   0.63921569]\n",
      " [ 0.3254902   0.43529412  0.99607843 ...,  0.32941176  0.0627451\n",
      "   0.29411765]\n",
      " ..., \n",
      " [ 0.67843137  0.67058824  0.52156863 ...,  0.71764706  0.56470588\n",
      "   0.01960784]\n",
      " [ 0.50196078  0.69019608  0.39607843 ...,  0.55294118  0.5372549\n",
      "   0.08627451]\n",
      " [ 0.43137255  0.72941176  0.4745098  ...,  0.45490196  0.42352941\n",
      "   0.01960784]]\n"
     ]
    }
   ],
   "source": [
    "train_set_x = train_set_x_flatten/255.\n",
    "test_set_x = test_set_x_flatten/255.\n",
    "print(\"train_set_x:\",train_set_x)\n",
    "print(\"test_set_x:\",test_set_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What you need to remember:\n",
    "\n",
    "Common steps for pre-processing a new dataset are: 对一个新数据集进行预处理的常见步骤如下：\n",
    "- Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, …) 想出计算出问题的规模维度形状\n",
    "- Reshape the datasets such that each example is now a vector of size (num_px * num_px * 3, 1) 最数据集矩阵形变，矩阵变维\n",
    "- “Standardize” the data ，标准化数据集\n",
    "\n",
    "# 3 - General Architecture of the learning algorithm 算法的一般体系结构\n",
    "\n",
    "It’s time to design a simple algorithm to distinguish cat images from non-cat images.\n",
    "是时候去设计一个简单算法去区分一张图片 is cat or non-cat\n",
    "You will build a Logistic Regression, using a Neural Network mindset. The following Figure explains **why Logistic Regression is actually a very simple Neural Network!**\n",
    "你将用一个神经网络去构建一个Logistic Regression,下面将会解释，为什么 Logistic Regression 就是一个简单的神经网络。\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/laobadao/Deep-Learning/master/homework/Jupyter/jupyter_note/pic/cat.png\">\n",
    "\n",
    "**Mathematical expression of the algorithm:**\n",
    "\n",
    "For one example $x^{(i)}$:\n",
    "\n",
    "$z^{(i)} = w^T x^{(i)} + b \\tag{1}$\n",
    "\n",
    "$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$\n",
    "\n",
    "$\\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$\n",
    "\n",
    "The cost is then computed by summing over all training examples: \n",
    "\n",
    "$J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}$\n",
    "\n",
    "**Key steps: \n",
    "In this exercise, you will carry out the following steps: ** 关键步骤，执行如下步骤\n",
    " \n",
    "- Initialize the parameters of the model 初始化模型的参数\n",
    "- Learn the parameters for the model by minimizing the cost 通过为模型最小化损失 学习参数\n",
    "- Use the learned parameters to make predictions (on the test set) 使用这个学习参数，相当于权重，去做预测，在测试集上\n",
    "- Analyse the results and conclude 分析结果并做分析\n",
    "\n",
    "# 4 - Building the parts of our algorithm  构建算法的各个部分\n",
    "\n",
    "The main steps for building a Neural Network are: 构建神经网络的主要步骤如下\n",
    "1. Define the model structure (such as number of input features)  定义模型结构 例如输入的特征\n",
    "2. Initialize the model’s parameters 初始化参数\n",
    "3. Loop: 循环计算\n",
    "    - Calculate current loss (forward propagation) 计算当前损失  正向传播\n",
    "    - Calculate current gradient (backward propagation) 计算当前梯度（导数），反向传播\n",
    "    - Update parameters (gradient descent) 更新参数，梯度下降\n",
    "\n",
    "You often build 1-3 separately and integrate them into one function we call model().通常将上面三个步骤分别创建，然后再 model() 中结合使用 \n",
    "\n",
    "## 4.1 - Helper functions \n",
    "\n",
    "**Exercise:** Using your code from “Python Basics”, implement `sigmoid()`. As you’ve seen in the figure above, you need to compute $sigmoid( w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$  to make predictions. Use np.exp()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "sigmoid([0, 2]) = [ 0.5         0.88079708]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# def sigmoid(x):\n",
    "#     s = 1.0 / (1 + 1/ np.exp(x))\n",
    "#     return s\n",
    "\n",
    "# print(sigmoid(0))\n",
    "\n",
    "\n",
    "# GRADED FUNCTION: sigmoid\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "    注释要写详细，这是个好习惯\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size. 输入参数可以是一个标量或者是 任意大小的 numpy 数组\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    s = 1.0/(1+np.exp(-z))\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return s\n",
    "\n",
    "print(sigmoid(0))  \n",
    "print (\"sigmoid([0, 2]) = \" + str(sigmoid(np.array([0,2]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Initializing parameters\n",
    "\n",
    "**Exercise:** Implement parameter initialization in the cell below. You have to initialize `w `as a vector of zeros. 初始化 w 为一个 0 矩阵，If you don’t know what numpy function to use, look up `np.zeros() `in the Numpy library’s documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]] b: 0\n"
     ]
    }
   ],
   "source": [
    "# def initialize_w(size):\n",
    "#     w = np.zeros((size,1))\n",
    "#     return w\n",
    "# print(initialize_w(5))\n",
    "\n",
    "# GRADED FUNCTION: initialize_with_zeros\n",
    "\n",
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "\n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "\n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    w = np.zeros((dim, 1))\n",
    "    b = 0\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Andrew Ng 经常说的，为了避免 bug 可以使用 assert 语句去检查其形状大小是否正确    \n",
    "    assert(w.shape == (dim, 1))\n",
    "    #  是否是 float 或 int  类型实例    \n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b\n",
    "\n",
    "w ,b = initialize_with_zeros(10)\n",
    "print(\"w:\",str(w),\"b:\",str(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For image inputs, w will be of shape (num_px × num_px × 3, 1). 对于图像输入来说 W 的 shape 应该是 （num_px * num_px * 3,1）\n",
    "\n",
    "## 4.3 - Forward and Backward propagation 正向传播和反向传播\n",
    "\n",
    "Now that your parameters are initialized, you can do the “forward” and “backward” propagation steps for learning the parameters.\n",
    "现在你已经将参数初始化完成， 可以进行 正向和反向传播的步骤 去进行 “学习参数”\n",
    "\n",
    "**Exercise:** Implement a function `propagate()` that computes the cost function and its gradient. 实现 传播 函数，并计算它的代价\\成本函数和它的梯度(包括 dw,db)。\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "Forward Propagation: \n",
    "- You get X 获取 X 矩阵 数据集 \n",
    "- You compute $A = \\sigma(w^T X + b) = (a^{(0)}, a^{(1)}, ..., a^{(m-1)}, a^{(m)})$\n",
    "- You calculate the cost function:$J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n",
    "\n",
    "Here are the two formulas you will be using:\n",
    "\n",
    "$\\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def propagate(w,b,X,Y):\n",
    "#     \"\"\"\n",
    "#     X matrix (n,m) m 列数\n",
    "#     \"\"\"\n",
    "#     # 矩阵 内积 np.dot\n",
    "#     A = sigmoid(np.dot(w.T,X)+ b) \n",
    "#     # !!!计算 m, X 矩阵的列数   \n",
    "#     m = X.shape[1] \n",
    "#     # 1 通常用 float 类型 都写成 1.0    \n",
    "#     cost = (-1.0/m) * np.sum(Y*np.log(A) + (1-Y)*np.log(1-A))\n",
    "#     # 矩阵相乘 np.dot 内积 矩阵 内积     \n",
    "#     dw = (1.0/m) * np.dot(X,(A-Y).T)\n",
    "    \n",
    "#     db = (1.0/m) * np.sum(A-Y)\n",
    "    \n",
    "#     grads = {\"dw\": dw,\n",
    "#              \"db\": db}\n",
    "    \n",
    "#     return cost,grads\n",
    "\n",
    "# w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\n",
    "                   \n",
    "# cost,grads = propagate(w, b, X, Y)\n",
    "                   \n",
    "# print (\"dw = \" + str(grads[\"dw\"]))\n",
    "# print (\"db = \" + str(grads[\"db\"]))\n",
    "# print (\"cost = \" + str(cost))\n",
    "\n",
    "# GRADED FUNCTION: propagate\n",
    "\n",
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "    实现 代价函数 和 梯度\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1) 权重\n",
    "    b -- bias, a scalar 偏差 标量\n",
    "    X -- data of size (num_px * num_px * 3, number of examples) 数据集\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples) 标签集\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression 负的 对数似然函数\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "\n",
    "    Tips:\n",
    "    - Write your code step by step for the propagation. np.log(), np.dot() 使用 np.log(), np.dot() np.sum()\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[1] # m 列 m 个训练样本\n",
    "\n",
    "    # FORWARD PROPAGATION (FROM X TO COST) 正向传播\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    A = sigmoid(np.dot(w.T, X)+b)                                    # compute activation 计算激活函数\n",
    "    cost = -(1.0/m)*np.sum(Y*np.log(A)+(1-Y)*np.log(1-A))                                 # compute cost 计算代价函数\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD) 反向传播\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    dw = (1.0/m)*np.dot(X,(A-Y).T)\n",
    "    db = (1.0/m)*np.sum(A-Y)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "\n",
    "    return grads, cost\n",
    "    \n",
    "\n",
    "w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\n",
    "grads, cost = propagate(w, b, X, Y)\n",
    "# print (\"dw = \" + str(grads[\"dw\"]))\n",
    "# print (\"db = \" + str(grads[\"db\"]))\n",
    "# print (\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Optimization 最优化\n",
    "\n",
    "You have initialized your parameters.\n",
    "\n",
    "You are also able to compute a cost function and its gradient.\n",
    "Now, you want to update the parameters using gradient descent.\n",
    "使用梯度下降 更新参数\n",
    "\n",
    "**Exercise:** Write down the optimization function. The goal is to learn w and b by minimizing the cost function $ J $. For a parameter $\\theta $, the update rule is $\\theta = \\theta - \\alpha \\text{ } d\\theta$ , where $\\alpha$ is the learning rate. 学习率\n",
    "最优化函数 ，目标是 通过最小化代价函数 去学习更新 w 和 b,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [[ 0.19033591]\n",
      " [ 0.12259159]]\n",
      "b = 1.92535983008\n",
      "dw = [[ 0.67752042]\n",
      " [ 1.41625495]]\n",
      "db = 0.219194504541\n"
     ]
    }
   ],
   "source": [
    "# def optimize(w,b,X,Y,max_iter,learning_rate,print_cost = False):\n",
    "#     一般初始化的变量放在最开头\n",
    "#     costs = [] \n",
    "#     grads, cost = propagate(w, b, X, Y)\n",
    "#     for i in range(max_iter):\n",
    "        \n",
    "#         dw = grads[\"dw\"]\n",
    "#         db = grads[\"db\"]\n",
    "#         w -= learning_rate * dw\n",
    "#         b -= learning_rate * db\n",
    "#         if i % 100 ==0:\n",
    "#                 costs.append(cost)\n",
    "#         if print_cost and i % 100 == 0:\n",
    "#             print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "            \n",
    "#     params = {\"w\": w,\n",
    "#               \"b\": b}\n",
    "\n",
    "#     grads = {\"dw\": dw,\n",
    "#              \"db\": db}\n",
    "\n",
    "#     return params, grads, costs\n",
    "\n",
    "# GRADED FUNCTION: optimize\n",
    "\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop 最大迭代次数\n",
    "    learning_rate -- learning rate of the gradient descent update rule 梯度下降的 更新规则 学习速率\n",
    "    print_cost -- True to print the loss every 100 steps 每迭代 100 次 打印 loss 损失\n",
    "\n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b 字典 包含权重和 偏差\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "\n",
    "    Tips:\n",
    "    You basically need to write down two steps and iterate through them: 基本写2步就可以进行迭代\n",
    "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
    "        2) Update the parameters using gradient descent rule for w and b.\n",
    "    \"\"\"\n",
    "\n",
    "    costs = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "\n",
    "        # Cost and gradient calculation (≈ 1-4 lines of code)\n",
    "        ### START CODE HERE ### \n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "\n",
    "        # update rule (≈ 2 lines of code)\n",
    "        ### START CODE HERE ###\n",
    "        w = w - learning_rate*dw\n",
    "        b = b - learning_rate*db\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Record the costs 记录 cost 代价 成本\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "\n",
    "        # Print the cost every 100 training examples\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "\n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "\n",
    "    return params, grads, costs\n",
    "\n",
    "\n",
    "params, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n",
    "\n",
    "print (\"w = \" + str(params[\"w\"]))\n",
    "print (\"b = \" + str(params[\"b\"]))\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** The previous function will output the learned w and b. (之前的函数将会输出 学习后的参数 w and b )We are able to use w and b to predict the labels for a dataset X. (用 w and b 去预测一个 数据集 X 的 label)Implement the `predict()` function. There is two steps to computing predictions:实现预测函数，用两步去实现\n",
    "\n",
    "Calculate $\\hat{Y} = A = \\sigma(w^T X + b)$ \n",
    "\n",
    "Convert the entries of a into 0 (if activation <= 0.5) or 1 (if activation > 0.5),根据激活函数值是 大于还是小于 0.5 ,去判断 a 属于 0 还是1   stores the predictions in a vector `Y_prediction`.存储预测值到一个向量中 If you wish, you can use an `if/else` statement in a `for` loop (though there is also a way to vectorize this).\n",
    "\n",
    "**注意：（n,m） n 是 n 行，代表 X 有 n 个特性，m 是 m  列 代表有 m 个训练样本**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions = [[ 1.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"def predict(w,b,X):\n",
    "    # Error: 错误记录\n",
    "    # 1.没有初始化  Y_prediction 作为一个向量，题目要求中已经明确指出来了，要初始化为一个向量，存储所有的 转化后的 y 值,\n",
    "    # 2.X 是一个数据集，就是 要对 所有的 X 中的数据集做预测，则会有 对应的 行数的 Y 向量 在存储\n",
    "    # 3.A 是做完预测后 返回的 激活函数的 一个向量集合 是一个 [0,1]  区间的概率，要根据 每个元素概率大小再去 转化为 0 或 1，且存到 向量中 \n",
    "    # 变量名经常写错，要经常用 np.zeros() and np.reshape()       \n",
    "    Y_prediction =np.zeros((1,X.shape[1]))\n",
    "    w = w.reshape(X.shape[0],1)\n",
    "    A = sigmoid(np.dot(w.T,X)+b)\n",
    "    print(\"A:\",A)\n",
    "    print(\"A.shape:\",A.shape)\n",
    "    for i in range(A.shape[1]):\n",
    "        if A[0,i] > 0.5:\n",
    "            Y_prediction[0,i] = 1\n",
    "        else:\n",
    "            Y_prediction[0,i] = 0\n",
    "    assert(Y.shape ==(1,X.shape[1]))\n",
    "    return Y_prediction\n",
    "\n",
    "w = np.array([[0.1124579],[0.23106775]])\n",
    "b = -0.3\n",
    "X = np.array([[1.,-1.1,-3.2],[1.2,2.,0.1]])\n",
    "print (\"predictions = \" + str(predict(w, b, X)))\"\"\"\n",
    "\n",
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    使用 学习后的 逻辑回归参数 w,b 去预测 标签是 0 还是1\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X,一个 数组向量 包含所有的对于 X 的预测值\n",
    "    '''\n",
    "\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "#     print(\"w.shape:\",w.shape)\n",
    "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    ### END CODE HERE ###\n",
    "#     print(\"A:\",A)\n",
    "#     print(\"A.shape:\",A.shape)\n",
    "# A: [[ 0.52241976  0.50960677  0.34597965]]\n",
    "# A.shape: (1, 3)\n",
    "    for i in range(A.shape[1]):\n",
    "\n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        ### START CODE HERE ### (≈ 4 lines of code)\n",
    "        if A[0,i] > 0.5:\n",
    "            Y_prediction[0,i] = 1\n",
    "        else:\n",
    "            Y_prediction[0,i] = 0\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "\n",
    "    return Y_prediction\n",
    "\n",
    "w = np.array([[0.1124579],[0.23106775]])\n",
    "b = -0.3\n",
    "X = np.array([[1.,-1.1,-3.2],[1.2,2.,0.1]])\n",
    "# print(\"X.shape:\",X.shape)\n",
    "# X.shape: (2, 3)\n",
    "# 注意：（n,m） n 是 n 行，代表 X 有 n 个特性，m 是 m  列 代表有 m 个训练样本\n",
    "print (\"predictions = \" + str(predict(w, b, X)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What to remember: **\n",
    "\n",
    "**You’ve implemented several functions that: \n",
    "- Initialize (w,b)  初始化参数 w and b\n",
    "- Optimize the loss iteratively to learn parameters (w,b):  最优化损失 迭代的去学习 参数 w and b \n",
    "- computing the cost and its gradient  计算 成本函数 和它的 梯度\n",
    "- updating the parameters using gradient descent 使用 梯度下降更新参数\n",
    "- Use the learned (w,b) to predict the labels for a given set of examples 使用学习后的 最佳拟合参数 w and b 去对给出的数据集做预测** \n",
    "\n",
    "## 5 - Merge all functions into a model 将所有函数合并到一个 model\n",
    "\n",
    "You will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order.\n",
    "\n",
    "**Exercise:** Implement the model function. Use the following notation: \n",
    "- Y_prediction for your predictions on the test set \n",
    "- Y_prediction_train for your predictions on the train set \n",
    "- w, costs, grads for the outputs of optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "\n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train) 训练集数据\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train) 训练集标签\n",
    "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test) 测试集数据\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test) 测试集标签\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters 最大迭代次数\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize() 学习速率\n",
    "    print_cost -- Set to true to print the cost every 100 iterations 每100次 打印一下 cost \n",
    "\n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model. 返回一个字典 包含 model 的相关信息\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # initialize parameters with zeros (≈ 1 line of code) 初始化 w,b X_train.shape[0] 是 行数，每个 x 的 n 个特征，每个特征对应要有 一个w 权重 拟合参数\n",
    "    # w ,b 其实就是 对x 的 每一个 特征 x_1_(1)  x1 样本的 第一个特征,进行拟合 给出 w 和 偏移 b 所以是跟 x 的行数 也就是 x 有多少个 特征有关\n",
    "    w, b = initialize_with_zeros(X_train.shape[0])\n",
    "\n",
    "    # Gradient descent (≈ 1 line of code) w and b 的梯度 dw and db ,以及 代价成本\n",
    "    # 最优化后的 最佳拟合参数 w and b     \n",
    "    #       params = {\"w\": w,\n",
    "    #               \"b\": b}\n",
    "\n",
    "    #     grads = {\"dw\": dw,\n",
    "    #              \"db\": db}\n",
    "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "\n",
    "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "\n",
    "    # Predict test/train set examples (≈ 2 lines of code)\n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Print train/test Errors 打印 训练集 和 测试集中 错误率\n",
    "    #  Y_prediction_train - Y_train 相同则 相减为 0 ,b不同 则相减 为 |1| np.abs（） 取绝对值 为错误个数 100 - errors 为正确率\n",
    "    # np.mean 求平均值     \n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "    \n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to train your model. 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: 0.584508\n",
      "Cost after iteration 200: 0.466949\n",
      "Cost after iteration 300: 0.376007\n",
      "Cost after iteration 400: 0.331463\n",
      "Cost after iteration 500: 0.303273\n",
      "Cost after iteration 600: 0.279880\n",
      "Cost after iteration 700: 0.260042\n",
      "Cost after iteration 800: 0.242941\n",
      "Cost after iteration 900: 0.228004\n",
      "Cost after iteration 1000: 0.214820\n",
      "Cost after iteration 1100: 0.203078\n",
      "Cost after iteration 1200: 0.192544\n",
      "Cost after iteration 1300: 0.183033\n",
      "Cost after iteration 1400: 0.174399\n",
      "Cost after iteration 1500: 0.166521\n",
      "Cost after iteration 1600: 0.159305\n",
      "Cost after iteration 1700: 0.152667\n",
      "Cost after iteration 1800: 0.146542\n",
      "Cost after iteration 1900: 0.140872\n",
      "train accuracy: 99.04306220095694 %\n",
      "test accuracy: 70.0 %\n"
     ]
    }
   ],
   "source": [
    "d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的 cost 成本代价值观察可得：训练的数据越多，其 w and b 参数拟合行越好，损失降低，最后正确率相对还可以不过测试集中正确率较低。\n",
    "\n",
    "**Comment:** Training accuracy is close to 100%. This is a good sanity check: your model is working and has high enough capacity to fit the training data. Test error is 68%. It is actually not bad for this simple model, given the small dataset we used and that logistic regression is a linear classifier. But no worries, you’ll build an even better classifier next week!\n",
    "\n",
    "Also, you see that the model is clearly overfitting the training data. Later in this specialization you will learn how to reduce overfitting, for example by using regularization. Using the code below (and changing the index variable) you can look at predictions on pictures of the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
