{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursera | Andrew Ng (04-week1)—convolutional neural networks\n",
    "\n",
    "在吴恩达深度学习视频以及大树先生的博客提炼笔记基础上添加个人理解，原大树先生博客可查看该链接地址[大树先生的博客](http://blog.csdn.net/koala_tree)- ZJ\n",
    "    \n",
    ">[Coursera 课程](https://www.coursera.org/specializations/deep-learning) |[deeplearning.ai](https://www.deeplearning.ai/) |[网易云课堂](https://mooc.study.163.com/smartSpec/detail/1001319001.htm)\n",
    "\n",
    "---\n",
    "   **转载请注明作者和出处：ZJ 微信公众号-「SelfImprovementLab」**\n",
    "   \n",
    "   [CSDN]()：\n",
    "   \n",
    "   [知乎](https://zhuanlan.zhihu.com/c_147249273)：https://zhuanlan.zhihu.com/c_147249273\n",
    "   \n",
    "\n",
    "---\n",
    "## **Convolutional Neural Networks (卷积神经网络)**\n",
    "\n",
    "### **Week 1: 卷积神经网络基础**\n",
    "\n",
    "####**1.1 计算机视觉 （Computer Vision）**\n",
    "\n",
    "神经网络让人们兴奋的两点原因：\n",
    "\n",
    "- 新型应用的产生。\n",
    "- 想象力和创造力产生新的神经网络结构与算法 ，启发计算机视觉与其他领域的交叉成果。\n",
    "\n",
    "计算机视觉（Computer Vision）包含很多不同类别的问题，如图片分类 （Image Clasification）、目标检测 (Object detection)、图片风格迁移 (Neural Style Transfer)等等。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180129083640215?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "计算机视觉 （CV） 的一个挑战：\n",
    "\n",
    "对于小尺寸的图片问题，也许我们用深度神经网络的结构可以较为简单的解决一定的问题。但是当应用在大尺寸的图片上，输入规模将变得十分庞大，使用神经网络将会有非常多的参数需要去学习，这个时候神经网络就不再适用。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180129083807956?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "面对数据输入规模大的解决方法：\n",
    "\n",
    "卷积神经网络（Convolutional Neural Networks）在计算机视觉问题上是一个非常好的网络结构。\n",
    "\n",
    "####**1.2 边缘检测示例 （Edge detection example）**\n",
    "\n",
    "**卷积运算是卷积神经网络的基本组成部分。**\n",
    "\n",
    "前面的视频提到过的几个步骤，1. 如何检测边缘（前几层）。2. 检测到物体的某一部分（中间几层）。3.检测到整个物体（最后几层）。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180129091847688?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "接下来重点讲解：如何检测一张图片的边缘。下面以边缘检测的例子来介绍卷积运算。\n",
    "\n",
    "所谓边缘检测，在下面的图中，分别通过垂直边缘检测（verticla edges）和水平边缘检测 (horizontal edgeds) 得到不同的结果：\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180129084037544?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "**垂直边缘检测：**\n",
    "\n",
    "假设对于一个 6×6 大小的图片（以数字表示），以及一个 3×3 大小的 filter（卷积核） 进行卷积运算，以“∗”符号表示。图片和垂直边缘检测器分别如左和中矩阵所示：\n",
    "\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180129084111311?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "filter 不断地和其大小相同的部分做对应元素的乘法运算并求和，(从左上到右下，从左到右，每次移动一格，之后再从最左下一个，在从左向右，循环)最终得到的数字相当于新图片的一个像素值，如右矩阵所示，最终得到一个 4×4 大小的图片。（6×6 ∗ 3×3  = 4×4）\n",
    "\n",
    "```\n",
    "#不同库的代码的卷积运算：\n",
    "Python: conv_forword\n",
    "tensorflow: tf.nn.conv2d\n",
    "keras: conv2D\n",
    "```\n",
    "\n",
    "**边缘检测的原理：**\n",
    "\n",
    "以一个有一条垂直边缘线的简单图片来说明。通过垂直边缘 filter 我们得到的最终结果图片可以明显地将边缘和非边缘区分出来：\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180129084133666?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "卷积运算提供了一个方便的方法来检测图像中的边缘，成为卷积神经网络中重要的一部分。\n",
    "\n",
    "####**1.3 更多的边缘检测内容（More edge detection）**\n",
    "\n",
    "如何区分正边和负边，从亮到暗，从暗到亮，边缘的过度。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180129094538512?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "<center>**亮到暗**</center>\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180129094547866?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "<center>**暗到亮**</center>\n",
    "\n",
    "**垂直和水平边缘检测**\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180129084753237?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "**更复杂的filter**\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180129084807367?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "对于复杂的图片，我们可以直接将 filter 中的数字直接看作是需要学习的参数，其可以学习到对于图片检测相比上面 filter 更好的更复杂的 filter，如相对于水平和垂直检测器，我们训练的 filter 参数也许可以知道不同角度的边缘。其他应用的过滤器，sobel filter ，scharr filter。\n",
    "\n",
    "通过卷积运算（convolution operation），在卷积神经网络中通过**反向传播算法**，去学习这 9 个参数 可以学习到相应于目标结果的 filter  （$w_1,w_2, w_3, ...w_8, w_9, $），将其应用于整个图片，输出其提取到的所有有用的特征。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180129084835030?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "**卷积和互相关：** （这部分视频在 1.5 中）\n",
    "\n",
    "在数学定义上，矩阵的卷积（convolution）操作为首先将卷积核同时在水平和垂直方向上进行翻转，构成一个卷积核的镜像，然后使用该镜像再和前面的矩阵进行移动相乘求和操作。如下面例子所示：\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180129084855962?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "在深度学习中，我们称为的卷积运算实则没有卷积核变换为镜像的这一步操作，因为在权重学习的角度，变换是没有必要的。深度学习的卷积操作在数学上准确度来说称为**互相关（cross-correlation）**。\n",
    "\n",
    "####**1.4 Padding**\n",
    "\n",
    "**没有 Padding 的缺点：**\n",
    "\n",
    "- 每次卷积操作，图片会缩小； \n",
    "就前面的例子来说，6×6 大小的图片，经过 3×3 大小的 filter，缩小成了 4×4 大小 ,神经网络层数越多，则最后输出的图片会很小。\n",
    "图片：$n×n –> (n−f+1)×(n−f+1)$\n",
    "- 角落和边缘位置的像素进行卷积运算的次数少，可能会丢失有用信息。\n",
    "\n",
    "其中，n 表示图片的长或宽的大小，f 表示 filter 的长或宽的大小。\n",
    "\n",
    "为了解决上面两个问题：\n",
    "\n",
    "**加 Padding：**\n",
    "\n",
    "为了解决上面的两个缺点，我们在进行卷积运算前为图片加 padding，包围角落和边缘的像素，使得通过 filter 的卷积运算后，图片大小不变，也不会丢失角落和边沿的信息。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180129095446763?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "以 p 表示 Padding 的值，则输入 n×n 大小的图片，最终得到的图片大小为$ (n+2p−f+1)×(n+2p−f+1)$，为使图片大小保持不变，需根据 filter 的大小调整 p 的值。\n",
    "\n",
    "**Valid / Same 卷积：**\n",
    "\n",
    "- Valid：no padding；$（n×n –> (n−f+1)×(n−f+1)$\n",
    "- Same：padding，输出与输入图片大小相同，\n",
    "$ (n+2p−f+1)×(n+2p−f+1) –> n×n  $   $(p=(f−1)/2)$。在计算机视觉中，一般来说 padding 的值为奇数 （3×3，5×5，7×7）（因为filter一般为奇数）\n",
    "\n",
    "####**1.5 卷积步长（Strided convolutions）**\n",
    "\n",
    "**卷积的步长是构建卷积神经网络的一个基本的操作。**\n",
    "\n",
    "如前面的例子中，我们使用的 stride=1，每次的卷积运算以 1 个步长进行移动。下面是 stride=2 时对图片进行卷积的结果：\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180129102421192?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "以 s 表示 stride 的大小，那么在进行卷积运算后，图片的变化为：n×n –> $\\left\\lfloor \\dfrac{n+2p-f}{s}+1 \\right\\rfloor\\times \\left\\lfloor \\dfrac{n+2p-f}{s}+1 \\right\\rfloor$\n",
    "\n",
    "注意，在当 padding≠1 时，若移动的窗口落在图片外面时，则不要再进行相乘的操作，丢弃边缘的数值信息，所以输出图片的最终维度为向下取整。\n",
    "\n",
    "**卷积和互相关：**\n",
    "\n",
    "在数学定义上，矩阵的卷积（convolution）操作为首先将卷积核同时在水平和垂直方向上进行翻转，构成一个卷积核的镜像，然后使用该镜像再和前面的矩阵进行移动相乘求和操作。如下面例子所示：\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180129084855962?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "在深度学习中，我们称为的卷积运算实则没有卷积核变换为镜像的这一步操作，因为在权重学习的角度，变换是没有必要的。深度学习的卷积操作在数学上准确度来说称为**互相关（cross-correlation）**。\n",
    "\n",
    "####1.6 立体卷积 ：卷积中的“卷”体现之处 （Convolutions over volumes）\n",
    "\n",
    "引子：从二维到三维。6×6×3 * 3×3×3 ->   4×4 \n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180129104628961?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "原理：\n",
    "\n",
    "**卷积核的通道数：**\n",
    "\n",
    "对于灰色图像中，卷积核和图像均是二维的。而应用于彩色图像中，因为图片有 R、G、B 三个颜色通道 (channel)，所以此时的卷积核应为三维卷积核。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180129104225592?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "卷积核的第三个维度需要与进行卷积运算的图片的通道数相同。\n",
    "\n",
    "**多卷积核：**\n",
    "\n",
    "单个卷积核应用于图片时，提取图片特定的特征，不同的卷积核提取不同的特征。如两个大小均为 3×3×3 的卷积核分别提取图片的垂直边缘和水平边缘。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180129104247336?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "由图可知，最终提取到彩色图片的垂直特征图和水平特征图，得到有 2 个通道的 4×4 大小的特征图片 即： 4×4×2。\n",
    "\n",
    "**Summary：**\n",
    "\n",
    "图片：$n×n×nc ∗ f×f×nc —>n−f+1×n−f+1×n′c$  no padding, stride=1\n",
    "其中，$n_c$ 表示通道的数量，$n'_{c}$表示下一层的通道数，同时也等于本层卷积核的个数。\n",
    "\n",
    "####**1.7 单层卷积网络 (One layer of convlutional network)**\n",
    "\n",
    "单层卷积网络的例子：\n",
    "\n",
    "和普通的神经网络单层前向传播的过程类似，卷积神经网络也是一个先由输入和权重及偏置做线性运算，然后得到的结果输入一个激活函数中，得到最终的输出：\n",
    "\n",
    "<center>$z^{[1]}=w^{[1]}a^{[0]}+b^{[1]}\\\\a^{[1]}=g(z^{[1]})$</center>\n",
    "\n",
    "不同点是在卷积神经网络中，权重和输入进行的是卷积运算。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180129111005479?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "单层卷积的参数个数：\n",
    "\n",
    "在一个卷积层中，如果我们有10个 3×3×3 大小的卷积核，那么加上每个卷积核对应的偏置 (bias)，则对于一个卷积层，我们共有的参数个数为：\n",
    "\n",
    "<center>$(3×3×3+1)×10=280$</center>\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180129111037579?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "无论图片大小是多少，该例子中的卷积层参数个数一直都是 280 个，相对于普通的神经网络，卷积神经网络的参数个数要少很多。\n",
    "\n",
    "标记的总结：\n",
    "\n",
    "如果 $l$表示一个卷积层:\n",
    "\n",
    "- $f^{[l]}$: filter 的大小；\n",
    "- $p^{[l]}$:padding；\n",
    "- $s^{[l]}$：步长（stride）；\n",
    "- 卷积核(过滤器 filter )的个数：$n^{[l]}_{C}$\n",
    "- filter 大小：$f^{[l]}\\times f^{[l]}\\times n^{[l-1]}_{C}$\n",
    "- 激活值（Activations）：$a^{[l]}—>n^{[l]}_{H}\\times n^{[l]}_{W}\\times n^{[l]}_{C}$\n",
    "- 权重（Weights）：$f^{[l]}\\times f^{[l]}\\times n^{[l-1]}_{C}\\times n^{[l]}_{C}$ 其中,$n^{[l]}_{C}$ 是 $l$ 层 filter 的个数\n",
    "- 偏置（bias）：$n^{[l]}_{C}——(1,1,1,n^{[l]}_{C})$\n",
    "- Input：$n^{[l-1]}_{H}\\times n^{[l-1]}_{W}\\times n^{[l-1]}_{C}$\n",
    "- Output：$n^{[l]}_{H}\\times n^{[l]}_{W}\\times n^{[l]}_{C}$\n",
    "- 其中，$n^{[l]}_{H} = \\left\\lfloor \\dfrac{n^{[l-1]}_{H}+2p^{[l]}-f^{[l]}}{s^{[l]}}+1 \\right\\rfloor$ \n",
    "$n^{[l]}_{W} = \\left\\lfloor \\dfrac{n^{[l-1]}_{W}+2p^{[l]}-f^{[l]}}{s^{[l]}}+1 \\right\\rfloor$\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180129111601707?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "####**1.8 简单卷积网络示例：(A simple convolution network example)**\n",
    "\n",
    "多层卷积构成卷积神经网络，下面是一个卷积神经网络的例子：\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180129111710168?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "一般情况下，图片的大小随着网络的加深逐渐减小，filters 随着层级增多，逐渐加大。\n",
    "\n",
    "卷积网络层的类型：\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180129135700229?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "- 卷积层（Convolution layers），Conv；\n",
    "- 池化层（Pooling layers），Pool；\n",
    "- 全连接层（Fully connected）：Fc；\n",
    "\n",
    "####**1.9 池化层 （Pooling layers）**\n",
    "\n",
    "池化层 （Pooling layers）缩减模型大小，提高计算速度。提高所提取特征的鲁棒性。\n",
    "\n",
    "最大池化（Max pooling）：\n",
    "\n",
    "最大池化是对前一层得到的特征图进行池化减小，仅由**当前小区域内的最大值**来代表最终池化后的值。\n",
    "\n",
    "二维的最大化池演示。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180129140112674?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "$n_c$维：$n_c$个信道中的每个信道都单独执行最大化池计算。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180129140218499?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "在最大池化中，有一组超参数需要进行调整，其中，f 表示池化的大小，s 表示步长。一般情况下，p = 0,因为其目的就是缩减模型大小，所以不会增加 padding。 \n",
    "\n",
    "- 池化前：n × n;\n",
    "- 池化后：$\\left\\lfloor \\dfrac{n+2p-f}{s}+1 \\right\\rfloor\\times \\left\\lfloor \\dfrac{n+2p-f}{s}+1 \\right\\rfloor$\n",
    "\n",
    "**平均池化（Average pooling）：**\n",
    "\n",
    "平均池化与最大池化唯一不同的是其选取的是小区域内的**均值**来代表该区域内的值。用到的比较少。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180129140132741?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "**池化 Summary：**\n",
    "\n",
    "池化层的超参数：\n",
    "\n",
    "- f：filter的大小； 一般常用的是 f=2, s=2 ,or  f=3 ,s=2\n",
    "- s：stride大小；\n",
    "- 最大池化或者平均池化；\n",
    "- p：padding，这里要注意，几乎很少使用。\n",
    "- 注意，池化层没有需要学习的参数。No parameters to learn!\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180129140150794?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "####**1.10 卷积神经网络示例 (Convolutional neural network example)**\n",
    "\n",
    "这里以 LeNet-5 为例，给出一个完整的卷积神经网络。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180129143004499?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "Guideline :尽量不要自己设置 hyperparameters ,而是查看文献中别人设置的 hyperparameters ，选一个在别人任务中效果很好的架构。\n",
    "\n",
    "**构建深度卷积的模式：**\n",
    "\n",
    "- 随着网络的深入，提取的特征图片大小（$n_H, n_W$）将会逐渐减小，但同时通道数量($n_C$)应随之增加；\n",
    "\n",
    "- 常见模式：Conv——Pool——Conv——Pool——Fc——Fc——Fc——softmax。\n",
    "\n",
    "**卷积神经网络的参数：**\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180129143300819?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "根据上表我们可以看出，对于卷积卷积神经网络的参数：\n",
    "\n",
    "- 在卷积层，仅有少量的参数；\n",
    "- 在池化层，没有参数；\n",
    "- 在全连接层，存在大量的参数。\n",
    "\n",
    "随着神经网络的加深，激活值会逐渐减少。\n",
    "\n",
    "神经网络的基本构造模块已经讲完了，一个卷积神经网络包括，卷几层，池化层，全连接层，经验：找到整合基本构造模块的最好的方法是大量阅读别人的案例。\n",
    "\n",
    "####**1.11为什么使用卷积？（Why convolutions?）**\n",
    "\n",
    "**参数少的优势：**\n",
    "\n",
    "与普通的全连接神经网络相比，卷积神经网络的参数更少。如图中的例子，卷积神经网络仅有6×(5×5+1)=156个参数，而普通的全连接网络有3072×4704≈14M 个参数。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180129143331339?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "- **参数共享：**一个特征检测器（filter）对图片的一部分有用的同时也有可能对图片的另外一部分有用。\n",
    "- **连接的稀疏性：**在每一层中，每个输出值只取决于少量的输入。减少过拟合的情况。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180129143357709?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "**训练卷积神经网络：**\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180129143346431?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "我们将训练集输入到卷积神经网络中，对网络进行训练。利用梯度下降（Adam、momentum RMSprop 等优化算法）最小化代价函数来寻找网络最优的参数。\n",
    "\n",
    "参考文献：\n",
    "\n",
    "[1]. 大树先生.[吴恩达Coursera深度学习课程 DeepLearning.ai 提炼笔记（4-1）-- 卷积神经网络基础](http://blog.csdn.net/koala_tree/article/details/78458067)\n",
    "\n",
    "\n",
    "---\n",
    " \n",
    "**PS: 欢迎扫码关注公众号：「SelfImprovementLab」！专注「深度学习」，「机器学习」，「人工智能」。以及 「早起」，「阅读」，「运动」，「英语 」「其他」不定期建群 打卡互助活动。**\n",
    "\n",
    "<center><img src=\"http://upload-images.jianshu.io/upload_images/1157146-cab5ba89dfeeec4b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\"></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
