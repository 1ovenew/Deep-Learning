{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursera | Andrew Ng (01-week-3-3.3)—Computing a Neural Networks's Output \n",
    "\n",
    "该系列仅在原课程基础上部分知识点添加个人学习笔记，或相关推导补充等。如有错误，还请批评指教。在学习了 Andrew Ng 课程的基础上，为了更方便的查阅复习，将其整理成文字。因本人一直在学习英语，所以该系列以英文为主，同时也建议读者以英文为主，中文辅助，以便后期进阶时，为学习相关领域的学术论文做铺垫。- ZJ\n",
    "    \n",
    ">[Coursera 课程](https://www.coursera.org/specializations/deep-learning) |[deeplearning.ai](https://www.deeplearning.ai/) |[网易云课堂](https://mooc.study.163.com/smartSpec/detail/1001319001.htm)\n",
    "\n",
    "---\n",
    "   **转载请注明作者和出处：ZJ 微信公众号-「SelfImprovementLab」**\n",
    "   \n",
    "   [知乎](https://zhuanlan.zhihu.com/c_147249273)：https://zhuanlan.zhihu.com/c_147249273\n",
    "   \n",
    "   [CSDN]()：\n",
    "   \n",
    "\n",
    "---\n",
    "3.3 Computing a Neural Networks's Output  计算神经网络的输出 \n",
    "\n",
    "\n",
    "在上一期的视频中我们已经见过(字幕来源：网易云课堂)\n",
    "In the last video you saw\n",
    "\n",
    "，单隐层神经网络长什么样\n",
    ",what a single hidden layer neural network looks like\n",
    "\n",
    "，在这期的视频中让我们了解\n",
    ",in this video let's go through the details of exactly\n",
    "\n",
    "，神经网络的输出 究竟是如何计算出来的\n",
    ",how this neural network computers outputs\n",
    "\n",
    "，你所看到的是像Logistic那样的运算过程\n",
    ",what you see is that is like logistic regression\n",
    "\n",
    "，但整个运算过程会重复很多遍\n",
    ",but repeat of all the times\n",
    "\n",
    "，看一下 这是一个两层的神经网络\n",
    ",let's take a look so this is what's a two layer neural network\n",
    "\n",
    "，让我们更深入地了解神经网络到底在计算什么\n",
    ",let's go more deeply into exactly what this neural network compute\n",
    "，我们之前说过 Logistic回归\n",
    ",now we've said before that logistic regression\n",
    "\n",
    "，这里的圆圈代表了回归计算的两个步骤\n",
    ",the circle images the regression really represents two steps of computation\n",
    "\n",
    "，首先你按步骤计算出z\n",
    ",first you compute z as follows\n",
    "\n",
    "，然后在第二步计算激活函数\n",
    ",and in second you compute the activation\n",
    "\n",
    "，就是函数sigmoid(z)\n",
    ",as a sigmoid function of z\n",
    "\n",
    "，所以神经网络只不过重复计算这些步骤很多次\n",
    ",so a neural network just does this a lot more times\n",
    "\n",
    "，我们先来看\n",
    ",let's start by focusing on just\n",
    "\n",
    "，隐层的其中一个节点\n",
    ",one of the nodes in the hidden layer\n",
    "\n",
    "，看看这个隐层的第一个节点\n",
    ",and this look at the first node in the hidden layer\n",
    "\n",
    "，我暂时先隐去其他的节点\n",
    ",so I've grayed out the other nodes for now\n",
    "\n",
    "，左边看上去和Logistic回归很相似\n",
    ",so similar to logistic regression on the left\n",
    "\n",
    "，隐层的这个节点进行两步计算\n",
    ",this node in a hidden layer does two steps of computation\n",
    "\n",
    "，第一步 我们可以看成是节点的左边\n",
    ",right the first step we think it's as the left half of this node\n",
    "\n",
    "，计算z = w^T x + b\n",
    ",it computes Z equals W transpose X plus B\n",
    "\n",
    "，这些我们会用到的标记\n",
    ",and the notation we'll use is um\n",
    "\n",
    "，这些都是和第一隐层有关的量\n",
    ",these are all quantities associated with the first hidden layer\n",
    "\n",
    "，所以才用了那么多[1]上标\n",
    ",so that's why we have a bunch of square brackets there\n",
    "\n",
    "，这是隐层的第一个节点\n",
    ",and this is the first node in the hidden layer\n",
    "\n",
    "，所以我们有个下标1\n",
    ",so that's why we have the subscript one over there\n",
    "\n",
    "，第一步就是这样\n",
    ",so first it does that\n",
    "\n",
    "，然后第二步是计算\n",
    ",and then a second step is it computes\n",
    "\n",
    "，a[1]1 = sigmoid(Z^[1]1) 就像这样\n",
    ",a11 equals sigmoid of z11 like so\n",
    "\n",
    "，所以对于z和a 按符号约定写成\n",
    ",so for both z and a the notational convention is that\n",
    "\n",
    "，写成a^[l]_i\n",
    ",a Li the L here in superscript square brackets\n",
    "\n",
    "，这里上标方括号表示层数\n",
    ",refers to layer number\n",
    "\n",
    "，而下标i则表示层中的第几个节点\n",
    ",and the i subscript here refers to the nodes in that layer\n",
    "\n",
    "，我们看的是第一隐层的第一个节点\n",
    ",so the node we will be looking at is layer 1 that is a hidden layer node 1\n",
    "\n",
    "，所以有上标 和下标都是1 1\n",
    " ,so that's why the superscript and subscript were on both 1 1\n",
    "\n",
    "，所以这个小圆圈 即神经网络的第一个节点\n",
    ",so that little circle that first node in neural network\n",
    "\n",
    "，表示执行这两步计算\n",
    ",represents carrying out these two steps of computation\n",
    "\n",
    "，现在让我们看看神经网络的第二个节点\n",
    ",now let's look at the second node in neural network\n",
    "\n",
    "，即神经网络中隐层的第二个节点\n",
    ",the second node in the hidden layer of in neural network\n",
    "\n",
    "，与左边的Logistic回归单元类似\n",
    ",similar to the logistic regression unit on the left\n",
    "\n",
    "，这个小圆圈代表了计算的两个步骤\n",
    ",this little circle represents two steps of computation\n",
    "\n",
    "，第一步是计算z\n",
    ",the first step is compute z\n",
    "\n",
    "，这还是在第一层 但变成第二个节点了\n",
    ",this is still layer 1 but now the second node\n",
    "\n",
    "，等于w^T x + b^[1]_2\n",
    ",equals W transpose x plus b^[1]_2\n",
    "\n",
    "，然后a[1]2 = sigmoid(Z^[1]2)\n",
    ",and then a 12 equals Sigma z12\n",
    "\n",
    "，再次 需要的话 可以暂停视频仔细看看\n",
    ",and again feel free to pause the video if you want\n",
    "\n",
    "，这样你就可以再次查看\n",
    ",but you can double check that\n",
    "\n",
    "，标记的上标和下标\n",
    ",the superscript and subscript notation\n",
    "\n",
    "，和我们上面所写的是保持一致的\n",
    ",is consistent with what we have written here above in purple\n",
    "\n",
    "，所以我们已经讨论了神经网络的前两个隐层单元\n",
    ",so we've talk through the first two hidden units in the neural network\n",
    "\n",
    "，第三四个隐藏单元\n",
    ",on hidden units three and four\n",
    "\n",
    "，也表示同样的计算\n",
    ",also represents some computations\n",
    "\n",
    "，现在让我们把这对等式\n",
    ",so now let me take this pair of equations\n",
    "\n",
    "，还有这对等式\n",
    ",and this pair of equations\n",
    "\n",
    "，把它们复制到下一个幻灯片中\n",
    ",and let's copy them to the next slide\n",
    "\n",
    "，这是我们的神经网络\n",
    ",so here's our network\n",
    "\n",
    "，这是第一个等式 这是第二个等式\n",
    ",and here's the first and there's a second equations\n",
    "\n",
    "，它们之前已经在隐层的\n",
    ",they were worked on previously for\n",
    "\n",
    "，第一二个节点中用过了\n",
    ",the first and the second hidden units\n",
    "\n",
    "，如果你接下去看并且写出相应的等式\n",
    ",if you then go through and write out the corresponding equations\n",
    "\n",
    "，对应于第三 第四个隐层单元\n",
    ",for the third and fourth hidden units\n",
    "\n",
    "，你就会得到下面的这些等式\n",
    ",you get the following\n",
    "\n",
    "，我确认一下你弄懂了这些符号\n",
    ",and let's make sure this notation is clear\n",
    "\n",
    "，这是向量w^[1]_1 这是向量的转置乘以 x\n",
    ",this is the vector W^[1]_1 this is a vector transpose times x ok\n",
    "\n",
    "，那个上标T\n",
    ",so that's what the superscript T there\n",
    "\n",
    "，表示向量转置\n",
    ",represents this is a vector transpose\n",
    "\n",
    "，现在就像你可能所猜想的那样\n",
    ",now as you might have guessed\n",
    "\n",
    "，如果你确实在神经网络中执行\n",
    ",if you're actually implementing in neural network doing this\n",
    "\n",
    "，用for循环来做这些看起来真的很低效\n",
    ",with a for loop seems really inefficient\n",
    "\n",
    "，所以接下来我们要做的就是\n",
    ",so what we're going to do is\n",
    "\n",
    "，把这四个等式向量化\n",
    ",take these four equations and vectorize\n",
    "\n",
    "，我将展示如何把z看做向量计算\n",
    ",so I'm going to start by showing how to compute z as a vector\n",
    "\n",
    "，结果显示 你可以这么做\n",
    ",it turns out you could do it as follows\n",
    "\n",
    "，让我们把这些w堆起来 构成一个矩阵\n",
    ",let me take these WS and stack them into a matrix\n",
    "\n",
    "，然后你就有W^[1]1转置\n",
    ",then you have W 1 1 transpose\n",
    "\n",
    "，所以这是这个行向量 是一个列向量的转置\n",
    ",so that say a row vector oh that's a column vector transpose\n",
    "\n",
    "，变为一个行向量\n",
    ",gives you a row vector\n",
    "\n",
    "，然后 W^[1]2转置 W^[1]3转置以及W^[1]4的转置\n",
    ",then W 1 2 transpose W 1 3 transpose and W 1 4 transpose\n",
    "\n",
    "，把这四个w向量堆叠在一起\n",
    ",and so this by stacking those four W vectors together\n",
    "\n",
    "，你会得出一个矩阵\n",
    ",you end up with a matrix\n",
    "\n",
    "，另一个看待这个的方法是\n",
    ",so another way to think of this is that\n",
    "\n",
    "，我们有四个Logistic回归单元\n",
    ",we have for logistic regression unions there\n",
    "\n",
    "，而每一个Logistic回归单元\n",
    ",and each of the logistic regression unions\n",
    "\n",
    "，都有对应的参数 向量w\n",
    ",have a corresponding parameter vector w\n",
    "\n",
    "，把这四个向量堆叠在一起\n",
    ",and by stacking those four vectors together\n",
    "\n",
    "，你会得出这个4 × 3 的矩阵\n",
    ",you end up with this four by three matrix\n",
    "\n",
    "，然后如果你把这个矩阵\n",
    ",so if you then take this matrix and multiply it\n",
    "\n",
    "，乘以你的输入特征 x1 x2 x3 你会得出\n",
    ",by your input features x1 x2 x3 you end up with\n",
    "\n",
    "，通过矩阵乘法你可以得出\n",
    ",by our matrix multiplication works you end up with\n",
    "\n",
    "，w^[1]_1转置x w^[1]_2转置x w^[1]_3转置x w^[1]_4转置x\n",
    ",w^[1]_1 transpose x w^[1]_2 transpose X of w^[1]_3 transpose x w^[1]_4 transpose x\n",
    "\n",
    "，然后别忘记了b\n",
    ",and then let's not forget the bs\n",
    "\n",
    "，让我们现在加上一个向量\n",
    ",so we now add to this a vector\n",
    "\n",
    "，b^[1]1 b^[1]2 b^[1]3 b^[1]4 所以它们看起来这样\n",
    ",b[1]1 b[1]2 b[1]3 b[1]4 so that they see this\n",
    "\n",
    "，然后 b^[1]1 b^[1]2 b^[1]3 b^[1]4\n",
    ",then this is b[1]1 b[1]2 b[1]3 b[1]4\n",
    "\n",
    "，然后你会看到这四行的结果\n",
    ",and so you see that each of the four rows of this outcome\n",
    "\n",
    "，恰好对应于这四行\n",
    ",correspond exactly to each of these four rows\n",
    "\n",
    "，对应于上面的这四个等式\n",
    ",of each these four quantities that we had above\n",
    "\n",
    "，换句话说 我们刚刚展示了\n",
    ",so in other words we've just shown that\n",
    "\n",
    "，这个东西是等于Z^[1]1 Z^[1]2 Z^[1]3 Z^[1]4\n",
    ",this thing is therefore equal to Z11 Z12 Z13 Z14\n",
    "\n",
    "，就如之前在这定义的\n",
    ",right as defined here\n",
    "\n",
    "，这可能并不奇怪 我们将\n",
    ",and maybe not surprisingly we're going to\n",
    "\n",
    "，把这整个东西称作向量Z^[1]\n",
    ",call this whole thing the vector Z1\n",
    "\n",
    "，我们是把单独的z堆叠起来构成一个列向量Z^[1]的\n",
    ",which is taken by stacking up these um individuals of z into a column vector\n",
    "\n",
    "，当我们向量化时一条经验法则\n",
    ",when we're vectorizing one of the rules of thumb\n",
    "\n",
    "，可能帮助你找到方向\n",
    ",that might help you navigate this\n",
    "\n",
    "，就是当我们在一层中有不同的节点\n",
    ",is that when we have different nodes in a layer\n",
    "\n",
    "，那就纵向地堆叠起来\n",
    ",we stack them vertically\n",
    "\n",
    "，所以这里有z^[1]1~Z^[1]4\n",
    ",so that's why when you have z^[1]1 to z^[1]4\n",
    "\n",
    "，对应隐层4个不同的节点\n",
    ",those correspond to four different nodes in the hidden layer\n",
    "\n",
    "，我们把这四个数竖向堆叠起来\n",
    ",and so we stack these four numbers vertically\n",
    "\n",
    "，得到向量Z^[1]\n",
    ",to form the vectors Z1\n",
    "\n",
    "，如果用另一种符号惯例来表示\n",
    ",and to use one more piece of notation\n",
    "\n",
    "，这个4×3的矩阵是我们通过\n",
    ",this 4 by 3 matrix here\n",
    "\n",
    "，堆叠 W^[1]1 W^[1]2 等等 形成的\n",
    ",which we obtained by stacking the lower case you know W11 W12 and so on\n",
    "\n",
    "，我们将这个矩阵称为大写W^[1]\n",
    ",we're going to call this matrix W capital 1\n",
    "\n",
    "，类似的这个矩阵\n",
    ",and similarly this vector\n",
    "\n",
    "，我们称为b^[1]\n",
    ",we going to call b superscript 1 square bracket\n",
    "\n",
    "，所以这是个4×1的向量\n",
    ",and so this is a 4 by 1 vector\n",
    "\n",
    "，所以现在我们使用矩阵表示来计算Z\n",
    ",so now we've computed Z using this vector matrix notation\n",
    "\n",
    "，最后一件需要做的事是计算这些a的值\n",
    ",the last thing we need to do is also compute these values of a\n",
    "\n",
    "，所以你应该不会惊讶\n",
    ",and so probably won't surprise you to see\n",
    "\n",
    "，我们要把a[1]定义为\n",
    ",that we're going to define a1\n",
    "\n",
    "，a[1]1~a[1]4这些激活值的堆叠\n",
    ",as just stacking together those activation values a11 to a14\n",
    "\n",
    "，所以把这四个值堆叠起来\n",
    ",so just take these 4 values and stack them together\n",
    "\n",
    "，称为一个向量a^[1]\n",
    ",in a vector called a^[1]\n",
    "\n",
    "，这里就会有sigmoid(Z^[1])\n",
    ",and this is going to be sigmoid of z1\n",
    "\n",
    "，这里面 它应用\n",
    ",where there's been implementation of the sigmoid function\n",
    "\n",
    "，sigmoid函数作用于Z的四个元素\n",
    ",that takes in the four elements of Z\n",
    "\n",
    "，也就相当于把sigmoid函数作用到Z的每个元素\n",
    ",and applies the sigmoid function element wise to it\n",
    "\n",
    "，概括一下 我们发现\n",
    ",so just a recap we figured out that\n",
    "\n",
    "，Z^[1] 等于 W^[1] × X + b^[1]\n",
    ",z1 is equal to W 1 times the vector X plus the vector B1\n",
    "\n",
    "，而a[1] = sigmoid(Z^[1])\n",
    ",and a^[1] a 1 is sigmoid of z1\n",
    "\n",
    "，让我们把这些复制到下一个幻灯片 可以看到\n",
    ",let's just copy this to the next slide and what we see is that\n",
    "\n",
    "，对于神经网络的第一层 给予一个输入X\n",
    ",for the first layer of the neural network given an input X\n",
    "\n",
    "，我们得出z^[1] = W^[1]·x + b^[1]\n",
    ",we have that z^[1] = W^[1]·x + b^[1]\n",
    "\n",
    "，而a[1] = sigmoid(Z^[1]) 而它的维度是4×1\n",
    ",and a 1is sigmoid of Z^[1] and the dimensions of this are 4 by 1\n",
    "\n",
    "，等于 这是一个4×3的矩阵乘以1一个3×1的向量\n",
    ",equals this is a 4 by 3 matrix times a 3 by 1 vector\n",
    "\n",
    "，加上一个4×1的向量b  这个同样是4×1的维度\n",
    ",plus a 4by 1 vector B and this is 4 by 1 same dimensions\n",
    "\n",
    "，记得我们说过 x等于a[0]\n",
    ",and remember that we said x is equal to a 0\n",
    "\n",
    "，就像y帽等于a[2]一样\n",
    ",right just like y hat is also equal to a 2\n",
    "\n",
    "，如果你确实想把x\n",
    ",so if you want you can actually take this X\n",
    "\n",
    "，用a[0]代替\n",
    ",and replace it with a 0\n",
    "\n",
    "，因为a^[0]可以作为输入特征x 这个向量的别名\n",
    ",since a^[0] is if you want as an alias for the vector of input feature x\n",
    "\n",
    "，用同样的方法推导\n",
    ",now through a similar derivation you can figure out\n",
    "\n",
    "，下一层的表示\n",
    ",that the representation for the next layer\n",
    "\n",
    "，可以写成类似的形式\n",
    ",can also be written similarly\n",
    "\n",
    "，而输出层的作用是\n",
    ",well what the output layer does is\n",
    "\n",
    "，它带参数W^[2] b^[2]\n",
    " ,it has associated with it so the parameters W^[2] and b^[2]\n",
    "\n",
    "，这里的W^[2]就是一个1×4的矩阵\n",
    ",so W 2 in this case is going to be a 1 by 4 matrix\n",
    "\n",
    "，而b^[2]就是一个实数 即1×1矩阵\n",
    ",and B 2 is just a real number as 1 by 1 and\n",
    "\n",
    "，所以Z^[2]是一个实数 即一个1×1的矩阵\n",
    ",so Z^[2] is going to be a real numbers right as a 1 by 1 matrix\n",
    "\n",
    "，这里就是一个 1×4的矩阵乘以 a 4×1向量 加上b^[2] 1×1\n",
    ",is going to be a 1 by 4 thing times a was 4 by 1 plus B2 is 1 by 1\n",
    "\n",
    "，这最后得出了一个实数\n",
    ",and so this gives you just a real number\n",
    "\n",
    "，如果你把这最后的输出单元\n",
    ",and if you think of this last output unit\n",
    "\n",
    "，看作是Logistic回归的类似物\n",
    ",as just being analogous to logistic regression\n",
    "\n",
    "，它有着参数W和b\n",
    ",which had parameters W and b\n",
    "\n",
    "，W实际上是类似于W^[2]转置\n",
    ",on W really plays analogous role to W^[2] transpose\n",
    "\n",
    "，W^[2]其实是W转置就是W b则等于b^[2]\n",
    ",or W^[2] is really W transpose and b is equal to b^[2]\n",
    "\n",
    "，就像你所知道的\n",
    ",right similar to you know\n",
    "，把网络左边部分盖住 先忽略这些\n",
    ",cover up the left of this network and ignore all that for now\n",
    "\n",
    "，那么这最后的输出单元\n",
    ",then this is just this last output uni\n",
    "，就像Logistic回归一样\n",
    ",there's a lot like logistic regression\n",
    "\n",
    "，不过我们不再把参数写成W和b\n",
    ",except that instead of writing the parameters as W and b\n",
    "\n",
    "，而是写成W^[2] 和 b^[2]\n",
    ",we're writing them as W^[2] and b^[2]\n",
    "\n",
    "，其维度分别为1×4和1×1\n",
    ",with dimensions one by four and one by one so\n",
    "\n",
    "，归纳一下 对于Logistic回归\n",
    ",just a recap for logistic regression\n",
    "\n",
    "，为了计算输出或者说预测\n",
    ",to implement the output or implement prediction\n",
    "\n",
    "，你要计算z = w^T x + b\n",
    ",you compute z = w^T x + b\n",
    "\n",
    "，和 y帽 = a = sigmoid(z)\n",
    ",and a y hat equals a equals sigmoid of z\n",
    "\n",
    "，当你有一个单隐层神经网络\n",
    ",when you have a neural network who have one hidden layer\n",
    "\n",
    "，你需要去在代码中实现的是\n",
    ",what you need to implement to computers output\n",
    "\n",
    "，计算这四个等式\n",
    ",is just the four equations\n",
    "\n",
    "，且你可以把这看成是\n",
    ",and you can think of this as\n",
    "\n",
    "，一个向量化的计算过程 计算出这四个\n",
    ",a vectorized implementation of computing the output of\n",
    "\n",
    "，四个隐层中的Logistic回归单元\n",
    ",first these four logistical regression units in a hidden layer\n",
    "\n",
    "，这就是这两个等式做的\n",
    ",that's what this does\n",
    "\n",
    "，而这个输出层的logistic回归\n",
    ",and then this logistic regression in the output layer\n",
    "\n",
    "，就是用这两个等式算的\n",
    ",which is what this does\n",
    "\n",
    "，我希望这些描述易于理解\n",
    ",I hope this description made sense\n",
    "\n",
    "，但总的说来要想计算神经网络的输出\n",
    ",but takeaway is to compute the output of this neural network\n",
    "\n",
    "，你所需要的只是这四行代码\n",
    ",all you need is those four lines of code\n",
    "\n",
    "，现在 你知道如何输入单个特征向量x\n",
    ",so now you've seen how given a single input feature vector x\n",
    "\n",
    "，你可以运用四行代码计算出 这个神经网络的输出\n",
    ",you can with four lines of code compute the outputs of this neural Network\n",
    "\n",
    "，和当时我们处理Logistic回归时的做法类似\n",
    ",um similar to what we did for logistic regression\n",
    "\n",
    "，我们也想把整个训练样本都向量化\n",
    ",We will also want to vectorize across multiple training examples\n",
    "\n",
    "，我们会发现\n",
    ",and we'll see that\n",
    "\n",
    "，通过把不同训练样本堆叠起来构成矩阵\n",
    ",by stacking up training examples in different column in the matrix\n",
    "\n",
    "，只需稍微修改这些公式\n",
    ",or just slight modification to this\n",
    "\n",
    "，你可以得到类似之前Logistic回归的结果\n",
    ",you also similar to what you saw in which is regression\n",
    "\n",
    "，能够同时计算出 不止一个样本的神经网络输出\n",
    ",be able to compute the output of this neural network not just on one example at a time\n",
    "\n",
    "，而是能一次性计算你的整个训练集\n",
    ",but to your say your entire training set at a time\n",
    "\n",
    "，让我们在下一期中了解这些细节\n",
    ",so let's see the details of that in the next video\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### <font color=#0099ff>重点总结：</font>\n",
    "\n",
    "\n",
    "参考文献：\n",
    "\n",
    "[1]. 大树先生.[吴恩达Coursera深度学习课程 DeepLearning.ai 提炼笔记（1-3）-- 浅层神经网络](http://blog.csdn.net/koala_tree/article/details/78059952)\n",
    "\n",
    "\n",
    "---\n",
    " \n",
    "**PS: 欢迎扫码关注公众号：「SelfImprovementLab」！专注「深度学习」，「机器学习」，「人工智能」。以及 「早起」，「阅读」，「运动」，「英语 」「其他」不定期建群 打卡互助活动。**\n",
    "\n",
    "<center><img src=\"http://upload-images.jianshu.io/upload_images/1157146-cab5ba89dfeeec4b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\"></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
