{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursera | Andrew Ng (01-week-3-3.6)—Activation functions\n",
    "\n",
    "该系列仅在原课程基础上部分知识点添加个人学习笔记，或相关推导补充等。如有错误，还请批评指教。在学习了 Andrew Ng 课程的基础上，为了更方便的查阅复习，将其整理成文字。因本人一直在学习英语，所以该系列以英文为主，同时也建议读者以英文为主，中文辅助，以便后期进阶时，为学习相关领域的学术论文做铺垫。- ZJ\n",
    "    \n",
    ">[Coursera 课程](https://www.coursera.org/specializations/deep-learning) |[deeplearning.ai](https://www.deeplearning.ai/) |[网易云课堂](https://mooc.study.163.com/smartSpec/detail/1001319001.htm)\n",
    "\n",
    "---\n",
    "   **转载请注明作者和出处：ZJ 微信公众号-「SelfImprovementLab」**\n",
    "   \n",
    "   [知乎](https://zhuanlan.zhihu.com/c_147249273)：https://zhuanlan.zhihu.com/c_147249273\n",
    "   \n",
    "   [CSDN]()：\n",
    "   \n",
    "\n",
    "---\n",
    "3.6 激活函数  Activation functions\n",
    "\n",
    "(字幕来源：网易云课堂)\n",
    "\n",
    "\n",
    "When you build a neural network,one of the choices you get to make is,what activation functions use in the hidden layers,as well as at the output unit of your neural network.So far we've just been using the sigmoid activation function,but sometimes other choices can work much better,let's take a look at some of the options,in the forward propagation steps for the neural network,we have these two steps where we use the sigmoid function here,so that sigmoid is called an activation function,and here is the familiar sigmoid function,a equals 1 over 1 plus e to negative z.So in the more general case we can have a different function g of z,which I'm gonna write here,where g could be a nonlinear function that may not be the sigmoid function\n",
    "\n",
    "\n",
    "要搭建一个神经网络，你可以选择的是，选择隐层里用那一个激活函数，还有神经网络的输出单元用什么激活函数，到目前为止 我们一直用的是σ激活函数，但有时其他函数效果要好得多，我们看看一些可供选择的函数，在神经网络的正向传播步骤中，我们有这两步 用的是σ函数，这个σ就是所谓的激活函数，这是大家很熟悉的σ函数，就是a = 1/(1+e^-z)，所以在更一般的情况下 我们可以使用不同的函数g(z)，我会在这里写出来，其中g可以是非线性函数 不一定是σ函数\n",
    "\n",
    "so for example the sigmoid function goes between 0 and 1,an activation function that almost always works better than the sigmoid function is,the tanh function or the hyperbolic tangent function,so this is z, this is a, this is a equals tanh of z,and this goes between plus 1 and minus 1,the formula for the tanh function is e to the z minus e to negative z over their sum,and it's actually mathematically a shifted version of the sigmoid function,so as a you know sigmoid function just like that,but shift it so that it now crosses a zero zero point and rescale,so it goes from minus one to plus one,and it turns out that for hidden units,if you let the function g(z) be equal to tanh(z)\n",
    "\n",
    "\n",
    "比如说 σ函数介于0和1之间，有个激活函数几乎总比σ表现更好，就是tanh函数 或者叫双曲正切函数，所以这是z 这是a 这是a=tanh(z)，这函数介于-1和1之间，tanh函数的公式是 e^z-e^(-z) 再除以它们之和，数学上这实际上是σ函数平移后的版本，所以你知道σ函数是这样的，但把它移动一下 让它穿过零点 然后重新标度，让它介于-1和1之间，事实证明 对于隐藏单元，如果你让函数g(z)等于tanh(z)\n",
    "\n",
    "this almost always works better than the sigmoid function,because with values between plus one and minus one,the mean of the activations that come out of your head in there,are closer to having a zero mean,and so just as sometimes when you train a learning algorithm,you might Center the data and have your data have zero mean,using a tanh instead of a sigmoid function kind of has the effect of centering your data,so that the mean of the data is close to the zero,rather than maybe a 0.5,and this actually makes learning for the next layer a little bit easier,we'll say more about this in the second course,when we talk about optimization algorithms as well,but one takeaway is that I pretty much never use the sigmoid activation function anymore,the tanh function is almost always strictly superior\n",
    "\n",
    "\n",
    "这几乎总比σ函数效果更好，因为现在函数输出介于-1和1之间，激活函数的平均值，就更接近0，就像有时在你训练学习算法时，你可能需要平移所有数据 让数据平均值为0，使用tanh而不是σ函数也有类似数据中心化的效果，使得数据的平均值接近0，而不是0.5，这实际上让下一层的学习更方便一点，我会在第二门课程里详细讨论这点，那时我们也会介绍算法优化，但这里要记住一点 我几乎不用σ激活函数了，tanh函数几乎在所有场合都更优越\n",
    "\n",
    "the one exception is for the output layer,because if y is either 0 or 1,then it makes sense for y hat to be a number that you want to output just between 0 and 1,rather than between minus 1 and 1,so the one exception where I would use the sigmoid activation function is,when you're using binary classification,in which case you might use the sigmoid activation function for the output layer,so g(z^[2]) here is equal to Sigma(z^[2]),and so what you see in this example is,where you might have a tanh activation function for the hidden layer,and sigmoid for the output layer,so the activation functions can be different for different layers,and sometimes to denote that the different activation functions are different for different layers,we might use these square brackets super scripts as well,to indicate that g of square bracket 1 may be different than g square bracket 2,to indicate, square bracket 1 superscript refers to this layer,and superscript square bracket 2 refers to the output layer\n",
    "\n",
    "\n",
    "一个例外是输出层，因为如果y是0或1，那么你希望y_帽介于0到1之间更合理，而不是-1和1之间，我会用σ激活函数的一个例外场合是，使用二元分类的时候，在这种情况下 你可以用σ激活函数作为输出层，所以g(z^[2])等于σ(z^[2])，所以在这个例子中你可以，在隐层里用tanh激活函数，输出层用σ函数，所以不同层的激活函数可以不一样，有时候为了表示不同层的不同激活函数，我们可能会用这些方括号上标，来表示g^[1]可能和g^[2]不同，上标方括号1表示这一层，上标方括号2表示输出层\n",
    "\n",
    "now one of the downsides of both the sigmoid function and the tanh function is,that if z is either very large or very small,then the gradient of the derivative or the slope of this function becomes very small,so z is very large or z is very small,the slope of the function you know ends up being close to zero,and so this can slow down gradient descent,so one of the toys that is very popular in machine learning is,what's called the rectified linear unit,so the ReLU function looks like this,and the formula is a equals max of 0 comma z,so the derivative is 1 so long as z is positive,and derivative or the slope is 0 when z is negative,if you're implementing this technically,the derivative when z is exactly 0 is not well-defined,but when you implement is in the computer the,odds that you get exactly z equals 0 0 0 0 0 0 0 0 0 0  is very small\n",
    "\n",
    "现在σ函数和tanh函数都有的一个缺点 就是，如果z非常大或非常小，那么导数的梯度 或者说这个函数的斜率可能就很小，所以z很大或很小的时候，函数的斜率很接近0，这样会拖慢梯度下降算法，在机器学习里 最受欢迎的一个玩具是，所谓的修正线性单元(ReLU)，所以一个ReLU函数长这样，公式就是 a=max(0,z)，只要z为正 导数就是1，当z为负时 斜率为0，如果你实际使用这个函数，z刚好为0时 导数是没有定义的，但如果你编程实现，那么你得到z刚好等于0 0 0 0 0 0的概率很低\n",
    "\n",
    "so you don't need to worry about it in practice,you could pretend a derivative when z is equal to 0,you can pretend is either 1 or 0 and you can work just fine,so the fact that is not differentiable  the fact that,so here are some rules of thumb for choosing activation functions,if your output is 0 1 value,if you're I'm using binary classification,then the sigmoid activation function is very natural for the output layer,and then for all other units on ReLU,or the rectified linear unit,is increasingly the default choice of activation function,so if you're not sure what to use for your hidden layer.\n",
    "\n",
    "所以实践中不用担心这点，你可以在z=0时 给导数赋值，你可以赋值成1或0 那样也是可以的，尽管事实上这个函数不可微，在选择激活函数时有一些经验法则，如果你的输出值是0和1，如果你在做二元分类，那么σ函数很适合作为输出层的激活函数，然后其他所有单元都用ReLU，所谓的修正线性单元，现在已经变成激活函数的默认选择了，如果你不确定隐层应该用哪个\n",
    "\n",
    "\n",
    "I would just use the ReLU activation function,that's what you see most people using these days,although sometimes people also use,the tanh activation function,one disadvantage of the ReLU is that,the derivative is equal to zero when z is negative,in practice this works just fine,but there is another version of the ReLU called the leaky ReLU,we will give you the formula on the next slide,but instead of it being zero when z is negative,it just takes a slight slope like so,so this is called the Leaky ReLU,this usually works better than the ReLU activation function,although it's just not used as much in practice,either one should be fine although if you had to pick one.\n",
    "\n",
    "我就用ReLU作为激活函数，这是今天大多数人都在用的，虽然人们有时候也会用，tanh激活函数，而ReLU的一个缺点是，当z为负时 导数等于零，在实践中这没什么问题，但ReLU还有另一个版本 叫做带泄漏的ReLU，我们会在下一页给出公式，当z为负时 函数不再为0，它有一个很平缓的斜率，这就是所谓的带泄漏 ReLU，这通常比ReLU激活函数更好，不过实际中使用的频率没那么高，这些选一个就好了 如果你一定要选一个\n",
    "\n",
    "\n",
    "I usually just use the ReLU,and the advantage of both the ReLU and the leaky ReLU is that,for a lot of the space of z,the derivative of the activation function,the slope of the activation function is very different from zero,and so in practice using the ReLU activation function,your neural network will often learn much faster,than using the tanh or the sigmoid activation function,and the main reason is that,there is less of this effect of the slope of the function going to zero,which slows down learning,and I know that for half of the range of z,the slope of ReLU is zero but in practice,enough of your hidden units will have z greater than zero,so learning can still be quite mask for most training examples\n",
    "\n",
    "我通常只用ReLU，ReLU和带泄漏的ReLU好处在于，对于很多z空间，激活函数的导数，激活函数的斜率和0差很远，所以在实践中使用ReLU激活函数，你的神经网络的学习速度通常会快得多，比使用tanh或σ激活函数快得多，主要原因在于，ReLu没有这种函数斜率接近0时，减慢学习速度的效应，我知道 对于z的一半范围来说，ReLU的斜率为零 但在实践中，有足够多的隐藏单元 令z大于0，所以对大多数训练样本来说还是很快的\n",
    "\n",
    "so let's just quickly recap,there are pros and cons of different activation functions,here's the sigmoid activation function.I would say never use this except for the output layer if you are doing binary classification,or maybe almost never use this,and the reason I almost never use this is because,the tanh is pretty much strictly superior,so the tanh activation function is this,and then the default the most commonly used activation function is the ReLU,which is this,so you're not sure what else to use, use this one,and maybe you know feel free also to try the leaky ReLU,where um might be (0.01z,z) right,so a is the max of 0.01 times z and z,so that gives you this some Bend in the function\n",
    "\n",
    "我们快速回顾一下，不同激活函数的利弊，这里是σ激活函数，除非用在二元分类的输出层 不然绝对不要用，或者几乎从来不会用，我几乎从来没用过 原因在于，tanh几乎在所有场合都更优越，所以tanh函数是这样的，还有最常用的默认激活函数是ReLU，就是这个，如果你不确定要用哪个 就用这个，或者你想用的话 也可以试试带泄漏的ReLU，公式可能是max(0.01z,z) 对吧，所以a是0.01*z和z两者的最大值，这样函数会这样拐一下\n",
    "\n",
    "\n",
    "and you might say you know why is that constant 0.01,well you can also make that another parameter of the learning algorithm,and some people say that works even better,but I hardly see people do that so,but if you feel like trying it in your application,you know please feel free to do so and,and you can just see how it works and how well it works,and stick with it if it gives you good result,so I hope that gives you a sense of,some of the choices of activation functions you can use in your network,one of the themes we'll see in deep learning is that,you often have a lot of different choices in how you build your neural network,ranging from number of hidden units,to the choice of activation function,to how you initialize the weights which we'll see later,a lot of choices like that and it turns out that is sometimes difficult,to get good guidelines for exactly what will work best for your problem\n",
    "\n",
    "你可能会问 为什么那个常数是0.01?，你也可以把它设成学习函数的另一个参数，有人说这样效果更好，但我很少见到有人这么做 所以，如果你想在你的应用里试试，自己喜欢就好，你可以看看效果如何 有多好，如果结果很好 那么就一直用它，我希望这样你就对，如何在你的网络里选择激活函数有概念，深度学习其中一个特点是，在建立神经网络时经常有很多不同的选择，比如隐藏单元数，激活函数，还有如何初始化权重 这个我们接下来会讲，有很多这样的选择 有时真的很难，去定下一个准则 来确定什么参数最适合你的问题\n",
    "\n",
    "so throughout all these three courses.I’ll keep on giving you a sense of what I see in the industry,in terms of what's more or less popular,but for your application with your applications idiosyncrasy,it's actually very difficult to know in advance exactly what will work best,so a piece of advice would be,if you're not sure which one of these activation functions work best,you know try them all and then evaluate on like a holdout validation set,or like a development set which we'll talk about later,and see which one works better  and then go with that,and I think that by testing these different choices for your application,you'd be better at future proofing your neural network architecture,against the the idiosyncrasy in our problem,as well evolutions of the algorithms,rather than you know if I were to tell you always use a ReLUactivation,and don't use anything else that that just,may or may not apply for whatever problem you end up working on,you know either either in the near future on the distant future\n",
    "\n",
    "\n",
    "\n",
    "所以在这三门课程中，我会让你大概了解我在行业里见到的，热门选择 或者冷门选择，但是对于你的应用 你的应用的特质，事实上很难预先准确地知道什么参数最有效，所以一个建议是，如果你不确定哪种激活函数最有效，你可以先试试 在你的保留交叉验证数据集上跑跑，或者在开发集上跑跑，看看哪个参数效果更好 就用那个，我想通过在你的应用中测试这些不同的选择，你可以搭建具有前瞻性的神经网络架构，可以对你问题的特质更有针对性，让你的算法迭代更流畅，我这里不会告诉你一定要用ReLU激活函数，而不用其他的..，那对你现在或者未来要处理的问题而言，可能管用 也可能不管用\n",
    "\n",
    "all right so that was a choice of activation functions,you've seen the most popular activation functions,there's one other question that sometimes is ask which is,why do you even need to use an activation function at all,why not just do away with that,so let's talk about that in the next video,and where you will see why neural network,do need some sort of nonlinear activation function\n",
    "\n",
    "\n",
    "好 这就是激活函数的选择，你们看到了最热门的激活函数，还有另一个问题 经常有人会问，为什么你需要激活函数呢?，为什么不直接去掉，在下一个视频我们会谈到为什么，神经网络确实需要，某种非线性激活函数\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### <font color=#0099ff>重点总结：</font>\n",
    "\n",
    "\n",
    "参考文献：\n",
    "\n",
    "[1]. 大树先生.[吴恩达Coursera深度学习课程 DeepLearning.ai 提炼笔记（1-3）-- 浅层神经网络](http://blog.csdn.net/koala_tree/article/details/78059952)\n",
    "\n",
    "\n",
    "---\n",
    " \n",
    "**PS: 欢迎扫码关注公众号：「SelfImprovementLab」！专注「深度学习」，「机器学习」，「人工智能」。以及 「早起」，「阅读」，「运动」，「英语 」「其他」不定期建群 打卡互助活动。**\n",
    "\n",
    "<center><img src=\"http://upload-images.jianshu.io/upload_images/1157146-cab5ba89dfeeec4b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\"></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
