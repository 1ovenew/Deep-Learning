{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursera | Andrew Ng (01-week-3-3.5)—Explanation for Vectorized implementation\n",
    "\n",
    "该系列仅在原课程基础上部分知识点添加个人学习笔记，或相关推导补充等。如有错误，还请批评指教。在学习了 Andrew Ng 课程的基础上，为了更方便的查阅复习，将其整理成文字。因本人一直在学习英语，所以该系列以英文为主，同时也建议读者以英文为主，中文辅助，以便后期进阶时，为学习相关领域的学术论文做铺垫。- ZJ\n",
    "    \n",
    ">[Coursera 课程](https://www.coursera.org/specializations/deep-learning) |[deeplearning.ai](https://www.deeplearning.ai/) |[网易云课堂](https://mooc.study.163.com/smartSpec/detail/1001319001.htm)\n",
    "\n",
    "---\n",
    "   **转载请注明作者和出处：ZJ 微信公众号-「SelfImprovementLab」**\n",
    "   \n",
    "   [知乎](https://zhuanlan.zhihu.com/c_147249273)：https://zhuanlan.zhihu.com/c_147249273\n",
    "   \n",
    "   [CSDN]()：\n",
    "   \n",
    "\n",
    "---\n",
    "3.5 向量化实现的解释  Explanation for Vectorized implementation\n",
    "\n",
    "(字幕来源：网易云课堂)\n",
    "\n",
    "在以前的视频中 我们看到如何将训练样本\n",
    "In the previous video we saw how with your training examples\n",
    "\n",
    "，横向堆叠起来构成矩阵X\n",
    ",stacked up horizontally in the matrix X\n",
    "\n",
    "，你就可以导出一个在网络中正向传播算法的\n",
    ",You can derive a vectorized implementation of\n",
    "\n",
    "，向量化实现\n",
    ",forward propagation through your network\n",
    "\n",
    "，这里我们讲一下更多的理由 说明\n",
    ",Let's give a bit more justification for\n",
    "\n",
    "，为什么我们写下的方程\n",
    ",Why the equations we wrote down\n",
    "\n",
    "，向量化在多样本时的正确实现\n",
    ",is a correct implementation of vectorizing across multiple examples\n",
    "，我们对几个样本手动算算正向传播\n",
    ",So let's go through part of the forward propagation calculation for a few examples\n",
    "\n",
    "，我们看第一个训练样本\n",
    ",Let's say that for the first training example\n",
    "\n",
    "，你最后计算出这个W^[1] x^(1)+b^[1]\n",
    ",You end up computing this W^[1] x^(1) plus b1^[1]\n",
    "\n",
    "，然后是第二个训练样本\n",
    ",And then for the second training example\n",
    "\n",
    "，你最后计算出这个.. x^(2)+b^[1]\n",
    ",You end up computing this .. x^(2) plus b^[1]\n",
    "\n",
    "，然后是第三个训练样本\n",
    ",And then for the third training example\n",
    "，你最后计算的是这个.. x^(3)+b^[1]\n",
    ",You end up computing this 3 plus b^[1]\n",
    "\n",
    "，所以为了简化幻灯片上的描述\n",
    ",So just to simplify the explanation on this slide\n",
    "\n",
    "，我要忽略b\n",
    ",I’m going to ignore b\n",
    "\n",
    "，我们看看 比如说 为了简化这个推导一点点\n",
    ",So let's just say you know for the to simplify this justification a little bit\n",
    "\n",
    "，令b=0 所以参数只需要变化一点点\n",
    ",that B is equal to 0, so the argument going to layout\n",
    "\n",
    "，就可以处理b非零的情况\n",
    ",will work with just a little bit of a change even when B is nonzero\n",
    "\n",
    "，它只是简化了这张幻灯片的描述\n",
    ".It does just simplify the description on this slide of it\n",
    "\n",
    "，现在W^[1]现在是个矩阵\n",
    ".Now w1 is going to be some matrix right\n",
    "\n",
    "，这个矩阵里有一定数目的行\n",
    ".So I have some number of rows in this matrix\n",
    "\n",
    "，所以你看这个x^(1)的计算\n",
    ".So if you look at this calculation x^(1)\n",
    "\n",
    "，你这里得到的是W^[1]乘以x^(1) 得到一些列向量\n",
    ".What you have is that W^[1] times x^(1) gives you some column vector\n",
    "\n",
    "，我这里用这样的小点表示\n",
    ",which you must draw a light ball like this\n",
    "\n",
    "，同样 你观察一下向量x^(2)\n",
    ".And similarly if you look at this vector x^(2)\n",
    "\n",
    "，这里有W^[1]乘以x^(2)得到其他一些列向量 对吧\n",
    ".You have that W^[1] times x^(2) gives some other column vector, right\n",
    "\n",
    "，然后我给你这个 我想是z^[1](2)\n",
    ".And that's what gives you this,  I guess z^[1](2)\n",
    "\n",
    "，最后你看x^(3)\n",
    ",and finally if you look at x^(3)\n",
    "\n",
    "，你有W^[1]乘x^(3)得到第三个列向量 就是z^[1](3)\n",
    ".You have W^[1] times x^(3) gives you some third column vector, that's this z^[1](3)\n",
    "\n",
    "，现在如果你考虑训练集X\n",
    ".So now if you consider the training set capital X\n",
    "\n",
    "，我们将所有训练样本堆叠起来得到的\n",
    ",which we form by stacking together all of our training examples\n",
    "\n",
    "，所以矩阵大写X是把向量x^(1)拿过来\n",
    ".So the matrix capital X is formed by taking the vector x^(1)\n",
    "\n",
    "，横向叠上x^(2) 然后用x^(3)\n",
    ",and stacking it vertically with x^(2) and then also x^(3)\n",
    "\n",
    "，就是我们只有三个训练样本的情况\n",
    ".This is a we have only three training examples\n",
    "\n",
    "，如果有更多的样本 你只要继续横向叠上去\n",
    ".If you have more you know they'll be a little keep stacking horizontally like that\n",
    "\n",
    "，但如果你现在取这个矩阵X 然后让它乘以W\n",
    ",but if you now take this matrix X and multiply it by W\n",
    "\n",
    "，最后你会得到\n",
    ",then you end up with\n",
    "\n",
    "，如果你想想矩阵乘法是怎么做的话\n",
    ".If you think about how matrix multiplication works\n",
    "\n",
    "，你的第一列还是这些一样的值\n",
    ",you end up with the first column being these same values\n",
    "\n",
    "，这些用紫色画出来的\n",
    ",that had drawn up there in purple\n",
    "\n",
    "，第二列就是那同样的四个值\n",
    ".The second column will be those same four values\n",
    "\n",
    "，第三列是这些橙色的值\n",
    ",and the third column will be those are orange values what they turn out to be\n",
    "\n",
    "，但当然了 这就等于将z^[1](1)写成列向量\n",
    ",but of course this is just equal to z^[1](1) expressed as a column vector\n",
    "\n",
    "，然后是列向量表示的z^[1](2)\n",
    ",followed by z^[1](2) express as a column vector\n",
    "\n",
    "，然后是列向量表示的z^[1](3)\n",
    ",followed by z^[1](3) also express as a column vector\n",
    "\n",
    "，这些是特征样本 如果你有更多样本\n",
    ",and this is featuring examples if you have more examples\n",
    "\n",
    "，那么列数会更多\n",
    ",and they'll be more columns\n",
    "\n",
    "，所以这就是我们的矩阵大写Z^[1]\n",
    ",and so this is just our matrix capital Z^[1]\n",
    "\n",
    "，我希望能让你们弄清楚 为什么我们之前要写成\n",
    ".So I hope this gives a justification to why when we had previously\n",
    "\n",
    "，W^[1]乘x^(i)等于z^[1](i)这个形式\n",
    ",W^[1] times x^(i) equals z^[1][i]\n",
    "\n",
    "，那是针对单个训练样本的公式\n",
    ",when we're looking at single training example at a time\n",
    "\n",
    "，当你处理不同训练样本时\n",
    ",when you took the different training examples\n",
    "\n",
    "，就将它们堆到各列中\n",
    ",and stack them up in different columns\n",
    "\n",
    "，那么对应的结果应该是这样的\n",
    ".Then the corresponding result is that\n",
    "\n",
    "，你最后会得到这些z叠起来 放在不同的列里\n",
    ",you end up with the z-s stacked as different columns\n",
    "\n",
    "，我不会写出具体形式 但你可以自己验证\n",
    ",and I won't show but you can convince yourself\n",
    "\n",
    "，如果你想用Python广播做矩阵和向量的加法\n",
    ",if you want stats with Python broadcasting\n",
    "\n",
    "，如果你把这些b值加回来\n",
    ",if you add back in these values of b\n",
    "\n",
    "，这些值还是对 但最后结果是\n",
    ".The values are still correct and what actually ends up happening is\n",
    "\n",
    "，你最后用到Python广播\n",
    ".You end up with Python broadcasting\n",
    "\n",
    "，你最后将b^[i]单独加到矩阵各列\n",
    ".You end up having b^[i] individually to each of the columns of this matrix\n",
    "\n",
    "，所以在这张幻灯片中 我只说明了\n",
    ".So on this slide I've only justified that\n",
    "\n",
    "，为什么Z^[1]等于W^[1] X加b^[1]\n",
    ",Z^[1] equals W^[1] X plus b^[1]\n",
    "\n",
    "，这是四步中第一步的正确向量化实现\n",
    ".That's a correct vectorization of the first step of the four steps\n",
    "\n",
    "，就是上一张幻灯片那四步\n",
    ".We have in the previous slide\n",
    "\n",
    "，但事实证明 类似的分析\n",
    ",but it turns out that the similar analysis\n",
    "\n",
    "，让你发现其他步骤\n",
    ",allows you to show that the other steps also work on\n",
    "\n",
    "，也可以使用非常相似的逻辑 如果将输入成列向量堆叠\n",
    ",using a very similar logic where if you stack the inputs in columns\n",
    "\n",
    "，那么在方程运算之后 你也能得到成列堆叠的输出\n",
    ".Then after the equation you get the corresponding outputs also stacked up in columns\n",
    "\n",
    "，最后我们回顾一下这段视频的内容\n",
    ".Finally let's just recap everything we talked about in this video\n",
    "\n",
    "，这是你的神经网络\n",
    ".This is your neural network\n",
    "\n",
    "，我说这就是如果你需要\n",
    ".We said that this is what you need to do if you were to implement\n",
    "\n",
    "，在单个训练样本中实现正向传播算法的话 就要这么做\n",
    ",forward propagation one training example at a time\n",
    "\n",
    "，就是从i从1到m遍历\n",
    ",going from i equals 1 through m\n",
    "\n",
    "，然后我说把这些训练样本以列向量堆叠起来\n",
    ".And we said let's stack up the training examples in columns like so\n",
    "\n",
    "，所以这里面每一个值 z^[1] a^[1] z^[2] a^[2]\n",
    ".And so each of these values z^[1] a^[1] z^[2] a^[2]\n",
    "\n",
    "，对应各列堆叠起来是这样的\n",
    ".The stack of the corresponding columns as follows\n",
    "\n",
    "，这对A^[1]成立 对Z^[1] A^[1] Z^[2]和A^[2]都成立\n",
    ".So this example for A^[1] but this is true for Z^[1] A^[1] Z^[2] and A^[2]\n",
    "\n",
    "，现在我们上一张幻灯片中展示的是\n",
    ".Then what we showed on the previous slide was that\n",
    "\n",
    "，这一行能让你对所有m个例子同时向量化\n",
    ".This line allows you to vectorize this across all m examples at the same time\n",
    "\n",
    "，事实证明 使用类似的推导\n",
    ".And it turns out with the similar reasoning\n",
    "\n",
    "，你可以证明所有其他行\n",
    ",you can show that all of the other lines\n",
    "\n",
    "，都是这四行代码的正确向量化形式\n",
    ",are correct vectorization of all four of these lines of code\n",
    "\n",
    "，这里提醒一下 因为X也等于A^[0]\n",
    ".And just as a reminder, because X is also equal to A^[0]\n",
    "\n",
    "，因为你还记得输入的特征向量x\n",
    ".Because you remember that the input feature vector x\n",
    "\n",
    "，是等于a^[0]的 所以x^(i)等于a^[0](i)\n",
    ",was equal to a^[0], so x^(i) equals a^[0](i)\n",
    "\n",
    "，其实这些方程有一定对称性\n",
    ".I then there's actually a certain symmetry to these equations\n",
    "\n",
    "，其中第一个方程也可以写成Z^[1]等于W^[1] A^[0]加b^[1]\n",
    ",where this first equation can also be written Z^[1] equals W^[1] A^[0] plus b^[1]\n",
    "\n",
    "，你看这对方程\n",
    ".And so you see that this pair of equations\n",
    "\n",
    "，还有这对方程形式其实很类似\n",
    ",and this pair of equations actually look very similar\n",
    "\n",
    "，只不过这里所有指标加了1\n",
    ",but just of all that the indices advance by one\n",
    "\n",
    "，所以这样就显示出神经网络的不同层次\n",
    ".So this kind of shows that the different layers of a neural network\n",
    "\n",
    "，你知道大概每一步做的都是一样的\n",
    ",are you know roughly doing the same thing\n",
    "\n",
    "，或者只不过同样的计算不断重复而已\n",
    ",or just doing the same computation over and over\n",
    "\n",
    "，这里我们有一个双层神经网络\n",
    ",and here we have a two layer neural network\n",
    "\n",
    "，我们在下周视频里会讲深得多的神经网络\n",
    ",where we go to a much deeper neural network in next week's videos\n",
    "\n",
    "，你看到随着网络的深度变大\n",
    ",you see that even deeper in your networks\n",
    "\n",
    "，基本上也还是重复这两步运算\n",
    ",are basically taking these two steps\n",
    "\n",
    "，只不过重复次数更多 而这里你看到的是\n",
    ",and just doing them even more times than you're seeing here\n",
    "\n",
    "，所以这就是对不同训练样本向量化的神经网络\n",
    ".So that's how you can vectorize your neural network across multiple training examples\n",
    "\n",
    "，接下来 到目前为止 我们一直用的是σ函数\n",
    ".Next, we've so far been using the sigmoid function throughout that neural network\n",
    "\n",
    "，事实证明 这不是最好的选择\n",
    ",turns out that's actually not the best choice\n",
    "\n",
    "，在下一个视频中 我们进一步深入研究\n",
    ".On the next video let's delve a little bit further into\n",
    "\n",
    "，如何使用不同种类的激活函数\n",
    ",how you can use different what's called activation functions\n",
    "\n",
    "，其中σ函数只是其中一个可能选择\n",
    ",of which the sigmoid function is just one possible choice\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### <font color=#0099ff>重点总结：</font>\n",
    "\n",
    "\n",
    "参考文献：\n",
    "\n",
    "[1]. 大树先生.[吴恩达Coursera深度学习课程 DeepLearning.ai 提炼笔记（1-3）-- 浅层神经网络](http://blog.csdn.net/koala_tree/article/details/78059952)\n",
    "\n",
    "\n",
    "---\n",
    " \n",
    "**PS: 欢迎扫码关注公众号：「SelfImprovementLab」！专注「深度学习」，「机器学习」，「人工智能」。以及 「早起」，「阅读」，「运动」，「英语 」「其他」不定期建群 打卡互助活动。**\n",
    "\n",
    "<center><img src=\"http://upload-images.jianshu.io/upload_images/1157146-cab5ba89dfeeec4b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\"></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
