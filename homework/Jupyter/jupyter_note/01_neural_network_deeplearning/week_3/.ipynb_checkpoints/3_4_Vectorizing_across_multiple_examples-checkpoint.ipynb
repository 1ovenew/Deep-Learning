{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursera | Andrew Ng (01-week-3-3.4)—Vectorizing across multiple examples\n",
    "\n",
    "该系列仅在原课程基础上部分知识点添加个人学习笔记，或相关推导补充等。如有错误，还请批评指教。在学习了 Andrew Ng 课程的基础上，为了更方便的查阅复习，将其整理成文字。因本人一直在学习英语，所以该系列以英文为主，同时也建议读者以英文为主，中文辅助，以便后期进阶时，为学习相关领域的学术论文做铺垫。- ZJ\n",
    "    \n",
    ">[Coursera 课程](https://www.coursera.org/specializations/deep-learning) |[deeplearning.ai](https://www.deeplearning.ai/) |[网易云课堂](https://mooc.study.163.com/smartSpec/detail/1001319001.htm)\n",
    "\n",
    "---\n",
    "   **转载请注明作者和出处：ZJ 微信公众号-「SelfImprovementLab」**\n",
    "   \n",
    "   [知乎](https://zhuanlan.zhihu.com/c_147249273)：https://zhuanlan.zhihu.com/c_147249273\n",
    "   \n",
    "   [CSDN]()：\n",
    "   \n",
    "\n",
    "---\n",
    "3.4 多个例子中的向量化  Vectorizing across multiple examples\n",
    "\n",
    "\n",
    "在上一个视频中 你们看到了如何在(字幕来源：网易云课堂)\n",
    "In the last video, you saw how to compute a prediction\n",
    "\n",
    "，已知单个训练样本时 计算神经网络的预测\n",
    ",on a neural network given a single training example.\n",
    "\n",
    "，在这个视频中 你们可以看到如何将不同训练样本向量化\n",
    "In this video, you see how to vectorize across multiple training examples,\n",
    "\n",
    "，输出结果和Logistic回归很相似\n",
    "and the outcome will be quite similar to what you saw for logistic regression,\n",
    "\n",
    "，如何将不同训练样本堆叠起来 放入矩阵的各列呢?\n",
    "whereby stacking up different training examples in different columns of a matrix,\n",
    "\n",
    "，你可以把上一个视频中的方程拿过来\n",
    "you'll be able to take the equations you had from the previous video,\n",
    "\n",
    "，然后稍微修改一下\n",
    "and with very little modification,\n",
    "\n",
    "，把输入方式变一下 让神经网络几乎同时\n",
    "change them to make the neural network compute the outputs on all the examples,\n",
    "\n",
    "，计算所有样本的输出\n",
    "pretty much all at the same time.\n",
    "\n",
    "，我们看看具体怎么做\n",
    "So let's see the details of how to do that.\n",
    "\n",
    "，这些是上一个视频的4个方程\n",
    "These were the four equations we had from the previous video of\n",
    "\n",
    "，如何计算z^[1],a^[1],z^[2]和a^[2]\n",
    "how you compute z^[1], a^[1], z^[2] and a^[2].\n",
    "\n",
    "，它们告诉你\n",
    "And they tell you how,\n",
    "\n",
    "，对于输入的特征向量x\n",
    "given an input feature vector x,\n",
    "\n",
    "，对于这单个训练样本 你可以用它们生成一个a^[2] = y_帽\n",
    "you can use them to generate a^[2] equals y_hat for a single training example.\n",
    "\n",
    "，现在 如果你有m个训练样本\n",
    "Now, if you have m training examples,\n",
    "\n",
    "，你可能需要重复这个过程 比如说\n",
    "you need to repeat this process for, say,\n",
    "\n",
    "，第一个训练样本 x^[1]来计算y_帽^[1]\n",
    "the first training example x_superscript_round_brackets_1 to compute y_hat^[1],\n",
    "\n",
    "，那是对你第一个训练样本的预测\n",
    "that's the prediction on your first training example.\n",
    "\n",
    "，然后x^[2] 用来生成预测y_帽 [2]\n",
    "Then x^[2], use that to generate prediction y_hat[2],\n",
    "\n",
    "，之类的 一直到x^[m] 生成预测y_帽^[m]\n",
    "and so on down to x^[m] to generate prediction y_hat^[m].\n",
    "\n",
    "，所以 要用激活函数来表示这些式子\n",
    "And so, in order to write this with the activation function notation as well,\n",
    "\n",
    "，我要把它写成a^[2](1)\n",
    "I'm going to write this as a^[2]_square_brackets_round_bracket_1.\n",
    "\n",
    "，这是[2](2)和a^[2](m)\n",
    "This is [2](2) and a^[2](m).\n",
    "\n",
    "，所以这个符号 a^[2](i)\n",
    "So this notation, a_square_bracket_2_round_bracket_i,\n",
    "\n",
    "，圆括号里的i表示指训练样本i\n",
    "the round bracket_i refers to training example i\n",
    "\n",
    "，方括号指的是第二层\n",
    "and the square bracket_2 refers to layer two.\n",
    "\n",
    "，这就是方括号圆括号写法的意义\n",
    "So that's how the square bracket and the round bracket indices work.\n",
    "\n",
    "，所以这表明如果你有一个没有向量化的实现\n",
    "And so this suggests that if you have an unvectorized implementation\n",
    "\n",
    "，并想要计算所有训练样本的预测\n",
    "and want to compute the prediction of all your training examples\n",
    "\n",
    "，你需要对i=1到m遍历\n",
    "you need to do for i equals 1 to m,\n",
    "\n",
    "，然后基本实现这4个方程\n",
    "then basically implement these 4 equations.\n",
    "\n",
    "，因为z^[1](i)等于W^[1] x ^(i)+ b^[1]\n",
    "because z^[1](i) equals W^[1] x^(i) + b^[1]\n",
    "\n",
    "，a^[1](i)等于σ(z^[1](i))\n",
    "a^[1](i) equals sigmoid of z^[1](i)\n",
    "\n",
    "，z^[2](i)等于W^[2] a^[1](i)+ b^[2]\n",
    "z^[2](i) equals W^[2] a^[1](i) + b^[2].\n",
    "\n",
    "，然后a^[2](i)等于σ(z^[2](i))\n",
    "and a^[2](i) equals sigmoid of z^[2](i)\n",
    "\n",
    "，对 所以基本上和上面四个方程一样\n",
    "Right, so basically these four equations on top\n",
    "\n",
    "，不过需要添加上标圆括号i\n",
    "by adding the superscript round bracket i\n",
    "\n",
    "，来表示训练样本中的所有变量\n",
    "to all the variables that depend on the training example,\n",
    "\n",
    "，所以要往x z和a加上圆括号上标i\n",
    "so adding the superscript round bracket i to x, z and a,\n",
    "\n",
    "，如果你想计算\n",
    "if you want to compute,\n",
    "\n",
    "，所有m个训练样本的输出\n",
    "all the outputs on your m training examples.\n",
    "\n",
    "，我们一般喜欢将整个计算向量化 就可以去掉这些公式\n",
    "What we like to do is vectorize this whole computation so as to get rid of this formula.\n",
    "\n",
    "，顺便说一句 如果你觉得我讲的太多深奥的线性代数\n",
    "And by the way, in case it seems like I'm getting a lot of nitty gritty linear algebra,\n",
    "\n",
    "，事实上 能够正确实现这些算法\n",
    "it turns out that being able to implement this correctly\n",
    "\n",
    "，在深度学习时代很重要\n",
    "is important in the deep learning era,\n",
    "\n",
    "，在备课时 我很注意符号的选择\n",
    "and we actually chose the notation very carefully for this course\n",
    "\n",
    "，使得这些向量化过程越简单越好\n",
    "and made these vectorizations as easy as possible.\n",
    "\n",
    "，所以我希望详细讲解线性代数细节 能够帮你\n",
    "So I hope that going through this nitty gritty will actually help you to\n",
    "\n",
    "，快速正确地实现这些算法\n",
    "more quickly get correct implementations of these algorithms working.\n",
    "\n",
    "，好的 我们把这段代码复制到下一张幻灯片里\n",
    "All right, so let me just copy this whole block of code to the next slide\n",
    "\n",
    "，然后看看怎么向量化这个\n",
    "and then we'll see how to vectorize this.\n",
    "\n",
    "，所以这是我们从上一张幻灯片中的\n",
    "So here's what we have from the previous slide\n",
    "\n",
    "，一个for循环 遍历所有的m训练样本\n",
    "with a for loop going over all m training examples.\n",
    "\n",
    "，还记得我们定义过矩阵X\n",
    "So recall that we define the matrix X to be\n",
    "\n",
    "，就是我们的训练样本堆到各列\n",
    "equal to our training examples stacked up in these columns like so.\n",
    "\n",
    "，所以把这些训练样本拿过来 堆到各列里\n",
    "So take the training examples, stack them in columns,\n",
    "\n",
    "，所以这也许就能做成一个n 或者n_x乘m维的矩阵\n",
    "so this becomes a n or maybe n_x by m dimensional matrix.\n",
    "\n",
    "，我直接把关键地方抖出来了 告诉你需要实现什么\n",
    "I'm just going to give away the punchline and tell you what you need to implement\n",
    "\n",
    "，才能把这个for循环变成向量化实现\n",
    "in order to have a vectorized implementation of this for loop.\n",
    "\n",
    "，实际上 你要计算的是Z^[1]等于W^[1] X + b^[1]\n",
    "Turns out what you need to do is compute Z^[1] equals W^[1]X + b^[1],\n",
    "\n",
    "，A^[1]等于σ(Z^[1])\n",
    "A^[1] equals sigmoid of Z^[1],\n",
    "\n",
    "，然后Z^[2就]等于W^[2]乘以A^[1] + b^[2]\n",
    "then Z^[2] equals W^[2] times A^[1] + b^[2],\n",
    "\n",
    "，然后A^[2]等于σ(Z^[2])\n",
    "and then A^[2] equals sigmoid of Z^[2].\n",
    "\n",
    "，所以如果你想的话 这就好比 把小x向量\n",
    "So if you want, the analogy is that we went from lowercase vector x's\n",
    "\n",
    "，堆叠到矩阵各列 构成大X矩阵\n",
    "to this capital case X matrix by stacking up the lowercase x's in different columns.\n",
    "\n",
    "，所以对于z 你也可以做同样的事情\n",
    "So if you do the same thing for the z's,\n",
    "\n",
    "，比如说 你可以取z^[1](1)\n",
    "so for example, if you take z^[1](1),\n",
    "\n",
    "，z^[1](2)等等 这些列向量一直排到z^[1](m)\n",
    "z^[1](2) and so on and these are all columns vectors up to z^[1](m), right,\n",
    "\n",
    "，所以这是第一个量\n",
    "so that's this first quantity,\n",
    "\n",
    "，但全部m个向量都以列向量堆叠起来\n",
    "but all m of them and stack them in columns,\n",
    "\n",
    "，这样就得到了矩阵Z^[1]\n",
    "then this gives you the matrix Z^[1].\n",
    "\n",
    "，同样 如果你看这个量 并且取a^[1](1)\n",
    "And similarly, if you look at say this quantity and take a^[1](1),\n",
    "\n",
    "，a^[1](2)等等一直到a^[1](m) 将它们以列向量堆叠起来\n",
    "a^[1](2) and so on and a^[1](m) and stack them up in columns,\n",
    "\n",
    "，这过程就和小x到大X的过程一样\n",
    "then this, just as we went from lowercase x's to capital case X\n",
    "\n",
    "，小z到大Z过程一样\n",
    "and lowercase z to capital case Z,\n",
    "\n",
    "，这样就从小a向量\n",
    "this goes from the lower case a, which are vectors,\n",
    "\n",
    "，变成那边的大A^[1]矩阵\n",
    "to this capital A^[1] over there.\n",
    "\n",
    "，同样 对于Z^[2]和A^[2]\n",
    "And similarly, for Z^[2] and A^[2],\n",
    "\n",
    "，也是通过\n",
    "they're also obtained by taking\n",
    "\n",
    "，将这些向量横向堆叠起来 然后\n",
    "these vectors and stacking them horizontally and then taking\n",
    "\n",
    "，再把这些向量横向堆叠起来\n",
    "these vectors and stacking them horizontally\n",
    "\n",
    "，就得到大写Z^[2]和大写A^[2]矩阵\n",
    "in order to get capital Z^[2] and capital A^[2].\n",
    "\n",
    "，这种写法可以帮助你们去想\n",
    "One of the property of this notation that might help you to think about it\n",
    "\n",
    "，这些矩阵 比如说Z和A\n",
    "is that these matrices say Z and A,\n",
    "\n",
    "，横向的话 我们有对所有训练样本用指标排序\n",
    "horizontally we're going to index across training examples,\n",
    "\n",
    "，所以横向指标就对应了不同的训练样本\n",
    "so that's why the horizontal index corresponds to different training examples.\n",
    "\n",
    "，当你从左到右扫的时候 就扫过了整个训练集\n",
    "As you sweep from left to right, you're scanning through the training set.\n",
    "\n",
    "，而在竖向 竖向指标就对应了神经网络里的不同节点\n",
    "And vertically, this vertical index corresponds to different nodes in the neural network.\n",
    "\n",
    "，所以例如 这个节点\n",
    "So for example, this node,\n",
    "\n",
    "，这个值位于矩阵的最左上角\n",
    "this value at the top leftmost corner of the matrix\n",
    "\n",
    "，对应第一个训练样本 第一个隐藏单元的激活函数\n",
    "corresponds to the activation of the first hidden unit on the first training example,\n",
    "\n",
    "，下面一个值就对应了\n",
    "one value down corresponds to\n",
    "\n",
    "，第二个隐藏单元对第一个训练样本的激活函数\n",
    "the activation in the second hidden unit on the first training example,\n",
    "\n",
    "，还有第三个隐藏单元对第一个训练样本 以此类推\n",
    "and the third hidden unit on the first training example and so on.\n",
    "\n",
    "，所以当你扫下来时\n",
    "So as you scan down,\n",
    "\n",
    "，这是隐藏单元的指标\n",
    "this is you indexing into the hidden unit's number,\n",
    "\n",
    "，如果你往横向移动的话\n",
    "whereas if you move horizontally,\n",
    "\n",
    "，就从第一个训练样本的第一个隐藏单元\n",
    "then you're going from the first hidden unit in the first training example\n",
    "\n",
    "，移动到第二个训练样本的第一个隐藏单元\n",
    "to now the first hidden unit in the second training example,\n",
    "\n",
    "，再移动到第三个训练样本 等等\n",
    "the third training example and so on\n",
    "\n",
    "，直到这里的节点对应最后第m个训练样本\n",
    "until this node here corresponds to the activation of the first hidden unit\n",
    "\n",
    "，第一个隐藏单元的激活函数为止\n",
    "in the final training example in the mth training example.\n",
    "\n",
    "，所以横向 矩阵A会扫过不同的训练样本\n",
    "So the horizontal, the matrix A goes over our different training examples,\n",
    "\n",
    "，竖向是矩阵A中的不同指标\n",
    "and vertically, the different indices in the matrix A\n",
    "\n",
    "，同样的形式也适用于矩阵Z和矩阵X\n",
    "And a similar intuition holds true for the matrix Z as well as was for X,\n",
    "\n",
    "，横向对应的是不同训练样本\n",
    "where horizontally corresponds to different training examples and\n",
    "\n",
    "，竖向对应不同的输入特征\n",
    "vertically corresponds to different input features,\n",
    "\n",
    "，这其实是神经网络输入层的不同节点\n",
    "which are really different nodes in the input layer of the neural network.\n",
    "\n",
    "，所以通过这些方程 你就知道如何实现\n",
    "So, with these equations you know know how to implement a neural network\n",
    "\n",
    "，把不同样本向量化的神经网络算法\n",
    "with vectorization that is the vectorization across multiple examples\n",
    "\n",
    "，在下一个视频中 我要给你们讲更多的理由 说明\n",
    "In the next video, I'm gonna show you more justification about\n",
    "\n",
    "，为什么这是向量化的正确实现\n",
    "why this is a correct implementation of this type of vectorization\n",
    "\n",
    "，事实证明这些理由\n",
    "It turns out that justification will be\n",
    "\n",
    "，和Logistics回归中见到的理由是很类似的\n",
    "similar to what you had seen for logistics regression\n",
    "\n",
    "，我们继续看下一个视频\n",
    "Let's go on to the next video\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### <font color=#0099ff>重点总结：</font>\n",
    "\n",
    "\n",
    "参考文献：\n",
    "\n",
    "[1]. 大树先生.[吴恩达Coursera深度学习课程 DeepLearning.ai 提炼笔记（1-3）-- 浅层神经网络](http://blog.csdn.net/koala_tree/article/details/78059952)\n",
    "\n",
    "\n",
    "---\n",
    " \n",
    "**PS: 欢迎扫码关注公众号：「SelfImprovementLab」！专注「深度学习」，「机器学习」，「人工智能」。以及 「早起」，「阅读」，「运动」，「英语 」「其他」不定期建群 打卡互助活动。**\n",
    "\n",
    "<center><img src=\"http://upload-images.jianshu.io/upload_images/1157146-cab5ba89dfeeec4b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\"></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
