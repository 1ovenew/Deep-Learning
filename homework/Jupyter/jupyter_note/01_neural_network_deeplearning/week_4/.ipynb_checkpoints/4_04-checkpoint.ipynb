{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursera | Andrew Ng (01-week-4-4.4)—Getting your matrix dimensions  right\n",
    "\n",
    "该系列仅在原课程基础上部分知识点添加个人学习笔记，或相关推导补充等。如有错误，还请批评指教。在学习了 Andrew Ng 课程的基础上，为了更方便的查阅复习，将其整理成文字。因本人一直在学习英语，所以该系列以英文为主，同时也建议读者以英文为主，中文辅助，以便后期进阶时，为学习相关领域的学术论文做铺垫。- ZJ\n",
    "    \n",
    ">[Coursera 课程](https://www.coursera.org/specializations/deep-learning) |[deeplearning.ai](https://www.deeplearning.ai/) |[网易云课堂](https://mooc.study.163.com/smartSpec/detail/1001319001.htm)\n",
    "\n",
    "---\n",
    "   **转载请注明作者和出处：ZJ 微信公众号-「SelfImprovementLab」**\n",
    "   \n",
    "   [知乎](https://zhuanlan.zhihu.com/c_147249273)：https://zhuanlan.zhihu.com/c_147249273\n",
    "   \n",
    "   [CSDN]()：\n",
    "   \n",
    "\n",
    "---\n",
    "\n",
    "4.4  Getting your matrix dimensions  right 核对矩阵的维数  \n",
    "\n",
    "(字幕来源：网易云课堂)\n",
    "\n",
    "\n",
    "when implementing the deep neural network,one of the debugging tools I often use,to check the correctness of my code,is to pull a piece of paper,and just work through the dimensions and matrix are working with,so let me show you how to do that,since I hope this will make it easier for you\n",
    "\n",
    "\n",
    "当实现深度神经网络的时候，其中一个我常用的，检查代码是否有错的方法，是拿出一张纸，然后过一遍算法中矩阵的维数，下面我会给大家展示具体怎么做，希望也能帮助大家更容易地\n",
    "\n",
    "\n",
    "，实现自己的深度网络\n",
    ",to implement your deep net as well\n",
    "\n",
    "，这里L等于5\n",
    ",so capital L is equal to five and come down quickly\n",
    "\n",
    "，除去输入层以外数下来 总共有5层\n",
    ",not counting the input layer there are five layers here\n",
    "\n",
    "，总共4个隐层 一个输出层\n",
    ",so 4 hidden layers and one output layer\n",
    "\n",
    "，如果你想实现正向传播\n",
    ",and so if you implement forward propagation\n",
    "\n",
    "，第一步是z^[1]等于w^[1]\n",
    ",the first step will be z^[1] equals w^[1]\n",
    "\n",
    "，乘以输入特征x加上b^[1]\n",
    ",times the input features x plus b^[1]\n",
    "\n",
    "，我们现在可以先忽略掉偏置项b\n",
    ",so let's ignore the um bias terms b for now\n",
    "\n",
    "，只关注参数w\n",
    ",and focus on the parameters w now\n",
    "\n",
    "，这里的第一隐层有三个隐藏单元\n",
    ",this first hidden layer has three hidden units\n",
    "\n",
    "，请看图上 我标一下0 1 2 3 4 5层\n",
    ",so this is um layer 0 layer 1 layer 2 layer 3 layer 4 and layer 5\n",
    "\n",
    "，我们可以用上个视频的符号约定\n",
    ",so using the notation we had from the previous video\n",
    "\n",
    "，第一层隐藏单元数是n^[1]\n",
    ",we have the n^[1] which is the number of hidden units\n",
    "\n",
    "，所以n^[1]等于3\n",
    ",and layer 1 is equal to 3\n",
    "\n",
    "，接下来是n^[2]等于5 n^[3]等于4\n",
    ",and here we would have that n^[2] is equal to 5 n^[3] is equal to 4\n",
    "\n",
    "，n^[4]等于2 以及n5等于1\n",
    ",n^[4] is equal to 2 and n^[5] is equal to 1\n",
    "\n",
    "，到目前为止我们只看到过只有一个输出单元的神经网络\n",
    ",and so far we've only seen neural networks with a single output unit\n",
    "\n",
    "，在之后的课程里我们也会学\n",
    ",that later in later courses we'll talk about\n",
    "\n",
    "，有多个输出单元的神经网络\n",
    ",neural networks with multiple output units as well\n",
    "\n",
    "，最后回到输入层\n",
    ",and finally um for the input layer\n",
    "\n",
    "，n^[0]等于n_x等于2\n",
    ",we also have n^[0] is equal to n_x is equal to 2\n",
    "\n",
    "，然后我们来看看z w和x的维数\n",
    ",so now let's think about the dimensions of z w and x\n",
    "\n",
    "，z是第一个隐层的激活函数向量\n",
    ",z is the vector of activations for this first hidden layer\n",
    "\n",
    "，这里z维度是3*1\n",
    ",so z is going to be 3 by 1\n",
    "\n",
    "，也就是一个三维的向量\n",
    ",is going to be a 3 dimensional vector\n",
    "\n",
    "，我也可以写成(n^[1],1)维向量\n",
    ",so I'm going to write it as a (n^[1],1) dimensional vector\n",
    "\n",
    "，(n^[1],1)维度的矩阵 在这个情况下就是(3,1)\n",
    ",n^[1] by 1 dimensional matrix is 3 by 1 in this case\n",
    "\n",
    "，接着来看输入特征x\n",
    ",now how about the input features x\n",
    "\n",
    "，x在这里有2个输入特征 所以x的维度是2,1\n",
    ",x we have 2 input features so x is in this example 2x1\n",
    "\n",
    "，归纳起来x的维度是n^[0],1\n",
    ",but more generally it will be n^[0] by 1\n",
    "\n",
    "，所以我们需要W^[1]这个矩阵能够实现这样的结果\n",
    ",so what we need is for the matrix W^[1] to be something that\n",
    "\n",
    "，也就是当我们用W^[1]乘以一个(n^[0],1)向量时\n",
    ",when we multiply an n0 by 1 vector to it\n",
    "\n",
    "，我们会得到一个n^[1],1的向量\n",
    ",we get an n1 by 1 vector right\n",
    "\n",
    "，所以你有一个三维的向量\n",
    ",so you have sort of a 3 dimensional vector\n",
    "\n",
    "，它等于某个向量乘以一个二维向量\n",
    ",equals something times a 2 dimensional vector\n",
    "\n",
    "，所以根据矩阵乘法法则\n",
    ",and so by the rules of matrix multiplication\n",
    "\n",
    "，会得到一个3*2矩阵\n",
    ",this has got to be a 3 by 2 matrix right\n",
    "\n",
    "，因为(3*2)矩阵\n",
    ",because the 3 by 2 matrix\n",
    "\n",
    "，乘以2*1矩阵或者向量\n",
    ",times a 2 by 1 matrix or times a 2 by 1 vector\n",
    "\n",
    "，你会得到一个3*1的向量\n",
    ",that gives you a 3 by 1 vector\n",
    "\n",
    "，稍微概括一下结果\n",
    ",and more generally this is\n",
    "\n",
    "，会是一个(n^[1],n^[0])维度的矩阵\n",
    ",going to be an n^[1] by n^[0] dimensional matrix\n",
    "\n",
    "，然后我们需要考虑一下为什么\n",
    ",so what we're think about here is that\n",
    "\n",
    "，W^[1]的维度会是n^[1],n^[0]\n",
    ",the dimensions of W^[1] has to be n1 by n0\n",
    "\n",
    "，总结起来W^[l]的维度必须是(n^[l],n^[l-1])\n",
    ",and more generally the dimensions of w^[l] must be n^[l] by n^[l-1]\n",
    "\n",
    "，举个例子 这里w^[2]的维度\n",
    ",so for example the dimensions of w^[2] for this\n",
    "\n",
    "，w^[2]的维度必需得是(5,3) 也就是(n^[2],n^[1])\n",
    ",it will have to be 5 by 3 or um it will be n2 by n1\n",
    "\n",
    "，因为我们需要用w^[2]乘以a^[1]来计算Z^[2]\n",
    ",because we're going to compute w^[2] as w^[2] times a^[1]\n",
    "\n",
    "，再次 我们先不管偏置项b\n",
    ",and again let's ignore the bias for now\n",
    "\n",
    "，所以在这个情况下结果得是3*1 也就是说这个必须是5*1\n",
    ",but so this is going to be 3 by 1 and we need this to be 5 by 1\n",
    "\n",
    "，那么这个矩阵必须是5*3\n",
    ",and so this had better be 5 by 3\n",
    "\n",
    "，类似地 W^[3]的维度\n",
    ",and similarly W^[3] will be is really the dimension of\n",
    "\n",
    "，维度是(下一层的维数,前一层的维数)\n",
    ",the next layer comma the dimension of the previous layer\n",
    "\n",
    "，结果就是4,5\n",
    ",so this is going to be 4 by 5\n",
    "\n",
    "，而W^[4]是(2,4) W^[5]是(1,2)\n",
    ",W^[4] is going to be on 2 by 4 and w^[5] is going to be 1 by 2\n",
    "\n",
    "，好 做这种运算时 一般要检查的公式是\n",
    ",okay so the general formula to check is that\n",
    "\n",
    "，在实现第l层中矩阵的时候 矩阵的维度\n",
    ",when you're implementing the matrix for a layer l to the dimension of that matrix\n",
    "\n",
    "，会是n^[l],n^[l-1]\n",
    ",will be n^[l] by n^[l-1]\n",
    "\n",
    "，现在再来看向量b的维度\n",
    ",now let's think about the dimension of this vector b\n",
    "\n",
    "，b是一个(3,1)向量\n",
    ",this is going to be a 3 by 1 vector\n",
    "\n",
    "，如果你要做向量加法\n",
    ",so you have to add that to another 3 by 1 vector\n",
    "\n",
    "，你必须再加上同样(3,1)维度的向量\n",
    ",in order to get a 3 by 1 vector as the output\n",
    "\n",
    "，或者在这个例子中 你需要加上这个\n",
    ",or in this example you need to add this\n",
    "\n",
    "，维度是(5,1)\n",
    ",which is going to be 5 by 1\n",
    "\n",
    "，这个向量必须是(5,1)\n",
    ",so it's going to be another 5 by 1 vector\n",
    "\n",
    "，才能做向量加法\n",
    ",in order for you to know the sum of these two things\n",
    "\n",
    "，我在这用方块标出来它们本身都是5,1的向量\n",
    ",we have in boxes to be on itself a 5 by 1 vector\n",
    "\n",
    "，所以再概括一下\n",
    ",so the more general rule is that\n",
    "\n",
    "，在这个例子中左边这个b^[1]是n^[1],1\n",
    ",in the example on the left b^[1] is n^[1] by 1 right\n",
    "\n",
    "，那是(3,1)\n",
    ",that's 3 by 1\n",
    "\n",
    "，在第二个例子中 是n^[2],1\n",
    ",and in the second example it is um this is n^[2] by 1\n",
    "\n",
    "，一般来说 b^[l]的维度\n",
    ",and so the more general case is that b^[l]\n",
    "\n",
    "，应该是n^[l],1\n",
    ",should be n^[l] by 1 dimensional\n",
    "\n",
    "，希望这两个式子能帮助检查\n",
    ",so hopefully these two equations help you to double check\n",
    "\n",
    "，你的矩阵W的维度\n",
    ",that the dimensions of your matrices W\n",
    "\n",
    "，以及你的向量b的维度\n",
    ",as well as of your vectors b are the correct dimensions\n",
    "\n",
    "，当然了你如果在实现反向传播的话\n",
    ",and of course if you're implementing back propagation\n",
    "\n",
    "，那么dW的维度应该和W的维度相同\n",
    ",then the dimensions of dW should be the same as dimension of W\n",
    "\n",
    "，所以dW应该和W有相同维度\n",
    ",so dW should be the same dimension as W\n",
    "\n",
    "，然后db会和b的维度一样\n",
    ",and db should be the same dimension as b\n",
    "\n",
    "，我们还需要注意检查\n",
    ",now the other key set of quantities\n",
    "\n",
    "，z x 和a^[l]的维度\n",
    ",whose dimensions to check are these z x as well as a^[l]\n",
    "\n",
    "，还没怎么讲到\n",
    ",which we didn’t talk too much about here\n",
    "\n",
    "，但是因为z^[l]等于对应元素的g^[l](a^[l])\n",
    ",but because z^[l] is equal to g^[l](a^[l]) applied element wise\n",
    "\n",
    "，那么在这类网络中z和a的维度应该相等\n",
    ",then z and a should have the same dimension in these types of networks\n",
    "\n",
    "，依照惯例我们接下来看看向量化的实现过程\n",
    ",now let's see what happens when you have a vectorized implementation\n",
    "\n",
    "，这样就可以同时作用于多个样本\n",
    ",that looked at multiple examples at a time\n",
    "\n",
    "，即使实现过程已经向量化了\n",
    ",even for a vectorized implementation\n",
    "\n",
    "，W b dw 和db的维度应该始终是一样的\n",
    ",of course the dimensions of W b dW and db will stay the same\n",
    "\n",
    "，但是Z A以及X的维度\n",
    ",but the dimensions of Z A as well as X\n",
    "\n",
    "，会在向量化后发生变化\n",
    ",will change a bit in your vectorized implementation\n",
    "\n",
    "，所以之前的情况 z^[1]等于W^[1]乘以x加上b^[1]\n",
    ",so previously we had z^[1] equals W^[1] times x plus b^[1]\n",
    "\n",
    "，在那个情况下维度分别是(n^[1],1)\n",
    ",where this was n^[1] by 1\n",
    "\n",
    "，这是(n^[1],n^[0])\n",
    ",this was n^[1] by n^[0]\n",
    "\n",
    "，x的维度是(n^[0],1)\n",
    ",x was n^[0] by 1\n",
    "\n",
    "，b的维度是(n^[1],1)\n",
    ",and b was n^[1] by 1\n",
    "\n",
    "，现在当一切向量化之后\n",
    ",now in a vectorized implementation\n",
    "\n",
    "，Z^[1]应该等于W^[1]乘以X加上b^[1]\n",
    ",you would have Z^[1] equals W^[1] times X plus b^[1]\n",
    "\n",
    "，现在Z^[1]是\n",
    ",where now Z^[1] is obtained\n",
    "\n",
    "，从每一个单独的z^[1]的值叠加得到的\n",
    ",by taking the z^[1]s for the individual example\n",
    "\n",
    "，也就是z^[1](1) z^[1](2)\n",
    ",so there's z^[1](1) z^[1](2)\n",
    "\n",
    "，到z^[1](m)叠在一块儿的结果\n",
    ",up to z^[1](m) and stacking them as follows\n",
    "\n",
    "，出来的结果就是z^[1]\n",
    ",and this gives you z1\n",
    "\n",
    "，所以Z^[1]的维度不再是(n^[1],1)\n",
    ",so the dimension of z^[1] is that instead of being n^[1] by 1\n",
    "，维度变成(n^[1],m) 其中m是训练集大小\n",
    ",it ends up being n^[1] by m if m is the size your training set\n",
    "\n",
    "，W^[1]的维度还是一样的\n",
    ",the dimensions of W^[1] stays the same\n",
    "\n",
    "，还是n^[1],n^[0]\n",
    ",so still n^[1] by n^[0]\n",
    "\n",
    "，不过X不再是(n^[0],1)\n",
    ",and X instead of being n^[0] by 1\n",
    "\n",
    "，而是把所有训练样本水平叠在一块儿\n",
    ",is now all your training examples stands horizontally\n",
    "\n",
    "，现在的维度是(n^[0],m)\n",
    ",so is now n^[0]by m\n",
    "\n",
    "，你会发现当你把一个(n^[1],n^[0])矩阵\n",
    ",and so you notice that when you take a n^[1] by n^[0] matrix\n",
    "\n",
    "，乘以一个(n^[0],m)矩阵\n",
    ",and multiply that by n^[0] by m matrix\n",
    "\n",
    "，你会得到一个(n^[1],m)的矩阵\n",
    ",that together that actually gives you an n^[1] by m dimensional matrix as expected\n",
    "\n",
    "，最后一点关于b^[1]的细节 就是b^[1]的维度还是n^[1],1\n",
    ",now the final detail is that b^[1] is still n^[1] by 1\n",
    "\n",
    "，但是当你把这个加上b的时候\n",
    ",but when you take this and add it to b\n",
    "\n",
    "，再用一下Python的broadcasting\n",
    ",then through Python broadcasting\n",
    "\n",
    "，会复制成一个n^[1],m的矩阵\n",
    ",this will get duplicated into an n^[1] by m matrix\n",
    "\n",
    "，然后逐个元素相加\n",
    ",and added element wise\n",
    "\n",
    "，在上一页幻灯片里我们已经说过\n",
    ",so on the previous slide we talked about\n",
    "\n",
    "，W b dW和db的维度\n",
    ",the dimensions of W b dW and db\n",
    "\n",
    "，在之前那一页z^[l]和a^[l]的维度\n",
    ",here what we see is that whereas z^[l] um as well as a^[l]\n",
    "\n",
    "，应该是n^[1],1\n",
    ",are of dimension n^[1] by 1\n",
    "\n",
    "，在现在这页要改成\n",
    ",we have now instead that\n",
    "\n",
    "，大写的Z^[1]和A^[1]维度相应变成n^[1],m\n",
    ",capital Zl as well as capital Al are nl by m\n",
    "\n",
    "，还有个特别情况就是当l等于0时\n",
    ",and a special case of this is when l is equal to 0\n",
    "\n",
    "，对应的A^[0]\n",
    ",in which case A^[0]\n",
    "\n",
    "，也就等于输入特征向量x\n",
    ",which is equal to just your training set input features x\n",
    "\n",
    "，A^[0]的维度应该是(n^[0],m)\n",
    ",is going to be equal to n^[0] by m as expected\n",
    "\n",
    "，当然如果你在实现反向传播的话\n",
    ",and of course when you're implementing this um in back propagation\n",
    "\n",
    "，我们会发现在计算了dZ和dA之后\n",
    ",we'll see later you end up computing dZ as well as dA\n",
    "\n",
    "，会发现它们的维度跟Z和A是一样的\n",
    ",and so these will of course have the same dimension as Z and A\n",
    "\n",
    "，我希望刚刚举的例子\n",
    ",so I hope through exercise went through\n",
    "\n",
    "，能帮助大家搞清楚\n",
    ",helps clarified the dimensions of\n",
    "\n",
    "，需要用代码实现的各个矩阵的维度\n",
    ",the various matrices you'll be working with\n",
    "\n",
    "，如果你想做深度神经网络的反向传播\n",
    ",when you implement back-propagation for deep neural network\n",
    "\n",
    "，在你写代码的时候\n",
    ",so long as you work through your code\n",
    "\n",
    "，一定要确认所有的矩阵维数是前后一致的\n",
    ",and make sure that all the matrices dimensions are consistent\n",
    "\n",
    "，这会大大地帮助你\n",
    ",that will usually help you\n",
    "\n",
    "，排除一些bug的来源\n",
    ",go some ways toward eliminating some cause of possible bugs\n",
    "\n",
    "，希望作业也能帮助你\n",
    ",so I hope that exercise for figuring out\n",
    "\n",
    "，真正弄明白不同矩阵的维度\n",
    ",the dimensions of the various matrices you'll be working with is helpful\n",
    "\n",
    "，当你自己去实现深度神经网络时\n",
    ",when you implement a deep neural network\n",
    "\n",
    "，如果你非常清晰地知道\n",
    ",if you keep straight the dimensions of\n",
    "\n",
    "，各个矩阵和向量的维度\n",
    ",these various matrices and vectors you're working with\n",
    "\n",
    "，这会帮助你排除程序里的一些错误\n",
    ",hopefully they'll help you eliminate some cause for possible bugs\n",
    "\n",
    "，起码对我自己调试的时候发现很管用\n",
    ",it certainly helps me get my code right\n",
    "\n",
    "，那么我们之前已经看过一些\n",
    ",so next we've now seen some of the mechanics of\n",
    "\n",
    "，如何在神经网络里实现正向传播算法了\n",
    ",how to do certain forward propagation in a neural network\n",
    "\n",
    "，机智的你一定很好奇 为啥深度神经网络这么好用\n",
    ",but why are deep neural networks so effective\n",
    "\n",
    "，又为啥它们就是比浅层一些的模型好用\n",
    ",and why do they do better than shallow representations\n",
    "\n",
    "，在下一个视频里我们会探讨一下这个问题\n",
    ".let's spend a few minutes in the next video to discuss that\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### <font color=#0099ff>重点总结：</font>\n",
    "\n",
    "\n",
    "参考文献：\n",
    "\n",
    "[1]. 大树先生.[吴恩达Coursera深度学习课程 DeepLearning.ai 提炼笔记（1-4）-- 浅层神经网络](http://blog.csdn.net/koala_tree/article/details/78087711)\n",
    "\n",
    "\n",
    "---\n",
    " \n",
    "**PS: 欢迎扫码关注公众号：「SelfImprovementLab」！专注「深度学习」，「机器学习」，「人工智能」。以及 「早起」，「阅读」，「运动」，「英语 」「其他」不定期建群 打卡互助活动。**\n",
    "\n",
    "<center><img src=\"http://upload-images.jianshu.io/upload_images/1157146-cab5ba89dfeeec4b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\"></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
