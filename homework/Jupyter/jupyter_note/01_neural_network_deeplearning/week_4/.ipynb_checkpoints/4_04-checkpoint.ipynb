{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursera | Andrew Ng (01-week-4-4.4)—Getting your matrix dimensions  right\n",
    "\n",
    "该系列仅在原课程基础上部分知识点添加个人学习笔记，或相关推导补充等。如有错误，还请批评指教。在学习了 Andrew Ng 课程的基础上，为了更方便的查阅复习，将其整理成文字。因本人一直在学习英语，所以该系列以英文为主，同时也建议读者以英文为主，中文辅助，以便后期进阶时，为学习相关领域的学术论文做铺垫。- ZJ\n",
    "    \n",
    ">[Coursera 课程](https://www.coursera.org/specializations/deep-learning) |[deeplearning.ai](https://www.deeplearning.ai/) |[网易云课堂](https://mooc.study.163.com/smartSpec/detail/1001319001.htm)\n",
    "\n",
    "---\n",
    "   **转载请注明作者和出处：ZJ 微信公众号-「SelfImprovementLab」**\n",
    "   \n",
    "   [知乎](https://zhuanlan.zhihu.com/c_147249273)：https://zhuanlan.zhihu.com/c_147249273\n",
    "   \n",
    "   [CSDN]()：\n",
    "   \n",
    "\n",
    "---\n",
    "\n",
    "4.4  Getting your matrix dimensions  right 核对矩阵的维数  \n",
    "\n",
    "(字幕来源：网易云课堂)\n",
    "\n",
    "\n",
    "when implementing the deep neural network,one of the debugging tools I often useis to pull a piece of paperso let me show you how to do thatto implement your deep net as wellnot counting the input layer there are five layers hereand so if you implement forward propagationtimes the input features x plus b^[1]and focus on the parameters w nowso this is um layer 0 layer 1 layer 2 layer 3 layer 4 and layer 5we have the n^[1] which is the number of hidden unitsand here we would have that n^[2] is equal to 5 n^[3] is equal to 4and so far we've only seen neural networks with a single output unitneural networks with multiple output units as wellwe also have n^[0] is equal to n_x is equal to 2z is the vector of activations for this first hidden layeris going to be a 3 dimensional vectorn^[1] by 1 dimensional matrix is 3 by 1 in this casex we have 2 input features so x is in this example 2x1so what we need is for the matrix W^[1] to be something thatwe get an n1 by 1 vector rightequals something times a 2 dimensional vectorthis has got to be a 3 by 2 matrix righttimes a 2 by 1 matrix or times a 2 by 1 vectorand more generally this isso what we're think about here is thatand more generally the dimensions of w^[l] must be n^[l] by n^[l-1]it will have to be 5 by 3 or um it will be n2 by n1and again let's ignore the bias for nowand so this had better be 5 by 3the next layer comma the dimension of the previous layerW^[4] is going to be on 2 by 4 and w^[5] is going to be 1 by 2when you're implementing the matrix for a layer l to the dimension of that matrixnow let's think about the dimension of this vector bso you have to add that to another 3 by 1 vectoror in this example you need to add thisso it's going to be another 5 by 1 vectorwe have in boxes to be on itself a 5 by 1 vectorin the example on the left b^[1] is n^[1] by 1 rightand in the second example it is um this is n^[2] by 1should be n^[l] by 1 dimensionalthat the dimensions of your matrices Wand of course if you're implementing back propagationso dW should be the same dimension as Wnow the other key set of quantitieswhich we didn’t talk too much about herethen z and a should have the same dimension in these types of networksthat looked at multiple examples at a timeof course the dimensions of W b dW and db will stay the samewill change a bit in your vectorized implementationwhere this was n^[1] by 1x was n^[0] by 1now in a vectorized implementationwhere now Z^[1] is obtainedso there's z^[1](1) z^[1](2)and this gives you z1it ends up being n^[1] by m if m is the size your training setso still n^[1] by n^[0]is now all your training examples stands horizontallyand so you notice that when you take a n^[1] by n^[0] matrixthat together that actually gives you an n^[1] by m dimensional matrix as expectedbut when you take this and add it to bthis will get duplicated into an n^[1] by m matrixso on the previous slide we talked abouthere what we see is that whereas z^[l] um as well as a^[l]we have now instead thatand a special case of this is when l is equal to 0which is equal to just your training set input features xand of course when you're implementing this um in back propagationand so these will of course have the same dimension as Z and Ahelps clarified the dimensions ofwhen you implement back-propagation for deep neural networkand make sure that all the matrices dimensions are consistentgo some ways toward eliminating some cause of possible bugsthe dimensions of the various matrices you'll be working with is helpfulif you keep straight the dimensions ofhopefully they'll help you eliminate some cause for possible bugsso next we've now seen some of the mechanics ofbut why are deep neural networks so effectivelet's spend a few minutes in the next video to discuss that\n",
    "\n",
    "当实现深度神经网络的时候,其中一个我常用的，是拿出一张纸，下面我会给大家展示具体怎么做，实现自己的深度网络，除去输入层以外数下来 总共有5层，如果你想实现正向传播，乘以输入特征x加上b^[1]，只关注参数w，请看图上 我标一下0 1 2 3 4 5层，第一层隐藏单元数是n^[1]，接下来是n^[2]等于5 n^[3]等于4，到目前为止我们只看到过只有一个输出单元的神经网络，有多个输出单元的神经网络，n^[0]等于n_x等于2，z是第一个隐层的激活函数向量，也就是一个三维的向量，(n^[1],1)维度的矩阵 在这个情况下就是(3,1)，x在这里有2个输入特征 所以x的维度是2,1，所以我们需要W^[1]这个矩阵能够实现这样的结果，我们会得到一个n^[1],1的向量，它等于某个向量乘以一个二维向量，会得到一个3*2矩阵，乘以2*1矩阵或者向量，稍微概括一下结果，然后我们需要考虑一下为什么，总结起来W^[l]的维度必须是(n^[l],n^[l-1])，w^[2]的维度必需得是(5,3) 也就是(n^[2],n^[1])，再次 我们先不管偏置项b，那么这个矩阵必须是5*3，维度是(下一层的维数,前一层的维数)，而W^[4]是(2,4) W^[5]是(1,2)，在实现第l层中矩阵的时候 矩阵的维度，现在再来看向量b的维度，如果你要做向量加法，或者在这个例子中 你需要加上这个，这个向量必须是(5,1)，我在这用方块标出来它们本身都是5,1的向量，在这个例子中左边这个b^[1]是n^[1],1，在第二个例子中 是n^[2],1，应该是n^[l],1，你的矩阵W的维度，当然了你如果在实现反向传播的话，所以dW应该和W有相同维度，我们还需要注意检查，还没怎么讲到，那么在这类网络中z和a的维度应该相等，这样就可以同时作用于多个样本，W b dw 和db的维度应该始终是一样的，会在向量化后发生变化，在那个情况下维度分别是(n^[1],1)，x的维度是(n^[0],1)，现在当一切向量化之后，现在Z^[1]是，也就是z^[1](1) z^[1](2)，出来的结果就是z^[1]，维度变成(n^[1],m) 其中m是训练集大小，还是n^[1],n^[0]，而是把所有训练样本水平叠在一块儿，你会发现当你把一个(n^[1],n^[0])矩阵，你会得到一个(n^[1],m)的矩阵，但是当你把这个加上b的时候，会复制成一个n^[1],m的矩阵，在上一页幻灯片里我们已经说过，在之前那一页z^[l]和a^[l]的维度，在现在这页要改成，还有个特别情况就是当l等于0时，也就等于输入特征向量x，当然如果你在实现反向传播的话，会发现它们的维度跟Z和A是一样的，能帮助大家搞清楚，如果你想做深度神经网络的反向传播，一定要确认所有的矩阵维数是前后一致的，排除一些bug的来源，真正弄明白不同矩阵的维度，如果你非常清晰地知道，这会帮助你排除程序里的一些错误，那么我们之前已经看过一些，机智的你一定很好奇 为啥深度神经网络这么好用，在下一个视频里我们会探讨一下这个问题，\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### <font color=#0099ff>重点总结：</font>\n",
    "\n",
    "\n",
    "参考文献：\n",
    "\n",
    "[1]. 大树先生.[吴恩达Coursera深度学习课程 DeepLearning.ai 提炼笔记（1-4）-- 浅层神经网络](http://blog.csdn.net/koala_tree/article/details/78087711)\n",
    "\n",
    "\n",
    "---\n",
    " \n",
    "**PS: 欢迎扫码关注公众号：「SelfImprovementLab」！专注「深度学习」，「机器学习」，「人工智能」。以及 「早起」，「阅读」，「运动」，「英语 」「其他」不定期建群 打卡互助活动。**\n",
    "\n",
    "<center><img src=\"http://upload-images.jianshu.io/upload_images/1157146-cab5ba89dfeeec4b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\"></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
