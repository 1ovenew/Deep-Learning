{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursera | Andrew Ng (01-week-4-4.6)—xxxxxx\n",
    "\n",
    "该系列仅在原课程基础上部分知识点添加个人学习笔记，或相关推导补充等。如有错误，还请批评指教。在学习了 Andrew Ng 课程的基础上，为了更方便的查阅复习，将其整理成文字。因本人一直在学习英语，所以该系列以英文为主，同时也建议读者以英文为主，中文辅助，以便后期进阶时，为学习相关领域的学术论文做铺垫。- ZJ\n",
    "    \n",
    ">[Coursera 课程](https://www.coursera.org/specializations/deep-learning) |[deeplearning.ai](https://www.deeplearning.ai/) |[网易云课堂](https://mooc.study.163.com/smartSpec/detail/1001319001.htm)\n",
    "\n",
    "---\n",
    "   **转载请注明作者和出处：ZJ 微信公众号-「SelfImprovementLab」**\n",
    "   \n",
    "   [知乎](https://zhuanlan.zhihu.com/c_147249273)：https://zhuanlan.zhihu.com/c_147249273\n",
    "   \n",
    "   [CSDN]()：\n",
    "   \n",
    "\n",
    "---\n",
    "\n",
    "4.6 搭建深层神经网络块\n",
    "\n",
    "这周的前几个视频(字幕来源：网易云课堂)\n",
    "in the earlier videos from this week\n",
    "\n",
    "，和之前几周的视频里\n",
    ",as well as from the videos from the past several weeks\n",
    "\n",
    "，你已经看到过\n",
    ",you've already seen the basic building blocks\n",
    "\n",
    "，正向反向传播的基础组成部分了\n",
    ",of forward propagation and back propagation\n",
    "\n",
    "，它们也是深度神经网络的重要组成部分\n",
    ",the key components you need to implement a deep neural network\n",
    "\n",
    "，现在我们来用它们建一个深度神经网络\n",
    ".let's see how you can put these components together to build a deep net\n",
    "\n",
    "，这是一个层数较少的神经网络 我们选择其中一层\n",
    ".Here's a network with a few layers let's pick one layer\n",
    "\n",
    "，从这一层的计算着手\n",
    ",and look at the computations focusing on just that layer for now\n",
    "\n",
    "，在第l层你有参数W^[l]和b^[l]\n",
    ",so for layer L you have some parameters W^[l] and b^[l]\n",
    "\n",
    "，正向传播里有输入的激活函数\n",
    ",and for the forward prop you will input the activations\n",
    "\n",
    "，输入是前一层a^[l-1] 输出是a^[l]\n",
    ",a^[l-1] from the previous layer and output a^[l]\n",
    "\n",
    "，我们之前讲过z^[l]\n",
    ",so the way we did this previously was you compute z^[l]\n",
    "\n",
    "，等于w^[l]乘以a^[l]减1加上b^[l]\n",
    ",equals w^[l] x a^[l-1] plus b^[l] um\n",
    "\n",
    "，a^[l]又等于g^[l](z^[l])\n",
    ",and then a^[l] equals g^[l](z^[l]) right\n",
    "\n",
    "，那么这就是你如何从输入a^[l-1]走到输出的\n",
    ",so that's how you go from the input al minus one to the output al\n",
    "\n",
    "，之后你就可以\n",
    ",and it turns out that for later use\n",
    "\n",
    "，把z^[l]的值缓存起来\n",
    ",will be useful to also cache the value z^[l]\n",
    "\n",
    "，我在这里也会把这包括在缓存中\n",
    ",so let me include this on cache as well because storing the value z^[l]\n",
    "\n",
    "，因为缓存的z^[l]对以后的正向反向传播的步骤非常有用\n",
    ",will be useful forward backward for the back propagation step later\n",
    "\n",
    "，然后是反向步骤 或者说反向传播步骤\n",
    ",and then for the backward step or three for the back propagation step\n",
    "\n",
    "，同样也是第l层的计算\n",
    ",again focusing on computation for this layer l\n",
    "\n",
    "，你会需要实现一个函数 输入为da^[l]\n",
    ",you're going to implement a function that inputs da^[l]\n",
    "\n",
    "，输出da^[l-1]的函数\n",
    ",and outputs da^[l-1]\n",
    "\n",
    "，一个小细节需要注意 输入在这里其实是da^[l]\n",
    ",and just refreshing the details the input is actually da^[l]\n",
    "\n",
    "，以及所缓存的z^[l]值\n",
    ",as well as the cache so you have available to you\n",
    "\n",
    "，之前计算好的z^[l]值\n",
    ",the value of z^[l] that you compute it\n",
    "\n",
    "，除了输出g^[l-1]的值以外\n",
    ",and in addition to outputting g^[l-1]\n",
    "\n",
    "，也需要输出你需要的梯度\n",
    ",you will output you know the gradients you want\n",
    "\n",
    "，这是为了实现梯度下降学习\n",
    ",in order to implement gradient descent for learning ok\n",
    "\n",
    "，这就是基本的正向步骤的结构\n",
    ",so this is the basic structure of how you implement this forward step\n",
    "\n",
    "，我把它成为称为正向函数\n",
    ".I'm going to call it a forward function\n",
    "\n",
    "，类似的在反向步骤中会称为反向函数\n",
    ",as well as backward step we shall call it backward function\n",
    "\n",
    "，总结起来就是\n",
    ",so just to summarize in layer l you're going to\n",
    "\n",
    "，在l层你会有正向函数\n",
    ",have you know the forward step or the forward prop forward function\n",
    "\n",
    "，输入a^[l-1]并且输出a^[l] 为了计算结果\n",
    "i,nput a^[l-1] and output a^[l] and in order to make this computation\n",
    "\n",
    "，你需要用W^[l]和b^[l]\n",
    ",you need to use W^[l] and b^[l] um\n",
    "\n",
    "，以及输出到缓存的z^[l]\n",
    ",and also output a cache which contains z^[l]\n",
    "\n",
    "，然后用作反向传播的反向函数\n",
    ",and then on the backward function using the back prop step\n",
    "\n",
    "，是另一个函数\n",
    ",will be another function\n",
    "\n",
    "，输入da^[l] 输出da^[l-1]\n",
    ",then now inputs da^[l] and outputs da^[l-1]\n",
    "\n",
    "，你就会得到对激活函数的导数\n",
    ",so it tells you given the derivatives respect to these activations\n",
    "\n",
    "，也就是da^[l] 这导数是多少呢? 我希望是什么?\n",
    ",that's da^[l] what are the derivatives or how much do i wish\n",
    "\n",
    "，a^[l-1]是会变的\n",
    ",you know a^[l-1] changes\n",
    "\n",
    "，前一层算出的激活函数导数\n",
    ",computed derivatives respect to the activations from the previous layer\n",
    "\n",
    "，在这个方块里你需要w^[l]和b^[l]\n",
    ",within this box right you need to use w^[l] and b^[l]\n",
    "\n",
    "，最后你要算的是dz^[l]\n",
    ",and it turns out along the way you end up computing dz^[l] um\n",
    "\n",
    "，然后这个方块中 这个反向函数\n",
    ",and then this box this backward function\n",
    "\n",
    "，可以计算输出dW^[l]和db^[l]\n",
    ",can also output dW^[l] and db^[l]\n",
    "\n",
    "，我会用红色箭头标注标注反向步骤\n",
    ",well now sometimes using red arrows to denote the backward iterations\n",
    "\n",
    "，如果你们喜欢 我可以把这些箭头涂成红色\n",
    ",so if you prefer we could draw these arrows in red\n",
    "\n",
    "，然后如果实现了这两个函数\n",
    ",so if you can implement these two functions\n",
    "\n",
    "，然后神经网络的计算过程会是这样的\n",
    ",then the basic computation of the neural network will be as follows\n",
    "\n",
    "，把输入特征a^[0]\n",
    ",you're going to take the input features a^[0]\n",
    "\n",
    "，放入第一层并计算第一层的激活函数\n",
    ".Feed that in and that will compute the activations of the first layer\n",
    "\n",
    "，用a^[1]表示 你需要w^[1]和b^[1]来计算\n",
    ".let's call that a^[1] and to do that you needed w^[1] and b^[1]\n",
    "\n",
    "，之后也缓存z^[l]值\n",
    ",and then we'll also you know cache the value z^[l]\n",
    "\n",
    "，之后喂到第二层\n",
    ",now having done that you feed that\n",
    "\n",
    "，第二层里 需要用到w^[2]和b^[2]\n",
    ",this is the second layer and then using w^[2] and b^[2]\n",
    "\n",
    "，你会需要计算第二层的激活函数a^[2] 后面几层以此类推\n",
    ",you're going to compute the activations our next layer a^[2]1 and so on\n",
    "\n",
    "，直到最后你算出了\n",
    ",until eventually you end up\n",
    "\n",
    "，a^[L]第L层的最终输出值y帽\n",
    ",outputting a capital l which is equal to y hat\n",
    "\n",
    "，在这些过程里我们缓存了所有的z值\n",
    ",and along the way we cashed all of these on values z\n",
    "\n",
    "，这就是正向传播的步骤\n",
    ",so that's the forward propagation step\n",
    "\n",
    "，对反向传播的步骤而言\n",
    ",now for the back propagation step what we're going to do\n",
    "\n",
    "，我们需要算一系列的反向迭代\n",
    ",will be a backward sequence of iterations\n",
    "\n",
    "，就是这样反向计算梯度\n",
    ",in which you're going backwards and computing gradients like so\n",
    "\n",
    "，你需要把da^[l]的值放在这里\n",
    ",so as you're going to feed in here da^[l]\n",
    "\n",
    "，然后这个方块会给我们da^[l-1]的值 以此类推\n",
    ",and then this box will give us da^[l-1] and so on\n",
    "\n",
    "，直到我们得到da^[2]和da^[1]\n",
    ",until we get da^[2] da^[1]\n",
    "\n",
    "，你还可以计算多一个输出值 就是da^[0]\n",
    ",you could actually get one more output to compute da^[0]\n",
    "\n",
    "，但这其实是你的输入特征的导数 并不重要\n",
    ",but this is derivative respect your input features which is not useful\n",
    "\n",
    "，起码对于训练监督学习的权重不算重要\n",
    ",at least for training the weights of these are supervised neural networks\n",
    "\n",
    "，你可以止步于此\n",
    ",so you could just stop it there\n",
    "\n",
    "，反向传播步骤中也会输出dw^[l]和db^[l]\n",
    ",along the way back prop also ends up outputting dw^[l] db^[l] right\n",
    "\n",
    "，这个会输出W^[L]和b^[L]\n",
    ",this used upon so W^[L] and b^[L]\n",
    "\n",
    "，这会输出dw^[3]和db^[3]等等\n",
    ",this would output dw^[3] db^[3] and so on\n",
    "\n",
    "，目前为止你算好了所有需要的导数\n",
    ",so you end up computing all the derivatives you need\n",
    "\n",
    "，稍微填一下这个流程图\n",
    ",just a maybe to fill in the structure a little bit more right\n",
    "\n",
    "，这些方块需要用到参数w^[l]和b^[l]\n",
    ",these boxes will use those parameters of slow w^[l] b^[l]\n",
    "\n",
    "，我们之后会看到在这些方块中\n",
    ",and it turns out that we'll see later\n",
    "\n",
    "，在这些方块里 我们最后会计算dz\n",
    ",that inside these boxes we'll end up computing dz as well\n",
    "\n",
    "，神经网络的一步训练 包含了\n",
    ",so one iteration of training for a neural network involves\n",
    "\n",
    "，从a^[0]开始\n",
    ",starting with a^[0]\n",
    "\n",
    "，也就是x 然后经过一系列正向传播计算得到y帽\n",
    ",which is x and going through for profit as follows computing y hat\n",
    "\n",
    "，之后再用输出值计算这个\n",
    ",and then using that to compute this\n",
    "\n",
    "，再实现反向传播\n",
    ",and then back prop right doing that\n",
    "\n",
    "，现在你就有所有的导数项了\n",
    ",and now you have all these derivative terms\n",
    "\n",
    "，w也会在每一层被更新为\n",
    ",and so you know w will get updated as\n",
    "\n",
    "，每一层里 W减去学习率乘以dW\n",
    ",some w minus the learning rate times d^[w] right for each of the layers\n",
    "\n",
    "，b也一样\n",
    ",and similarly for b right\n",
    "\n",
    "，反向传播就都计算完毕 我们有所有的导数值\n",
    ",now we've compute the back prop and have all these derivatives\n",
    "\n",
    "，那么这是神经网络一个梯度下降循环\n",
    ",so that's one iteration of gradient descent for your neural network\n",
    "\n",
    "，继续下去之前再补充一个细节\n",
    ",now before moving on just one more implementational detail\n",
    "\n",
    "，概念上会非常有帮助\n",
    ",conceptually will be useful to\n",
    "\n",
    "，那就是把反向函数\n",
    ",think of the cache here as storing\n",
    "\n",
    "，计算出来的z值缓存下来\n",
    ",the value of Z for the backward functions\n",
    "\n",
    "，当你做编程练习的时候去实现它时\n",
    ",but when you implement this you see this in the programming exercise\n",
    "\n",
    "，你会发现缓存可能很方便\n",
    ",when you implement it you find that the cache may be a convenient way\n",
    "\n",
    "，可以在分享传播函数迅速得到W^[1]和b^[1]的值\n",
    ",to get the value of the parameters at W^[1] b^[1]\n",
    "\n",
    "，非常方便的一个方法\n",
    ",into the backward function as well\n",
    "\n",
    "，在编程练习中你缓存了z\n",
    ",so the program exercise you actually store the cache is z\n",
    "\n",
    "，还有W和b 对吧\n",
    ",as well as W and b all right\n",
    "\n",
    "，缓存z^[2] W^[2] 从实现角度上看\n",
    ",so to store z^[2] W^[2] be to go from an implementational standpoint\n",
    "\n",
    "，我认为是一个很方便的方法\n",
    ".I just find this a convenient way\n",
    "\n",
    "，可以将参数复制到\n",
    ",to just you know get the parameters copied to\n",
    "\n",
    "，你在计算反向传播时\n",
    ",where you need to need to use them later\n",
    "\n",
    "，所需要的地方\n",
    ",when you're computing back propagation\n",
    "\n",
    "，好 这就是实现过程的细节\n",
    ",so that's just an implementational detail that you see\n",
    "\n",
    "，做编程练习时会用到\n",
    ",when you do the programming exercise\n",
    "\n",
    "，现在你们见过\n",
    ",so you've now seen one of the basic building blocks\n",
    "\n",
    "，实现深度神经网络的基本元件\n",
    ",for implementing a deep neural network\n",
    "\n",
    "，在每一层中 有一个正向传播步骤\n",
    ".In each layer there's a forward propagation step\n",
    "\n",
    "，以及对应的反向传播步骤\n",
    ",and there's a corresponding backward propagation step\n",
    "\n",
    "，以及把信息从一步传递到另一步的缓存\n",
    ",and as cache deposit information from one to the other\n",
    "\n",
    "，下一个视频我们会\n",
    ",in the next video we'll talk about\n",
    "\n",
    "，这些元件具体实现过程\n",
    ",how you can actually implement these building blocks\n",
    "\n",
    "，我们来看下一个视频吧。\n",
    ".let's go into the next video\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### <font color=#0099ff>重点总结：</font>\n",
    "\n",
    "\n",
    "参考文献：\n",
    "\n",
    "[1]. 大树先生.[吴恩达Coursera深度学习课程 DeepLearning.ai 提炼笔记（1-4）-- 浅层神经网络](http://blog.csdn.net/koala_tree/article/details/78087711)\n",
    "\n",
    "\n",
    "---\n",
    " \n",
    "**PS: 欢迎扫码关注公众号：「SelfImprovementLab」！专注「深度学习」，「机器学习」，「人工智能」。以及 「早起」，「阅读」，「运动」，「英语 」「其他」不定期建群 打卡互助活动。**\n",
    "\n",
    "<center><img src=\"http://upload-images.jianshu.io/upload_images/1157146-cab5ba89dfeeec4b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\"></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
