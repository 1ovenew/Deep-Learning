{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursera | Andrew Ng (01-week-4-4.X)—xxxxxx\n",
    "\n",
    "该系列仅在原课程基础上部分知识点添加个人学习笔记，或相关推导补充等。如有错误，还请批评指教。在学习了 Andrew Ng 课程的基础上，为了更方便的查阅复习，将其整理成文字。因本人一直在学习英语，所以该系列以英文为主，同时也建议读者以英文为主，中文辅助，以便后期进阶时，为学习相关领域的学术论文做铺垫。- ZJ\n",
    "    \n",
    ">[Coursera 课程](https://www.coursera.org/specializations/deep-learning) |[deeplearning.ai](https://www.deeplearning.ai/) |[网易云课堂](https://mooc.study.163.com/smartSpec/detail/1001319001.htm)\n",
    "\n",
    "---\n",
    "   **转载请注明作者和出处：ZJ 微信公众号-「SelfImprovementLab」**\n",
    "   \n",
    "   [知乎](https://zhuanlan.zhihu.com/c_147249273)：https://zhuanlan.zhihu.com/c_147249273\n",
    "   \n",
    "   [CSDN]()：\n",
    "   \n",
    "\n",
    "---\n",
    "\n",
    "4.2 前向和反向传播\n",
    "\n",
    "(字幕来源：网易云课堂)\n",
    "\n",
    "\n",
    "In a previous video you saw,the basic blocks of implementing a deep neural network,a forward propagation step for each layer,and a corresponding backward propagation step.let's see how you can actually implement these steps.we'll start at forward propagation,recall that what this will do is input a^[l-1],and output a^[l] and the cache z^[l],and we just said that from implementational point of view,maybe we'll cache w^[l] and b^[l] as well,just to make the function call easier in different exercises,and so the equations for this should already look familiar,the way to improve the forward function is just,this equals w^[l] x a^[l-1] plus b^[l],and then al equals the activation function applied to z\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "在之前的视频你看过了，构成深度神经网络的基本模块，比如每一层都有的前向传播步骤，以及一个相对的反向传播步骤，现在我们来看看如何实现这些步骤，我们从前向传播开始，回忆一下 这里的符号表示 输入a^[l-1]，输出是a^[l] 缓存是z^[l]，我们就可以从实现的角度来说，可能也可以缓存一下w^[l]和b^[l]，这样就可以更容易地在不同的练习中调用函数\n",
    "，那么这些等式应该看起来比较熟悉了\n",
    "，更新前向函数的方法是\n",
    "，z^[l] 等于w^[l]乘以a^[l-1]加b^[l]\n",
    "，然后a^[l]等于作用于z的激活函数\n",
    "，如果你想要向量化的实现过程\n",
    ",and if you want a vectorized implementation\n",
    "，那就是Z^[l]等于W^[l]乘以A^[l-1]加b\n",
    ",then it's just that times A l minus 1 plus b\n",
    "，其中b是python广播\n",
    ",where the b being python broadcasting\n",
    "，A^[l]等于g作用于Z^[l]的每一个元素\n",
    ",and A^[l] equals g applied element wise to Z^[l]\n",
    "，你应该还记得在图中的第四步\n",
    ",and you'll remember on the diagram for the forth step\n",
    "，也就是我们有这一系列往前走的小盒子\n",
    ",where we have this chain of boxes going forward\n",
    "，需要喂入 A^[0] 也就是X 来初始化\n",
    ",so you initialize that with feeding in A^[0], which is equal to X\n",
    "，初始化是第一层的输入值 对吧\n",
    ",so you initialize this really was the input to the first one right\n",
    "，a^[0]就是\n",
    ",it's really a0 which is\n",
    "，对于一个训练样本的输入特征\n",
    ",the input features to either for one training example\n",
    "，如果你每一次只做一个样本 或者\n",
    ",if you're doing one example at a time or um\n",
    "，如果针对一整个训练集的话输入特征就是A0\n",
    ",A^[0] the entire training so if you are processing the entire training set\n",
    "，所以这就是这条链的第一个前向函数的输入\n",
    ",so that's the input to the first forward function in the chain\n",
    "，重复这个步骤\n",
    ",and then just repeating this allows you\n",
    "，就能从左到右计算前向传播\n",
    ",to compute forward propagation from left to right\n",
    "，下面我们讲一下反向传播的步骤\n",
    ",next let's talk about the backward propagation step\n",
    "，这里的输入是da^[l] 输出是da^[l-1] dW^[l] 和db^[l]\n",
    ",here you go is the input da^[l] and output da^[l-1] and dW^[l] & db^[l]\n",
    "，我在这会写下来计算所需要的步骤\n",
    ",let me just write out the steps you need to compute these things\n",
    "，dz^[l]等于da^[l]对应相乘g^[l]'(z^[l])的每一个元素\n",
    ",dz^[l] is equal to da^[l] element-wise product with g^[l]'(z^[l])\n",
    "，然后求导dw^[l] 等于dz^[l]乘以a^[l-1]\n",
    ",and then compute the derivatives dw^[l] equals dz^[l] times a^[l-1]\n",
    "，我没有特别把这一步放进缓存\n",
    ".I didn't explicitly put that in the cache\n",
    "，但之后可能会需要\n",
    ",where it turns out you need this as well\n",
    "，db^[l]等于dz^[l]\n",
    ",and then db^[l] is equal to dz^[l]\n",
    "，最后da^[l-1]等于W^[l]的转置阵乘以dz^[l]\n",
    ",and finally da^[l-1] there's equal to W^[l] transpose times dz^[l]\n",
    "，我省略了求导过程的很多细节\n",
    ",ok and I don't want to go through the detailed derivation for this\n",
    "，但是最后的结果是\n",
    ",but it turns out that\n",
    "，如果你把da的定义代入这个式子\n",
    ",if you take this definition to da and plug it in here\n",
    "，那么你会得到一个和之前一样的式子\n",
    ",then you get the same formula as we had in there previously\n",
    "，也就是用来计算前一个dz^[l]函数的dz^[l]\n",
    ",for how you compute dz^[l] as a function of the previous dz l\n",
    "，实际上如果我只把它代入到 会得到dz^[l]\n",
    ",in fact well if I just plug that in here you end up that dz^[l]\n",
    "，等于W^[l+1]的转置阵dz^[l+1]乘以g^[l]’(z^[l])\n",
    ",is equal to W^[l+1] transpose dz^[l+1] times g^[l]’(z^[l])\n",
    "，我知道这里有很多代数运算\n",
    ".I know this is a looks like a lot of algebra and\n",
    "，你可以自己确认一下 这就是之前\n",
    ",you can actually double check for yourself that this is the equation\n",
    "，我在上周曾经写过的反向传播的等式\n",
    ",where I've written down for back propagation last week\n",
    "，我们之前建的是单隐层网络\n",
    ",when we were doing in your network with just a single hidden layer\n",
    "，提醒一下大家这次是逐个元素相乘\n",
    ",and as you reminder this times this element-wise product\n",
    "，但是你依然只需要\n",
    ",but so all you need is\n",
    "，这4个等式来实现反向函数\n",
    ",those four equations to implement your backward function\n",
    "，最后我会写出来这个向量化的版本\n",
    ",and then finally I'll just write out the vectorized version\n",
    "，那第一行是dz^[l]\n",
    ",so the first line becomes dz^[l]\n",
    "，等于dA^[l]乘以对应元素的g[l]’(z[l])\n",
    ",equals dA^[l] element-wise product with g[l]’(z[l])\n",
    "，这应该还挺明显的\n",
    ",may be no surprise there\n",
    "，dW^[l]等于1/m乘以dz^[l] 再乘以A^[l-1]的转置阵\n",
    ",dW^[l] becomes 1 over m dz^[l] times A^[l-1] transpose\n",
    "，db^[l]等于1/m\n",
    ",and then db^[l] becomes one over m\n",
    "，乘以np.sum(dz^[l], axis=1, keepdims=True)\n",
    ",np dot sum dz^[l] then axis is equal to one keepdims equals true\n",
    "，我们在上周讨论过\n",
    ",we talked about the use of\n",
    "，用来计算db的np.sum的用法\n",
    ",np dot sum in the previous week to compute db\n",
    "，最后dA^[l-1]等于W^[l]的转置阵乘以dz^[l]\n",
    ",and then finally dA^[l-1] is W^[l] transpose times dz^[l]\n",
    "，这可以用来作为da的输入值\n",
    ",so this allows you to input this quantity da over here\n",
    "，以及dw^[l]和db^[l]的输出值\n",
    ",and output on dw^[l] db^[l]\n",
    "，以及一些需要求的导数 还有da^[l-1]\n",
    ",the derivatives you need as well as da^[l-1] right as follows\n",
    "，所以这就是实现反向函数的方法\n",
    ",so that's how you implement the backward function\n",
    "，最后小结一下 用x作为输入\n",
    ",so just to summarize um take the input x\n",
    "，第一层你可能会有一个修正线性单元激活函数\n",
    ",you might have the first layer maybe has a ReLU activation function\n",
    "，然后第二层可能会用\n",
    ",then go to the second layer maybe uses\n",
    "，另一个修正线性单元激活函数\n",
    ",another ReLU activation function\n",
    "，到第三层可能是sigmoid函数\n",
    ",goes to the third layer maybe has a sigmoid activation function\n",
    "，如果你想做二分分类 输出值是y帽\n",
    ",if you're doing binary classification and this outputs y hat\n",
    "，用y帽你可以计算出损失\n",
    ",and then using Y hat you can compute the loss\n",
    "，这样你就可以开始向后迭代\n",
    ",and this allows you to start your backward iteration\n",
    "，我先画把箭头全画好 这样就不用经常换笔啦\n",
    ".I draw the arrows first I guess I don't have to change pens too much\n",
    "，接下来你就可以用反向传播求导\n",
    ",where you we'll then have back prop compute the derivatives\n",
    "，用来求dw^[3] db^[3] dw^[2] db^[2] dw^[1] db^[1]\n",
    ",compute your dw^[3] db^[3] dw^[2] db^[2] dw^[1] db^[1]\n",
    "，在这途中你会计算\n",
    ",and along the way you would be computing\n",
    "，缓存会把z^[1] z^[2] z^[3]传递过来\n",
    ".I guess the cache will transfer z^[1] z^[2] z^[3]\n",
    "，然后这里回传da^[2]和da^[1]\n",
    ",and here are you pass backward da^[2] and da^[1]\n",
    "，可以用来计算da^[0] 但是我们不会直接用\n",
    ",this could compute da^[0] but we won't use that\n",
    "，你可以直接抛弃\n",
    ",so you can just discard that\n",
    "，好啦 现在讲完了一个三层网络的\n",
    ",and so this is how you implement\n",
    "，前向和反向传播\n",
    ",forward prop and back prop for a three-layer your network\n",
    "，现在就只剩下最后一点细节我还没讲\n",
    ",now there's just one last detail delight didn't talk about\n",
    "，就是前向递归\n",
    ",which is for the forward recursion\n",
    "，我们会用输入数据x来初始化\n",
    ",we would initialize it with the input data x\n",
    "，那么反向递归呢\n",
    ",how about the backward recursion\n",
    "，当你用Logistic回归\n",
    ",well it turns out that um da of l when you're using logistic regression\n",
    "，做二分分类的时候 da^[l]\n",
    ",when you're doing binary classification is equal to\n",
    "，等于-y/a+(1-y)/(1-a)\n",
    ",y over a plus 1 minus y over 1 minus a\n",
    "，然后相对于y帽的\n",
    ",so it turns out that the derivative of\n",
    "，损失函数的导数\n",
    ",the loss function respect to the output\n",
    "，可以被证明等于da^[l]的这个式子\n",
    ",with respect of y hat can be shown to be equal to this\n",
    "，如果你对微积分很熟悉 可以再研究一下损失函数L\n",
    ",if you're familiar with calculus if you look up the loss function L\n",
    "，对y帽和a求导\n",
    ",and take derivatives respect to y hat with respect to a\n",
    "，你就可以证明你能得到这个公式\n",
    ",you can show that you get that formula\n",
    "，这个da的公式应该用在L\n",
    ",so this is the formula you should use\n",
    "，也就是用在最后一层上\n",
    ",for da for the final layer capital L\n",
    "，当然了如果你想要向量化这个实现过程\n",
    ",and of course if you were to have a vectorized implementation\n",
    "，那就需要初始化反向递归\n",
    ",then you initialize the backward recursion\n",
    "，那就不能用这个公式 而应该用dA^[l]的公式\n",
    ",not with this but with da capital A for the layer l\n",
    "，不一样的情况\n",
    ",which is going to be you know the same thing\n",
    "，但是同样的道理 -y/a\n",
    ",for the different examples y over a\n",
    "，在第一个训练样本中\n",
    ",for the first training example\n",
    "，dA^[l]就是-y/a + (1-y)/(1-a)\n",
    ",plus 1 minus y for the first training example over 1 minus a\n",
    "，从第一个例子 一直到\n",
    ",for the first training example dot dot dot\n",
    "，第m个训练样本\n",
    ",down to the  training example 1 minus a of M\n",
    "，这就实现向量化版本的方式\n",
    ",so that's how you ought to implement the vectorized version\n",
    "，换句话说就是\n",
    ",that's how you initialize\n",
    "，初始一个向量化反向传播的方法\n",
    ",the vectorized version of backward propagation\n",
    "，目前为止你已经看过了\n",
    ",so you've now seen\n",
    "，前向和反向传播的一些基石\n",
    ",the basic building blocks of both forward prop as well as back prop\n",
    "，如果你现在去尝试实现这些式子\n",
    ",now if you implement these equations\n",
    "，你会得到一个正确实现的前向和反向传播\n",
    ",you will get a correct implementation of forward prop and back prop\n",
    "，来得到你所需要算的导数\n",
    ",to get to the derivatives you need\n",
    "，你可能在吐槽 这也太多公式了吧\n",
    ",you might be thinking well there's a lot equations\n",
    "，可能懵了 没搞懂这都是咋回事\n",
    ".I'm slightly confused I'm not quite sure I see how this works\n",
    "，要是你这么觉得 我给你个建议\n",
    ",and if you're feeling that way my advice is\n",
    "，就是当你开始这周的编程作业时\n",
    ",when you get to this week's programming assignment\n",
    "，你需要亲自实现这一切\n",
    ",you will be able to implement these for yourself\n",
    "，那时候就会学的更加扎实\n",
    ",and there'll be much more concrete\n",
    "，我知道这里有好多式子\n",
    ",and I know there's a lot of equations\n",
    "，你不需要弄到所有式子\n",
    ",and maybe some equations doesn’t make complete sense\n",
    "，但是如果你稍微补补微积分和线性代数\n",
    ",but if you work through the calculus and the linear algebra\n",
    "，虽然并不容易 但是还是要试试\n",
    ",which is not easy so you know feel free to try\n",
    "，况且这还是机器学习里面\n",
    ",but that's actually one of\n",
    "，相对比较难的推导了\n",
    ",the more difficult derivations in machine learning\n",
    "，所以其实刚刚讲的那些式子\n",
    ",it turns out the equations wrote down\n",
    "，都不过是用来在反向传播中\n",
    ",are just the calculus equations for\n",
    "，求导的微积分公式\n",
    ",computing the derivatives especially in back prop\n",
    "，不过我要再强调一下\n",
    ",but once again if this feels\n",
    "，如果它们看起来很是抽象 有点神秘\n",
    ",a bit abstract a little bit mysterious to you\n",
    "，我的建议还是 做做作业\n",
    ",my advice is when you've done the problem exercise\n",
    "，做完就会觉得神清气爽\n",
    ",it will feel a bit more concrete to you\n",
    "，虽然我必须要说\n",
    ",although I have to say you know even today\n",
    "，到现在我在实现一个算法的时候\n",
    ",when I implement a learning algorithm\n",
    "，有时候我也会惊讶\n",
    ",sometimes even I'm surprised\n",
    "，怎么莫名其妙就成功了\n",
    ",when my learning algorithm implementation works\n",
    "，那是因为机器学习里的复杂性\n",
    ",and it's because lot of complexity of machine learning\n",
    "，是来源于数据本身\n",
    ",comes from the data\n",
    "，而不是一行行的代码\n",
    ",rather than from the lines of code\n",
    "，所有有时候你会感觉\n",
    ",so sometimes you feel like\n",
    "，你实现了几行代码 但是不太确定它具体做了什么\n",
    ",you implement a few lines of code not quite sure what it did\n",
    "，但是奇迹般地实现了\n",
    ",but it almost magically work\n",
    "，那是其实不是\n",
    ",and it's because of all the magic is\n",
    "，因为真正神奇的东西不是你写的程序\n",
    ",actually not in the piece of code you write\n",
    "，通常情况下你的代码段不会很长 虽然也不会太简单\n",
    ",which is often you know not too long it's not it's not exactly simple\n",
    "，但是不会需要写一万或者十万行代码\n",
    ",but there's not you know 10,000 100,000 lines of code\n",
    "，但有时当你喂入超多数据之后\n",
    ",but you feed it so much data that sometimes\n",
    "，就算我已经搞机器学习这么长时间了\n",
    ",even though I work the machine learning for a long time\n",
    "，有时候也还是在算法管用的时候\n",
    ",sometimes it's so you know surprises me a bit\n",
    "，惊讶一下下\n",
    ",when my learning algorithm works\n",
    "，因为实际上你的算法的复杂性\n",
    ",because lots of complexity of your learning algorithm\n",
    "，来源于数据 而不是你写的代码\n",
    ",comes from the data rather than necessarily from your writing\n",
    "，因为你不需要写个几千行的代码\n",
    ",you know thousands and thousands of lines of code\n",
    "，好 这节课就讲了怎么实现深度神经网络\n",
    ",all right so that's um how do you implement deep neural networks\n",
    "，再重申一下 这些知识点会\n",
    ",and again this will become more concrete\n",
    "，在做完编程作业之后得到巩固\n",
    ",when you've done the programming exercise\n",
    "，在继续下一课之前 我想说一下\n",
    ",before moving on I want to discuss in the next video\n",
    "，之后我们会讨论超参数和参数\n",
    ",want to discuss hyper parameters and parameters\n",
    "，其实当你训练深度网络时\n",
    ",it turns out that when you're training deep nets\n",
    "，能够好好安排超参数\n",
    ",being able to organize your hyper params well\n",
    "，会帮助你提高开发网络的效率\n",
    ",will help you be more efficient in developing your networks\n",
    "，在下个视频中 我们会谈谈具体的原因\n",
    ".In the next video let's talk about exactly what that means\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### <font color=#0099ff>重点总结：</font>\n",
    "\n",
    "\n",
    "参考文献：\n",
    "\n",
    "[1]. 大树先生.[吴恩达Coursera深度学习课程 DeepLearning.ai 提炼笔记（1-4）-- 浅层神经网络](http://blog.csdn.net/koala_tree/article/details/78087711)\n",
    "\n",
    "\n",
    "---\n",
    " \n",
    "**PS: 欢迎扫码关注公众号：「SelfImprovementLab」！专注「深度学习」，「机器学习」，「人工智能」。以及 「早起」，「阅读」，「运动」，「英语 」「其他」不定期建群 打卡互助活动。**\n",
    "\n",
    "<center><img src=\"http://upload-images.jianshu.io/upload_images/1157146-cab5ba89dfeeec4b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\"></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
