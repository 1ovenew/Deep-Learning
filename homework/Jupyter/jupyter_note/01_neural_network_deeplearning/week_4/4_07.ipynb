{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursera | Andrew Ng (01-week-4-4.7)—xxxxxx\n",
    "\n",
    "该系列仅在原课程基础上部分知识点添加个人学习笔记，或相关推导补充等。如有错误，还请批评指教。在学习了 Andrew Ng 课程的基础上，为了更方便的查阅复习，将其整理成文字。因本人一直在学习英语，所以该系列以英文为主，同时也建议读者以英文为主，中文辅助，以便后期进阶时，为学习相关领域的学术论文做铺垫。- ZJ\n",
    "    \n",
    ">[Coursera 课程](https://www.coursera.org/specializations/deep-learning) |[deeplearning.ai](https://www.deeplearning.ai/) |[网易云课堂](https://mooc.study.163.com/smartSpec/detail/1001319001.htm)\n",
    "\n",
    "---\n",
    "   **转载请注明作者和出处：ZJ 微信公众号-「SelfImprovementLab」**\n",
    "   \n",
    "   [知乎](https://zhuanlan.zhihu.com/c_147249273)：https://zhuanlan.zhihu.com/c_147249273\n",
    "   \n",
    "   [CSDN]()：\n",
    "   \n",
    "\n",
    "---\n",
    "\n",
    "4.7 参数 VS 超参数\n",
    "\n",
    "\n",
    "想要你的深度神经网络起很好的效果 (字幕来源：网易云课堂)\n",
    "being effective in developing your deep neural nets requires that\n",
    "\n",
    "你还需要规划好你的参数\n",
    "you not only organize your parameters well\n",
    "\n",
    "以及超参数\n",
    "but also your hyper parameters\n",
    "\n",
    "那么啥是超参数呢\n",
    "so what are hyper parameters\n",
    "\n",
    "我们来看看模型里的参数W和b\n",
    "let's take a look so the parameters your model are W and b\n",
    "\n",
    "在学习算法中还有其他参数 需要输入到学习算法中\n",
    "and there are other things you need to tell your learning algorithm\n",
    "\n",
    "比如学习率α\n",
    "such as the learning rate alpha\n",
    "\n",
    "因为我们需要设置α来决定\n",
    "because we need to set alpha and that in turn will determine\n",
    "\n",
    "你的参数如何进化\n",
    "how your parameters evolve\n",
    "\n",
    "或者还有梯度下降法循环的数量\n",
    "or maybe the number of iterations of gradient descent you carry out\n",
    "\n",
    "在你的算法中也许也有其他你想要设置的数字\n",
    "your learning algorithm has other you know numbers that you need to set\n",
    "\n",
    "比如隐层数L\n",
    "such as the number of hidden layers so we call that capital L\n",
    "\n",
    "或是隐藏单元数\n",
    "or the number of hidden units right\n",
    "\n",
    "比如n^[1], n^[2], 等等\n",
    "such as n^[1], n^[2], ..\n",
    "\n",
    "你还可以选择激活函数\n",
    "and then you also have the choice of activation function\n",
    "\n",
    "在隐层中 用修正线性单元 还是tanh\n",
    "do you want to use a relu you or tangent or\n",
    "\n",
    "还是sigmoid函数\n",
    "a sigmoid little something especially in the hidden layers\n",
    "\n",
    "那么算法中的这些数字\n",
    "and so all of these things are things that\n",
    "\n",
    "都需要你来设置\n",
    "you need to tell your learning algorithm\n",
    "\n",
    "这些数字实际上控制了\n",
    "and so these are parameters\n",
    "\n",
    "最后参数W和b的值\n",
    "that control the ultimate parameters W and b and\n",
    "\n",
    "所以它们被称作超参数\n",
    "so we call all of these things below hyper parameters\n",
    "\n",
    "因为这些超参数 比如α即是学习率\n",
    "because these things like alpha the learning rate\n",
    "\n",
    "循环的数量 隐层的数量等等\n",
    "the number of iterations number of hidden layers and so on\n",
    "\n",
    "都是能够控制W和b的\n",
    "these are all parameters that control W and b\n",
    "\n",
    "所以这些东西称为超参数\n",
    "so we call these things hyper parameters\n",
    "\n",
    "因为这些超参数某种程度上\n",
    "because it is the hyper parameters that you know somehow\n",
    "\n",
    "决定了最终得到的W和b\n",
    "determine the final value of the parameters W and b that you end up with\n",
    "\n",
    "实际上深度学习有很多不同的超参数\n",
    "in fact deep learning has a lot of different hyper parameters\n",
    "\n",
    "之后我们也会过一下其他的超参数\n",
    "later in the later course we'll see other hyper parameters as well\n",
    "\n",
    "比如momentum 再比如mini batch的大小\n",
    "such as the momentum term the mini batch size\n",
    "\n",
    "几种不同的正则化参数等等\n",
    "various forms of regularization parameters and so on\n",
    "\n",
    "如果这些词你还不太确定是什么意思\n",
    "and if none of these terms at the bottom make sense yet\n",
    "\n",
    "不要担心 我们会在课程2里提到的\n",
    "don't worry about it we'll talk about them in the second course\n",
    "\n",
    "正因为深度学习有这么多的超参数\n",
    "because deep learning has so many hyper parameters\n",
    "\n",
    "和机器学习时代的早期相比\n",
    "in contrast to earlier eras of machine learning\n",
    "\n",
    "我会保持前后一致\n",
    "I'm going to try to be very consistent in\n",
    "\n",
    "把学习率α称为一个超参数\n",
    "calling the learning rate alpha a hyper parameter\n",
    "\n",
    "而不是参数\n",
    "rather than calling the parameter\n",
    "\n",
    "可能在早期的机器学习中\n",
    "I think in earlier eras of machine learning\n",
    "\n",
    "还没有那么多超参数\n",
    "when we didn't have so many hyper parameters\n",
    "\n",
    "我们很多人以前都很随便\n",
    "most of us used to be a bit sloppy here\n",
    "\n",
    "以前就把α称为参数\n",
    "and just call alpha a parameter\n",
    "\n",
    "但是技术上讲α是\n",
    "and technically alpha is a parameter\n",
    "\n",
    "一个控制实际参数的参数\n",
    "but is a parameter that determines the real parameters\n",
    "\n",
    "秉持前后一致的原则 我们应该把α这类参数\n",
    "so try to be consistent in calling these things like alpha\n",
    "\n",
    "循环的数量等等 称作超参数\n",
    "the number of iterations and so on hyper parameters\n",
    "\n",
    "所以当你自己着手于训练自己的深度神经网络时\n",
    "so when you're training a deep net for your own application\n",
    "\n",
    "你会发现超参数的选择\n",
    "you find that there may be a lot of possible settings\n",
    "\n",
    "有很多可能性\n",
    "for the hyper parameters\n",
    "\n",
    "所以你得尝试不同的值\n",
    "that you need to just try out\n",
    "\n",
    "今天的深度学习应用领域 还是很经验性的过程\n",
    "so applied deep learning today is a very empirical process\n",
    "\n",
    "通常你有个想法\n",
    "where often you might have an idea\n",
    "\n",
    "比如你可能大致知道\n",
    "for example you might have an idea\n",
    "\n",
    "一个最好的学习率值\n",
    "for the best value for the learning rate\n",
    "\n",
    "可能说α等于0.01最好\n",
    "you might say well maybe alpha equals 0.01\n",
    "\n",
    "我会想先试试看\n",
    "I want to try that\n",
    "\n",
    "然后你可以实际试一下 训练一下看看效果如何\n",
    "then you implemented try it out and then see how that works\n",
    "\n",
    "然后基于尝试的结果你会发现\n",
    "and then based on that outcome you might say\n",
    "\n",
    "你觉得学习率设定\n",
    "you know what I've changed online\n",
    "\n",
    "再提高到0.05会比较好\n",
    "I want to increase the learning rate to 0.05\n",
    "\n",
    "如果你不确定\n",
    "and so if you're not sure\n",
    "\n",
    "什么值是最好的\n",
    "what's the best value for the learning ready-to-use\n",
    "\n",
    "你大可以先试试一个学习率α\n",
    "you might try one value of the learning rate alpha\n",
    "\n",
    "再看看损失函数J的值有没有下降\n",
    "and see their cost function J go down like this\n",
    "\n",
    "然后你可以试一试大一些的值\n",
    "then you might try a larger value for the learning rate alpha\n",
    "\n",
    "然后发现损失函数的值增加并发散了\n",
    "and see the cost function blow up and diverge\n",
    "\n",
    "然后可能试试其他数\n",
    "then you might try another version\n",
    "\n",
    "看结果是否下降的很快 或者收敛到在更高的位置\n",
    "and see it go down really fast it's converge higher value\n",
    "\n",
    "你可能尝试不同的α 并观察\n",
    "you might try another version and see it\n",
    "\n",
    "损失函数J这么变了\n",
    "you know see the cost function J do that\n",
    "\n",
    "试试一组值 然后可能损失函数变成这样\n",
    "off to try a set of values, you might say okay looks like this\n",
    "\n",
    "这个α值会加快学习过程\n",
    "the value of alpha gives me a pretty fast learning\n",
    "\n",
    "并且收敛在更低的损失函数值上\n",
    "and allows me to converge to a lower cost function J\n",
    "\n",
    "我就用这个α值了\n",
    "I'm going to use this value of alpha\n",
    "\n",
    "在前面几页中\n",
    "you saw in a previous slide\n",
    "\n",
    "还有很多不同的超参数\n",
    "that there are a lot of different hyperparameters\n",
    "\n",
    "然而 当你开始开发新应用时\n",
    "and it turns out that when you're starting on the new application\n",
    "\n",
    "预先很难确切知道\n",
    "I should find it very difficult to know in advance exactly\n",
    "\n",
    "究竟超参数的最优值应该是什么\n",
    "what's the best value of the hyper parameters\n",
    "\n",
    "所以通常 你必须\n",
    "so what often happen is you just have to\n",
    "\n",
    "尝试很多不同的值\n",
    "try out many different values\n",
    "\n",
    "并走这个循环 试试各种参数\n",
    "and go around this cycle your trial some value\n",
    "\n",
    "试试看5个隐层 这个数目的隐藏单元\n",
    "really try five hidden layers with this many number of hidden units\n",
    "\n",
    "实现模型并观察是否成功 然后再迭代\n",
    "implement that see if it works and then iterate\n",
    "\n",
    "这页的标题是 应用深度学习领域\n",
    "so the title of this slide is that applied deep learning\n",
    "\n",
    "一个很大程度基于经验的过程\n",
    "is very empirical process\n",
    "\n",
    "凭经验的过程通俗来说\n",
    "and empirical process is maybe a fancy way of saying\n",
    "\n",
    "就是试 试 试 直到你找到合适的数值\n",
    "you just have to try a lot of things and see what works\n",
    "\n",
    "另一个近来深度学习的影响是\n",
    "another effect I've seen is that deep learning today\n",
    "\n",
    "它用于解决很多问题\n",
    "is applied to so many problems\n",
    "\n",
    "从计算机视觉到语音识别\n",
    "ranging from computer vision to speech recognition\n",
    "\n",
    "到自然语言处理\n",
    "to natural language processing\n",
    "\n",
    "到很多结构化的数据应用\n",
    "to a lot of structured data applications\n",
    "\n",
    "比如网络广告\n",
    "such as maybe a online advertising\n",
    "\n",
    "或是网页搜索 或产品推荐等等\n",
    "or web search or product recommendations and so on\n",
    "\n",
    "我所看到过的就有\n",
    "and what I've seen is that\n",
    "\n",
    "很多其中一个领域的研究员\n",
    "first I've seen researchers from one discipline\n",
    "\n",
    "这些领域中的一个  尝试了不同的设置\n",
    "any one of these try to go to a different one\n",
    "\n",
    "有时候这种设置超参数的直觉 可以推广\n",
    "and sometimes the intuitions about hyper parameters carries over\n",
    "\n",
    "但有时又不会\n",
    "and sometimes it doesn't\n",
    "\n",
    "所以我经常建议人们 特别是刚开始应用于新问题的人们\n",
    "so I often advise people especially when starting on a new problem\n",
    "\n",
    "去试一定范围的值看看结果如何\n",
    "to just try out a range of values and see what works\n",
    "\n",
    "然后下一门课程 我们会用更系统的方法\n",
    "and then the next course we'll see a systematic way\n",
    "\n",
    "用系统性的尝试各种超参数取值\n",
    "we'll see some systematic ways for trying out a range of values all right\n",
    "\n",
    "然后其次 甚至是你\n",
    "and second even if you're working on\n",
    "\n",
    "已经用了很久的模型\n",
    "one application for a long time you know\n",
    "\n",
    "可能你在做网络广告应用\n",
    "maybe you're working on online advertising\n",
    "\n",
    "在你开发途中\n",
    "as you make progress on the problem\n",
    "\n",
    "很有可能学习率的最优数值\n",
    "It is quite possible there the best value for the learning rate\n",
    "\n",
    "或是其他超参数的最优值 是会变的\n",
    "a number of hidden units and so on might change\n",
    "\n",
    "所以即使你每天都在用当前最优的参数调试你的系统\n",
    "so even if you tune your system to the best value\n",
    "\n",
    "你还是会发现\n",
    "of hyper parameters to daily as possible you find\n",
    "\n",
    "最优值过一年就会变化\n",
    "that the best value might change a year from now\n",
    "\n",
    "因为电脑的基础设施\n",
    "maybe because the computer infrastructure\n",
    "\n",
    "CPU或是GPU\n",
    "I'd be it you know CPUs or the type of GPU\n",
    "\n",
    "可能会变化很大\n",
    "running on or something has changed but\n",
    "\n",
    "所以有一条经验规律\n",
    "so maybe one rule of thumb is you know every now\n",
    "\n",
    "可能每几个月就会变\n",
    "and then maybe every few months\n",
    "\n",
    "如果你所解决的问题 需要很多年时间\n",
    "if you're working on a problem for an extended period of time for many years\n",
    "\n",
    "只要经常试试不同的超参数 勤于检验结果\n",
    "just try a few values for the hyper parameters and double check\n",
    "\n",
    "看看有没有更好的超参数数值\n",
    "if there's a better value for the hyper parameters and as you do\n",
    "\n",
    "相信你慢慢会得到设定超参数的直觉\n",
    "so you slowly gain intuition as well about the hyper parameters\n",
    "\n",
    "知道你的问题最好用什么数值\n",
    "that work best for your problems\n",
    "\n",
    "可能的确是深度学习\n",
    "and I know that this might seem like\n",
    "\n",
    "比较让人不满的一部分\n",
    "an unsatisfying part of deep learning\n",
    "\n",
    "也就是你必须尝试很多次不同可能性\n",
    "that you just have to try on all the values for these hyper parameters\n",
    "\n",
    "但参数设定这个领域\n",
    "but maybe this is one area\n",
    "\n",
    "深度学习研究还在进步中\n",
    "where deep learning research is still advancing\n",
    "\n",
    "所以可能过段时间就会有更好的方法\n",
    "and maybe over time we'll be able to give better guidance\n",
    "\n",
    "决定超参数的值\n",
    "for the best hyper parameters to use\n",
    "\n",
    "也很有可能由于CPU GPU 网络\n",
    "but it's also possible that because CPUs and GPUs and networks\n",
    "\n",
    "和数据都在变化\n",
    "and datasets are all changing\n",
    "\n",
    "这样的指南可能只会在一段时间内起作用\n",
    "and it is possible that the guidance won't to converge for some time\n",
    "\n",
    "只要你不断尝试 并且\n",
    "and you just need to keep trying out different values and\n",
    "\n",
    "尝试保留交叉检验或类似的检验方法\n",
    "evaluate them on a hold out cross-validation set or something\n",
    "\n",
    "然后挑一个对你的问题效果比较好的数值\n",
    "and pick the value that works for your problems\n",
    "\n",
    "以上我们简短地讨论完了超参数\n",
    "so that was a brief discussion of hyper parameters\n",
    "\n",
    "在课程2中我们会给更多具体的建议\n",
    "in the second course we'll also give some suggestions for\n",
    "\n",
    "关于如何系统化地探索超参数的可能空间\n",
    "how to systematically explore the space of hyper parameters\n",
    "\n",
    "到现在你应该已经有了\n",
    "but by now you actually have pretty much all the tools\n",
    "\n",
    "完成这次编程作业的工具\n",
    "you need to do their programming exercise\n",
    "\n",
    "在做练习之前我想要再分享一些想法\n",
    "before you do that adjust or share view one more set of ideas\n",
    "\n",
    "很多人经常问我 深度学习和人类大脑\n",
    "which is I often asked what does deep learning\n",
    "\n",
    "有什么样的关联\n",
    "have to do the human brain\n",
    "\n",
    "---\n",
    "\n",
    "4.8 这和大脑有什么关系？\n",
    "\n",
    "\n",
    "那么深度学习和大脑有什么关联性吗？(字幕来源：网易云课堂)\n",
    "so what a deep learning have to do the brain\n",
    "\n",
    "这句话可能有剧透嫌疑\n",
    "at the risk of giving away the punchline\n",
    "\n",
    "但是我觉得关联不大\n",
    "I would say not a whole lot\n",
    "\n",
    "我们来看看为什么人们做这样的类比\n",
    "but let's take a quick look at why people keep making the analogy\n",
    "\n",
    "为什么说深度学习和人类大脑相关\n",
    "between deep learning and the human brain\n",
    "\n",
    "当你实现一个神经网络时 这是你在做的东西\n",
    "when you implement a neural network this is what you do\n",
    "\n",
    "你会做正反向传播\n",
    "for prop and back prop\n",
    "\n",
    "其实很难表述这些公式\n",
    "and I think because it's been difficult to convey intuitions about\n",
    "\n",
    "具体做了什么 就是在做这些\n",
    "what these equations are doing really\n",
    "\n",
    "复杂函数的梯度下降法 到底具体在做什么\n",
    "gradient descent on a very complex function\n",
    "\n",
    "而这样的类比其实过度简化了\n",
    "the analogy that is like the brain has become\n",
    "\n",
    "我们的大脑具体在做什么\n",
    "really an oversimplified explanation for what this is doing\n",
    "\n",
    "但因为这种形式很简洁\n",
    "but the simplicity of this\n",
    "\n",
    "也让普通人更愿意公开讨论\n",
    "makes it you know kind of seductive for people to just say it publicly\n",
    "\n",
    "也方便新闻媒体报道\n",
    "as well as the media to report it\n",
    "\n",
    "并且吸引大众眼球\n",
    "and certainly caught the popular imagination\n",
    "\n",
    "但这个类比还是很粗略的\n",
    "and there is a very loose analogy between\n",
    "\n",
    "这是一个logistic回归单元的sigmoid激活函数\n",
    "let's say a logistic regression unit with a sigmoid activation function\n",
    "\n",
    "这里是一个大脑中的神经元\n",
    "and here's a cartoon of a single neuron in the brain\n",
    "\n",
    "图中这个生物神经元\n",
    "in this picture of a biological neuron on this neuron\n",
    "\n",
    "也是你大脑中的一个细胞\n",
    "which is a cell in your brain\n",
    "\n",
    "它能接受来自其他神经元的电信号 比如x1 x2 x3\n",
    "receives electric signals from you know other neuronsx1 x2 x3\n",
    "\n",
    "或可能来自于其他神经元a1 a2 a3\n",
    "or maybe from other neurons a1 a2 a3\n",
    "\n",
    "其中有一个简单的临界计算值\n",
    "there's a simple thresholded computation\n",
    "\n",
    "如果这个神经元突然激发了\n",
    "and then if this neuron fires\n",
    "\n",
    "它会让电脉冲沿着这条长长的轴突\n",
    "it sends a pulse of electricity down the axon\n",
    "\n",
    "或者说一条导线 传到另一个神经元\n",
    "down this long wire perhaps to other neurons\n",
    "\n",
    "所以这是一个过度简化的对比 把一个神经网络的逻辑单元\n",
    "so there is a very simplistic analogy between a single logistic unit\n",
    "\n",
    "和右边的生物神经元对比\n",
    "between a single neuron and network and a biological neuron like that shown on a right\n",
    "\n",
    "至今为止其实连神经科学家们都很难解释\n",
    "but I think that today even neuroscientists have almost no idea\n",
    "\n",
    "究竟一个神经元能做什么\n",
    "what even a single neuron is doing\n",
    "\n",
    "一个小小的神经元其实却是极其复杂的\n",
    "a single neuron appears to be much more complex than\n",
    "\n",
    "以至于我们无法在神经科学的角度描述清楚\n",
    "we are able to characterize with neuroscience\n",
    "\n",
    "它的一些功能 可能真的是类似logistic回归的运算\n",
    "and while some of what is doing is a little bit like logistic regression\n",
    "\n",
    "但单个神经元到底在做什么\n",
    "there's still a lot about what even a single neuron does that no one there\n",
    "\n",
    "目前还没有人能够真正解释\n",
    "no human today understands\n",
    "\n",
    "大脑中的神经元是怎么学习的\n",
    "for example exactly how neurons in the human brain learn\n",
    "\n",
    "至今这仍是一个谜之过程\n",
    "this is still a very mysterious process\n",
    "\n",
    "到底大脑\n",
    "and it's completely unclear today\n",
    "\n",
    "是用类似于后向传播\n",
    "whether the human brain uses an algorithm does anything\n",
    "\n",
    "或是梯度下降的算法\n",
    "like back propagation or gradient descent\n",
    "\n",
    "或者人类大脑的学习过程用的是完全不同的原理\n",
    "or if there's some fundamentally different learning principle that the human brain uses\n",
    "\n",
    "所以虽然深度学习的确是个很好的工具\n",
    "so when I think of deep learning I think of it as being very good\n",
    "\n",
    "能学习到各种很灵活很复杂的函数\n",
    "and learning very flexible functions very complex functions\n",
    "\n",
    "来学到从x到y的映射\n",
    "to learn x to y mappings\n",
    "\n",
    "在监督学习中 学到输入到输出的映射\n",
    "to learn input-output mappings in supervised learning\n",
    "\n",
    "但这种和人类大脑的类比\n",
    "and whereas it is like the brain analogy\n",
    "\n",
    "在这个领域的早期 也许值得一提 但现在\n",
    "maybe that was useful once I think the field has moved to the point\n",
    "\n",
    "这种类比已经逐渐过时了\n",
    "where that analogy is breaking down\n",
    "\n",
    "我自己也在尽量少用这样的说法\n",
    "and I tend not to use that analogy much anymore\n",
    "\n",
    "这就是神经网络和大脑的关系\n",
    "so that's it so neural networks and the brain\n",
    "\n",
    "我相信在计算机视觉\n",
    "I do think that maybe the field of computer vision\n",
    "\n",
    "或其他的学科都曾受人类大脑启发\n",
    "has taken a bit more inspiration from the human brains\n",
    "\n",
    "还有其他深度学习的领域也曾受人类大脑启发\n",
    "and other disciplines that also apply to deep  learning\n",
    "\n",
    "但是个人来讲我用这个人类大脑类比的次数\n",
    "but I personally use the analogy you know\n",
    "\n",
    "逐渐减少了\n",
    "to the human brain less than I used to\n",
    "\n",
    "差不多结束了\n",
    "so that's it for this video\n",
    "\n",
    "现在你知道怎么实现深度神经网络里\n",
    "you now know how to implement for prop and back prop\n",
    "\n",
    "梯度下降法的正反向传播了\n",
    "in gradient descent even for deep neural networks\n",
    "\n",
    "祝你做编程练习的时候好运\n",
    "best of luck with the programming exercise\n",
    "\n",
    "我很期待在第二门课 和大家分享更多的知识\n",
    "and I look forward to sharing more of these ideas of you in the second course\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### <font color=#0099ff>重点总结：</font>\n",
    "\n",
    "\n",
    "参考文献：\n",
    "\n",
    "[1]. 大树先生.[吴恩达Coursera深度学习课程 DeepLearning.ai 提炼笔记（1-4）-- 浅层神经网络](http://blog.csdn.net/koala_tree/article/details/78087711)\n",
    "\n",
    "\n",
    "---\n",
    " \n",
    "**PS: 欢迎扫码关注公众号：「SelfImprovementLab」！专注「深度学习」，「机器学习」，「人工智能」。以及 「早起」，「阅读」，「运动」，「英语 」「其他」不定期建群 打卡互助活动。**\n",
    "\n",
    "<center><img src=\"http://upload-images.jianshu.io/upload_images/1157146-cab5ba89dfeeec4b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\"></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
