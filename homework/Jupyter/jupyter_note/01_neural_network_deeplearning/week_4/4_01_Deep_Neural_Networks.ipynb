{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursera | Andrew Ng (01-week-4-4.1)—Deep Neural Network\n",
    "\n",
    "该系列仅在原课程基础上部分知识点添加个人学习笔记，或相关推导补充等。如有错误，还请批评指教。在学习了 Andrew Ng 课程的基础上，为了更方便的查阅复习，将其整理成文字。因本人一直在学习英语，所以该系列以英文为主，同时也建议读者以英文为主，中文辅助，以便后期进阶时，为学习相关领域的学术论文做铺垫。- ZJ\n",
    "    \n",
    ">[Coursera 课程](https://www.coursera.org/specializations/deep-learning) |[deeplearning.ai](https://www.deeplearning.ai/) |[网易云课堂](https://mooc.study.163.com/smartSpec/detail/1001319001.htm)\n",
    "\n",
    "---\n",
    "   **转载请注明作者和出处：ZJ 微信公众号-「SelfImprovementLab」**\n",
    "   \n",
    "   [知乎](https://zhuanlan.zhihu.com/c_147249273)：https://zhuanlan.zhihu.com/c_147249273\n",
    "   \n",
    "   [CSDN]()：\n",
    "   \n",
    "\n",
    "---\n",
    "\n",
    "4.1 Deep Neural Network （深层神经网络 ）\n",
    "\n",
    "(字幕来源：网易云课堂)\n",
    "\n",
    "欢迎听讲第四周的课程\n",
    "welcome to the fourth week of this course\n",
    "\n",
    "，到现在为止你应该见过\n",
    ",by now you've seen forward propagation and back propagation\n",
    "\n",
    "，单隐层神经网络中的前向和后向传播\n",
    ",in the context of a neural network with a single hidden layer\n",
    "\n",
    "，以及logistic回归\n",
    ",as well as logistic regression\n",
    "\n",
    "，并且也学了向量化\n",
    ",and you've learned about vectorization\n",
    "\n",
    "，和为什么随机初始化比较重要\n",
    ",and why it's important initialized the weight randomly\n",
    "\n",
    "，如果你已经完成了之前的作业\n",
    ",if you've done the past week's homework\n",
    "\n",
    "，我们也已经亲自实现过一些理念\n",
    ",we've also implemented and seen some of these ideas work for yourself\n",
    "\n",
    "，所以到现在你应该已经看过大多数的\n",
    ",so by now you've actually seen most of the ideas you need\n",
    "\n",
    "，实现深度神经网络的理念\n",
    ",to implement a deep neural network\n",
    "\n",
    "，这周我们将会学习如何把这些理念组合起来\n",
    ",what we're going to do in this week is take those ideas\n",
    "\n",
    "，从而实现你自己的\n",
    ",and put them together so that you'll\n",
    "\n",
    "，深度神经网络模型\n",
    ",be able to implement your own deep neural network\n",
    "\n",
    "，因为这周的练习题目会更长\n",
    ",because this we problem exercise is longer\n",
    "\n",
    "，可能会需要更多时间完成\n",
    ",and just has a bit more work\n",
    "\n",
    "，这周的视频会稍微剪短一些\n",
    ",going to keep the video so this week shorter\n",
    "\n",
    "，所以你可以更快一点看完视频\n",
    ",as you get to the videos a little bit more quickly\n",
    "\n",
    "，就有更多的时间\n",
    ",and then I'll have more time to do\n",
    "\n",
    "，在之后做一个编程大作业\n",
    ",a significant program exercise at the end\n",
    "\n",
    "，希望你能够搭建\n",
    ",which I hope will leave you having built\n",
    "\n",
    "，一个能让你引以为傲的深度学习模型\n",
    ",a deep neural network that you feel proud of\n",
    "\n",
    "，好那么什么是深度学习网络\n",
    ",so what is a deep neural network\n",
    "\n",
    "，你已经学过logistic回归\n",
    ",you've seen this picture for logistic regression\n",
    "\n",
    "，并且见过单隐层神经网络了\n",
    ",and you've also seen neural networks with a single hidden layer\n",
    "\n",
    "，这里有一个双隐层神经网络\n",
    ",so here is an example of a neural network with two hidden layers\n",
    "\n",
    "，以及一个五隐层的神经网络的例子\n",
    ",and a neural network with five hidden layers\n",
    "\n",
    "，我们说logistic回归是一个浅层模型\n",
    ",we say that logistic regression is a very shallow model\n",
    "\n",
    "，而这里的这个模型深要深得多\n",
    ",whereas this model here is a much deeper model\n",
    "\n",
    "，浅层或是深层是一个程度的问题\n",
    ",and shallow versus depth is a matter of degree\n",
    "\n",
    "，所以单隐层神经网络\n",
    ",so neural network of a single hidden layer\n",
    "\n",
    "，这是一个双层神经网络\n",
    ",this would be a two layer neural network\n",
    "\n",
    "，要记住当我们数神经网络有几层的时候\n",
    ",remember when we count layers in neural network\n",
    "\n",
    "，我们不能把输入层数进去 只算上隐层的数量\n",
    ",we don't count the input layer we just count the hidden layers\n",
    "\n",
    "，还算上输出层\n",
    ",as well as the output layer\n",
    "\n",
    "，那么这就是个双层神经网络 还是比较浅的\n",
    ",so this would be a two layer neural network is still quite shallow\n",
    "\n",
    "，但logistic回归更浅\n",
    ",but not as shallow as logistic regression\n",
    "\n",
    "，技术层面上说logistic回归是单层神经网络\n",
    ",technically logistic regression is a you know one layer neural network\n",
    "\n",
    "，但是前几年在人工智能或机器学习社区中\n",
    ",but over the last several years the AI or the machine learning community\n",
    "\n",
    "，大家发觉有些函数\n",
    ",has realized that there are functions\n",
    "\n",
    "，只有非常深层的神经网络能够学习\n",
    ",that very deep neural networks can learn\n",
    "\n",
    "，而浅一些的模型通常无法学习\n",
    ",that shallower models are often unable to\n",
    "\n",
    "，虽然处理任何具体问题的时候都会很难\n",
    ",although for any given problem it might be hard\n",
    "\n",
    "，预先准确地判断需要多深的神经网络\n",
    ",to predict in advance exactly how deep a neural network you would want\n",
    "\n",
    "，所以先试试看logistic回归是非常合理的做法\n",
    ",so it would be reasonable to try logistic regression\n",
    "\n",
    "，试一下单层然后两层\n",
    ",try one and then two hidden layers\n",
    "\n",
    "，然后把隐层数量当成另一个\n",
    ",and view the number of hidden layers as another hyper parameter\n",
    "\n",
    "，可以自由选择数值大小的超参数\n",
    ",that you could try a variety of values of\n",
    "\n",
    "，然后在保留交叉验证数据上评估\n",
    ",and evaluate on hold-out cross validation data\n",
    "\n",
    "，或者用你自己的开发集评估\n",
    ",or on your development set\n",
    "\n",
    "，后面也会讲到这个\n",
    ",say more about that later as well\n",
    "\n",
    "，那我们现在来过一遍\n",
    ".let's now go through the notation we used\n",
    "\n",
    "，用来描述深度神经网络的符号\n",
    ",to describe deep neural networks\n",
    "\n",
    "，这里是一个一二三四\n",
    ",here is a one two three four layer neural network\n",
    "\n",
    "，四层的有三个隐层的神经网络\n",
    ",with three hidden layers\n",
    "\n",
    "，然后隐层中的单元数目 是五 五 三\n",
    ",and the number of units in these hidden layers are i guess five five three\n",
    "\n",
    "，然后有一个输出单元\n",
    ",and then there's one output unit\n",
    "\n",
    "，那么我们要用的符号是大写的L\n",
    ",so the notation we're going to use it's going to use capital L\n",
    "\n",
    "，用来表示神经网络的层数\n",
    ",to denote the number of layers in the network\n",
    "\n",
    "，那这里L等于4\n",
    ",so in this case L is equal to four and\n",
    "\n",
    "，也就是层数\n",
    ",so that's the number of layers and\n",
    "\n",
    "，然后我们用n上标l来表示\n",
    ",we're going to use n superscript l to denote\n",
    "\n",
    "，节点的数量 或者小l层上的单元数量\n",
    ",the number of nodes or the number of units in layer lowercase l\n",
    "\n",
    "，所以当我们把输入层标为第0层的话\n",
    ",so if we index this the input as layer 0\n",
    "\n",
    "，这是第一层 这是第二层 这是第三层 然后这是第四层\n",
    ",this is layer 1 this is layer 2 this is layer 3 and this is layer 4\n",
    "\n",
    "，之后我们就有 举个例子n^[1]\n",
    ",then we have that for example n^[1]\n",
    "\n",
    "，也就是第一个隐层 单元数等于5\n",
    ",that would be this the first hidden layer would be equal to 5\n",
    "\n",
    "，因为这儿我们有5个隐藏单元\n",
    ",because we have 5 hidden units there\n",
    "\n",
    "，再一个我们来看看n^[2] 也就是第二隐层的单元数\n",
    ",for this one we have the n 2 the number of units in the second set\n",
    "\n",
    "，这里也等于5\n",
    ",in there is also equal to 5\n",
    "\n",
    "，n^[3]等于3 以及n^[4]也就是n^[L]\n",
    ",n^[3] is equal to 3 and n 4 which is n^[L]\n",
    "\n",
    "，这个输出单元数等于1\n",
    ",this number of units is this number of output units is equal to one\n",
    "\n",
    "，因为这里我们的L等于4\n",
    ",because here our capital L is equal to 4\n",
    "\n",
    "，那我们也得有输入层\n",
    ",and we're also going to have here that so the input layer\n",
    "\n",
    "，n^[0]也就是n_x 等于3\n",
    ",n^[0] is just equal to n_x is equal to 3\n",
    "\n",
    "，好咯 那么这些就会是我们之后会用到的\n",
    ",okay so that's the notation we'll use to describe\n",
    "\n",
    "，用来描述各个层中节点数的符号\n",
    ",the number of nodes we have in different layers\n",
    "\n",
    "，对于各个第l层\n",
    ",for each layer l also also going to\n",
    "\n",
    "，会用a^[l]来表示l层中的激活函数\n",
    ",use a^[l] to denote the activations in layer l\n",
    "\n",
    "，那么我们一会儿就会看到在前向传播中你最后要算的\n",
    ",so we'll see later that in forward propagation you end up computing\n",
    "\n",
    "，a^[l]是激活函数g(z^[l])\n",
    ",a^[l] as the activation g apply to z^[l]\n",
    "\n",
    "，激活函数也会用层数l来标注\n",
    ",perhaps the activation is indexed by the layer l as well\n",
    "\n",
    "，然后我们用W^[l]来表示\n",
    ",and then we'll use W^[l] to denote the weights\n",
    "\n",
    "，在a^[l]中计算z^[l]值的权重\n",
    ",for computing the values z^[l] in the a^[l]\n",
    "\n",
    "，z^[l]方程里的b^[l]也一样\n",
    ",and similarly b^[l] is used to compute z^[l]\n",
    "\n",
    "，最后总结一下符号约定\n",
    ",finally just to wrap up on the notation\n",
    "\n",
    "\n",
    "，输入特征用x表示\n",
    ",the input features are called x\n",
    "\n",
    "，但x也是第0层的激活函数\n",
    ",but x is also the activations of layer 0\n",
    "\n",
    "，那么a^[0]等于x\n",
    ",so a^[0] is equal to x\n",
    "\n",
    "，最后一层的激活函数\n",
    ",and the activation of the final layer\n",
    "\n",
    "，a^[L]等于y帽\n",
    ",a capital L is equal to Y hat\n",
    "\n",
    "，就是说a^[L]等于预测输出\n",
    ",so a superscript bracket capital L is equal to the predicted output\n",
    "\n",
    "，也就是这个神经网络预测出来的y帽\n",
    ",to prediction y hats of the neural network\n",
    "\n",
    "，那么现在你知道一个深度神经网络是啥样的了\n",
    ",so you now know what a deep neural network looks like\n",
    "\n",
    "，以及我们在深度网络中会用到的\n",
    ",as well as the notation we'll use to describe\n",
    "\n",
    "，以及用来计算的符号约定\n",
    ",and to compute for deep network\n",
    "\n",
    "，好啦我知道这个视频里我讲了好多符号\n",
    ".I know I've introduced a lot of notation in this video\n",
    "\n",
    "，如果你偶尔忘记一些符号的意思\n",
    ",but if you ever forget what some symbol means\n",
    "\n",
    "，我们也在课程网站上贴了解释符号约定的资料\n",
    ",we've also posted on the course website a notation sheet\n",
    "\n",
    "，你就可以去暗中观察\n",
    ",or notation guide you can use to look up\n",
    "\n",
    "，这些不同的符号表示什么\n",
    ",what these different symbols means\n",
    "\n",
    "，接下来我会描述一下在这类网络中\n",
    ",next I like to describe what forward propagation\n",
    "\n",
    "，前向传播是啥样的\n",
    ",in this type of network looks like\n",
    "\n",
    "，且听下回分解\n",
    ",let's go into the next video\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### <font color=#0099ff>重点总结：</font>\n",
    "\n",
    "\n",
    "参考文献：\n",
    "\n",
    "[1]. 大树先生.[吴恩达Coursera深度学习课程 DeepLearning.ai 提炼笔记（1-4）-- 浅层神经网络](http://blog.csdn.net/koala_tree/article/details/78087711)\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    " \n",
    "**PS: 欢迎扫码关注公众号：「SelfImprovementLab」！专注「深度学习」，「机器学习」，「人工智能」。以及 「早起」，「阅读」，「运动」，「英语 」「其他」不定期建群 打卡互助活动。**\n",
    "\n",
    "<center><img src=\"http://upload-images.jianshu.io/upload_images/1157146-cab5ba89dfeeec4b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\"></center>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
