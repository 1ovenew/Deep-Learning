{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursera | Andrew Ng (01-week-2-2.2)—Logistic Regression\n",
    "\n",
    " 该系列仅在原课程基础上部分知识点添加个人学习笔记，或相关推导补充等。如有错误，还请批评指教。在学习了 Andrew Ng 课程的基础上，为了更方便的查阅复习，将其整理成文字。因本人一直在学习英语，所以该系列以英文为主，同时也建议读者以英文为主，中文辅助，以便后期进阶时，为学习相关领域的学术论文做铺垫。- ZJ\n",
    "    \n",
    ">[Coursera 课程](https://www.coursera.org/specializations/deep-learning) |[deeplearning.ai](https://www.deeplearning.ai/) |[网易云课堂](https://mooc.study.163.com/smartSpec/detail/1001319001.htm)\n",
    "\n",
    "---\n",
    "   **转载请注明作者和出处：ZJ 微信公众号-「SelfImprovementLab」**\n",
    "   \n",
    "   [知乎](https://zhuanlan.zhihu.com/c_147249273)：https://zhuanlan.zhihu.com/c_147249273\n",
    "   \n",
    "   [CSDN]()：\n",
    "   \n",
    "\n",
    "---\n",
    "\n",
    "In this video, we'll go over logistic regression.This is a learning algorithm that you use when the output labels y in a supervised learning problem are all either zero or one,so for binary classification problems.Given an input feature vector x maybe corresponding to an image that you want to recognize as ,either a cat picture or not a cat picture,,you want an algorithm that can output a prediction,which we'll call y hat,which is your estimate of y.More formally, you want y hat to be the probability of the chance that,y is equal to one given the input features x.So in other words, if x is a picture,as we saw in the last video,you want y hat to tell you,what is the chance that this is a cat picture.So x, as we said in the previous video,is an nx dimensional vector,given that the parameters of logistic regression will be wwhich is also an nx dimensional vector,together with b which is just a real number.So given an input X and the parameters W and b,how do we generate the output y hat?Well, one thing you could try, that doesn't work,would be to have y hat be w transpose X plus B,kind of a linear function of the input X.\n",
    "\n",
    "本视频中 我们讲讲logistic回归，这是一个学习算法 用在监督学习问题中，输出y标签是0或1时，这是一个二元分类问题，已知的输入特征向量x 可能是，一张图 你希望把识别出，这是不是猫图，你需要一个算法 可以给出一个预测值，我们说预测值y帽，就是你对y的预测，更正式的说 你希望y帽是一个概率，当输入特征x满足条件时 y就是1，所以换句话说 如果x是图片，正如我们在上一个视频中看到的，你希望y帽能告诉你，这是一张猫图的概率，所以x 正如我们之前的视频里说过的，是一个n_x维向量，已知Logistic回归的参数是w，也是一个n_x维向量，而b就是一个实数，所以已知输入x和参数w和b，我们如何计算输出预测y帽?好 你可以这么试 但其实不靠谱，就是y帽 = w^T x +b，输入x的线性函数\n",
    "\n",
    "\n",
    "\n",
    "And in fact, this is what you use if you were doing linear regression.But this isn't a very good algorithm for binary classification,because you want y hat to be the chance that Y is equal to one.So y hat should really be between zero and one,and it's difficult to enforce that because w^Tx+b can be much bigger then one or it can even be negative,which doesn't make sense for probability,that you want it to be between zero and one.So in logistic regression our output is instead going to be y hat  equals the sigmoid function applied to this quantity.This is what the sigmoid function looks like.If on the horizontal axis I plot Z then the function sigmoid of Z looks like this.So it goes smoothly from zero up to one.Let me label my axes here,this is zero and it crosses the vertical axis as 0.5.So this is what sigmoid of Z looks likeand we're going to use Z to denote this quantity,W transpose X plus B.\n",
    "\n",
    "事实上 如果你做线性回归 就是这么算的，但这不是一个非常好的二元分类算法，因为你希望y帽是y=1的概率，所以y帽应该介于0和1之间，但实际上这很难实现 因为w^Tx+b，可能比1大得多 或者甚至是负值 ，这样的概率是没意义的，你希望概率介于0和1之间，所以在Logisitc回归中 我们的输出变成，y帽等于sigmoid函数作用到这个量上，这就是sigmoid函数的图形，横轴是z，那么sigmoid(z)就是这样的，从0到1的光滑函数，我标记一下这里的轴，这是0 然后和垂直轴相交在0.5处，这就是sigmoid(z)的图形，我们用z来表示这个量，w转置x+b\n",
    "\n",
    "\n",
    "\n",
    "Here's the formula for the sigmoid function.Sigmoid of Z, where Z is a real number,is one over one plus E to the negative Z.So notice a couple of things.If Z is very large then E to the negative Z will be close to zero.So then sigmoid of Z will be approximately one over one plus something very close to zero,because E to the negative of very large number will be close to zero.So this is close to 1.And indeed, if you look in the plot on the left,if Z is very large the sigmoid of Z is very close to one.Conversely, if Z is very small,or it is a very large negative number,then sigmoid of Z becomes one over one plus E to the negative Z,and this becomes, it's a huge number.\n",
    "\n",
    "这是Sigmoid函数的公式，sigmoid(z)其中z是实数，就是1/(1+e^-z)，要注意一些事情，如果z非常大 那么e^-z就很接近0，那么sigmoid(z)就是，大约等于1/(1+某个很接近0的量，因为e^-z 在z很大时就很接近0，所以这接近1，事实上 如果你看看左边的图，z很大时 sigmoid(z)就很接近1，相反 如果z很小，或者是非常大的负数，那么sigmoid(z)就变成1/(1+e^-z)，就变成很大的数字\n",
    "\n",
    "\n",
    "So this becomes, think of it as one over one plus a number that is very, very big, and so,that's close to zero.And indeed, you see that as Z becomes a very large negative number,sigmoid of Z goes very close to zero.So when you implement logistic regression,\n",
    "\n",
    "这就变成.. 想一下，1除以1加上很大的数字 ，那是非常大的数字，所以这接近0，确实 当你看到z变成非常大的负值时，sigmoid(z)就很接近0，所以当你实现logistic回归时\n",
    "\n",
    "\n",
    "\n",
    "，你要做的是学习参数w和b\n",
    "your job is to try to learn parameters W and B\n",
    "\n",
    "，所以y帽变成了比较好的估计\n",
    "so that y hat becomes a good estimate\n",
    "\n",
    "，对y=1概率的比较好的估计\n",
    "of the chance of Y being equal to one.\n",
    "\n",
    "，在继续之前 我们再讲讲符号约定\n",
    "Before moving on, just another note on the notation.\n",
    "\n",
    "，当我们对神经网络编程时\n",
    "When we programmed neural networks,\n",
    "\n",
    "，我们通常会把w和参数b分开\n",
    "we'll usually keep the parameter W and parameter B separate,\n",
    "\n",
    "，这里b对应一个拦截器\n",
    "where here, B corresponds to an interceptor\n",
    "\n",
    "，在其他一些课程中\n",
    "In some other courses,\n",
    "\n",
    "，你们可能看过不同的表示\n",
    "you might have seen a notation that handles this differently.\n",
    "\n",
    "，在一些符号约定中\n",
    "In some conventions you define\n",
    "\n",
    "，你定义一个额外的特征向量 叫x_0  那等于1\n",
    "an extra feature called X0 and that equals a one.\n",
    "\n",
    "，所以出现x就是R^(n_x+1)维向量\n",
    "So that now x is in R^(n_x+1).\n",
    "\n",
    "，然后你将y帽定义为σ(θ^Tx)\n",
    "And then you define y hat to be equal to sigma of theta transpose X.\n",
    "\n",
    "，在这另一种符号约定中\n",
    "In this alternative notational convention,\n",
    "\n",
    "，你有一个向量参数θ\n",
    "you have vector parameters theta,\n",
    "\n",
    "，θ_0 θ_1还有θ_2\n",
    "theta zero, theta one, theta two,\n",
    "\n",
    "，一直到θ_nx\n",
    "down to theta NX.\n",
    "\n",
    "，所以θ_0 扮演的是b的角色\n",
    "And so, theta zero, plays the role of b\n",
    "\n",
    "，这是一个实数\n",
    "that's just a real number,\n",
    "\n",
    "，而θ_1直到θ_nx的作用和w一样\n",
    "and theta one down to theta NX play the role of W.\n",
    "\n",
    "，事实上 当你实现你的神经网络时\n",
    "It turns out, when you implement you implement your neural network,\n",
    "\n",
    "，将b和w看作独立的参数可能更好\n",
    "it will be easier to just keep B and W as separate parameters.\n",
    "\n",
    "，所以对于这门课\n",
    "And so, in this class,\n",
    "\n",
    "，我们不会用那种符号约定\n",
    "we will not use any of this notational convention\n",
    "\n",
    "，就是红色写的那些 我不会用\n",
    "that I just wrote in red.\n",
    "\n",
    "，如果你们没有在其他课程里见过这个符号约定\n",
    "If you've not seen this notation before in other courses,\n",
    "\n",
    "，不要担心太多\n",
    "don't worry about it.\n",
    "\n",
    "，我讲这个是为了服务\n",
    "It's just that for those of you\n",
    "\n",
    "，那些见过这种符号约定的学生\n",
    "that have seen this notation I wanted to mention explicitly\n",
    "\n",
    "，我们在本课中不会使用那种符号约定\n",
    "that we're not using that notation in this course.\n",
    "\n",
    "，如果你以前没见过\n",
    "But if you've not seen this before,\n",
    "\n",
    "，这不重要 所以不用担心\n",
    "it's not important and you don't need to worry about it.\n",
    "\n",
    "，现在你看到了logistic回归模型长什么样\n",
    "So you have now seen what the logistic regression model looks like.\n",
    "\n",
    "，接下来我们看参数w和b\n",
    "Next to change the parameters W and B\n",
    "\n",
    "，你需要定义一个成本函数\n",
    "you need to define a cost function.\n",
    "\n",
    "，我们下一个视频来讨论\n",
    "Let's do that in the next video.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    " \n",
    "**PS: 欢迎扫码关注公众号：「SelfImprovementLab」！专注「深度学习」，「机器学习」，「人工智能」。以及 「早起」，「阅读」，「运动」，「英语 」「其他」不定期建群 打卡互助活动。**\n",
    "\n",
    "<img src=\"http://upload-images.jianshu.io/upload_images/1157146-cab5ba89dfeeec4b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
