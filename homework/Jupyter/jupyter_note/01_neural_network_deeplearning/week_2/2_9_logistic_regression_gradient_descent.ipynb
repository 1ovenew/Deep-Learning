{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursera | Andrew Ng (01-week-2-2.9)—Logistic Regression Gradient Descent\n",
    "\n",
    "\n",
    "该系列仅在原课程基础上部分知识点添加个人学习笔记，或相关推导补充等。如有错误，还请批评指教。在学习了 Andrew Ng 课程的基础上，为了更方便的查阅复习，将其整理成文字。因本人一直在学习英语，所以该系列以英文为主，同时也建议读者以英文为主，中文辅助，以便后期进阶时，为学习相关领域的学术论文做铺垫。- ZJ\n",
    "    \n",
    ">[Coursera 课程](https://www.coursera.org/specializations/deep-learning) |[deeplearning.ai](https://www.deeplearning.ai/) |[网易云课堂](https://mooc.study.163.com/smartSpec/detail/1001319001.htm)\n",
    "\n",
    "---\n",
    "   **转载请注明作者和出处：ZJ 微信公众号-「SelfImprovementLab」**\n",
    "   \n",
    "   [知乎](https://zhuanlan.zhihu.com/c_147249273)：https://zhuanlan.zhihu.com/c_147249273\n",
    "   \n",
    "   [CSDN]()：\n",
    "   \n",
    "\n",
    "---\n",
    "\n",
    "(字幕来源：网易云课堂)\n",
    "\n",
    "welcome back, in this video we'll talk about how to compute derivatives for you,to implement gradient descent for logistic regression.the key take aways will be what you need to implement that are the key equations you need,in order to implement gradient descent for logistic regression.but in this video I want to do this computation,using the computation graph.I have to admit using the computation graph,is a little bit of an overkill,for deriving gradient descent for logistic regression.but I want to start explaining things this way,to get you familiar with these ideas.so that hopefully you'll make a bit more sense,when we talk about full fledged neural networks.\n",
    "\n",
    "欢迎回来，在本节我们将**讨论怎样计算偏导数**，来实现 logistic 回归的梯度下降法，它的核心关键点是 **其中有几个重要的公式，用来实现 logistic 回归的梯度下降法**，但是在本节视频中 我将使用**导数流程图**，来计算梯度，必须承认，用导数流程图来计算 logistic 回归的梯度下降 有点大材小用了，但是我认为以这种方式来讲解，可以更好地理解梯度下降，从而在讨论神经网络时，可以更深刻而全面地理解神经网络。\n",
    "\n",
    "\n",
    "so let's dive into gradient descent for logistic regression to recap we had set up logistic regression as follows,your predictions y hat is defined as follows,where z is that and if we focus on just one example for now,then the loss or respect to that one,example is defined as follows,where a is the output of the logistic regression,and y is the ground truth label,so let's write this out as a computation graph,and for this example let's say we have only two features x1 and x2.so in order to compute z,we'll need to input w1 w2 and b,in addition to the feature values x1 x2\n",
    "\n",
    "接下来开始学习 logistic回归的梯度下降法，回想一下logistic回归的公式，y帽定义如下 y帽的定义是这样的，z则是这样 现在只考虑单个样本的情况，关于该样本的损失函数，定义如下，其中a是logistic回归的输出，y是样本的基本真值标签值，现在写出该样本的偏导数流程图，假设样本只有两个 特征x1和x2，为了计算z，我们需要输入参数w1、w2和b，还有样本特征值x1 x2\n",
    "\n",
    "so these things in a computation graph get used to compute,z which is w1 x1 plus w2 x2 plus b.draw a rectangle box around that and then we compute y hat,or a equals Sigma of Z that's the next step in a computation graph,and then finally we compute L(a,y),and I won't copy the formula again,so in logistic regression what we want to do,is to modify the parameters w and b,in order to reduce this loss,we've described before propagation steps,of how you actually compute the loss on a single training example\n",
    "\n",
    "因此用来计算z的 偏导数计算公式，z等于w1*x1+w2*x2+b，如图用矩形括起来了  然后计算y帽，即a 等于sigma(z) 也就是偏导数流程图的下一步，最后计算L(a,y)，我不会再写出这个公式了，因此在logistic回归中 我们需要做的是，变换参数w和b的值，来最小化损失函数，在前面 我们已经前向传播步骤，在单个训练样本上 计算损失函数\n",
    "\n",
    "now let's talk about how you can go backwards to,talk to compute the derivatives,here's the cleaned up version of the diagram,because what we want to do is compute derivatives respect to this loss,the first thing we want to do we're going backwards,is to compute the derivative of this loss with,respect to the script over there,with respect to this variable a,and so in the code,you know you just use da right to denote this variable,and it turns out that if you are familiar with calculus,you can show that this ends up being negative y over a,plus one minus y over one minus a,and the way you do that is you take the formula,for the loss and if you are familiar with calculus,you can compute the derivative with respect to the variable,lowercase a and you get this formula\n",
    "\n",
    "现在让我们来 讨论怎样向后，计算偏导数，这是整洁版本的图，要想计算损失函数L的导数，首先我们要，向前一步 先计算损失函数的导数，这里有个标记，关于变量a的导数，在代码中，你只需要使用da来表示这个变量，事实上 如果你熟悉微积分，这个结果是 -y/a，加上 (1-y)/(1-a)，损失函数导数的计算公式，就是这样 如果你熟悉微积分，你计算的 关于变量a的导数，就是这个式子\n",
    "\n",
    "\n",
    "but if you're not familiar of calculus,don't worry about it,we'll provide the derivative formulas you need,through out this course so if you are a expert in calculus,you'll encourage you to look up,the formula for the loss from their previous slide,and try to get director for respect to a using you know calculus,but if you don't know enough calculus to do that,don't worry about it,now having computed this quantity or da,the derivative of your final output variable respect to a,you can then go backwards and it turns out that you can show dz,which this is the Python code variable name,this is going to be you know the derivative of the loss,with respect to z or for L you can really write,the loss including a and y explicitly as parameters,or not right give either type of notation,is equally acceptable they can show that,this is equal to a minus y,just a couple comments only for those of you did a experts in calculus\n",
    "\n",
    "如果你不熟悉微积分，也不必太担心，我们会列出本课程涉及的所有求导公式，因此 如果你非常熟悉微积分，我们鼓励你，通过上一张幻灯片中的损失函数公式，使用微积分 直接求出变量a的导数，如果你不太了解微积分，也不用太担心，现在计算出da，最终结果关于变量a的导数，现在可以再向后一步 计算dz，dz是代码中的变量名，dz是损失函数关于z的导数，对于dL，可以写成dL(a,y)，两种形式都可以，都是等价的，结果等于 a-y，给熟悉微积分的人解释一下\n",
    "\n",
    "\n",
    "\n",
    "if you're not explain calculus,don't worry about it,but it turns out that this right dL dz,this can be expressed as dL da times,da dz and it turns out that da dz,this turns out to be a times 1 minus a,and dL da we are previously worked out over here,and so if you take these two quantities dL da,which is this term together with da dz,which is this term and just take these two things,and multiply them you can show that you,the equation simplifies the a minus y,so that's how you derive it,and this is really the chain rule that\n",
    ",and this is really the chain rule that.\n",
    "\n",
    "如果你对微积分不熟悉，也不用担心，实际上 dL/dz，等于(dL/da)乘上，da/dz 而da/dz，等于a*(1-a)，而dL/da 在前面已经计算过了，因此将这两项dL/da，即这部分 和da/dz，这部分进行相乘，最终的公式，化简成 (a-y)，这个推导的过程，就是我之前提到过的“链式法则”\n",
    "\n",
    "\n",
    "\n",
    "I briefly clue to you before ok,so feel free to go through that calculation yourself,if you are knowledgeable calculus,but if you aren't all you need to know is that,you can compute dz as a minus y,and it already done the calculus for you,and then the final step in back propagation,is to go back to compute,how much you need to change w and b,so in particular you can show that the derivative respect to w1,and will call this DW 1 that,this is equal to x1 times dz,and then similarly dw 2,which is how much you want to change w2 is x2 times dz,and b excuse me db is equal to dz\n",
    "\n",
    "如果你对微积分熟悉，放心地去计算，整个求导过程，如果不熟悉微积分 你只需要知道，dz 等于 (a-y)，已经计算好了，现在向后传播的最后一步，特别地 关于w1的导数 dL/dw1，写成 dw1，等于x1*dz，同样地 dw2，w2的变化量 dw2等于 x2*dz，b 不好意思 应该是db 等于dz\n",
    "\n",
    "\n",
    "\n",
    "so if you want to do gradient descents,with respect to just this one example,what you will do is the following,you would use this formula to compute dz,and then use these formulas to compute dw1 dw2,and db and then you perform these,updates w1 gets updated w1 - learning rate alpha,rate alpha times dw1 w2 gets updated,similarly and B gets set as b - the learning rate times db,and so this will be one step of gradicent descent,with respect to a single example,so you've seen how to compute derivatives,and implement gradient descent for logistic regression,with respect to a single training example\n",
    "\n",
    "因此关于单个样本的，梯度下降法，你所需要做的就是这些事情，使用这个公式计算dz，使用这个计算dw1、dw2，还有db 然后，更新w1为 w1减去学习率乘以dw1，类似地 更新w2，更新b 为b减去学习率乘以db，这就是单个样本实例的，一次梯度更新步骤，现在你已经知道了，怎样计算导数 并且实现了单个训练样本的，logistic回归的梯度下降法\n",
    "\n",
    "\n",
    "\n",
    "but to train logistic regression model you have,ot just one training example,given entire training set of m training examples,so in the next video.let's see how you can take these ideas and apply them,to learning not just from one example,but from an entire training set\n",
    "\n",
    "但是训练logistic回归模型，不仅仅只有一个训练样本，而是有m个训练样本的整个训练集，因此在下一节视频中，这些想法如何应用到，整个训练样本集中，而不仅仅只是单个样本上\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "2.10\n",
    "\n",
    "\n",
    "in the previous video you saw how to compute derivatives,and implement gradient descent,with respect to just one training example for logistic regression,now we want to do it for m training examples to get started,let's remind ourselves that the definition of,the cost function J cost function (w,b),which you care about is this average right,1 over m sum from i equals 1 through m,you know the loss when your algorithm,output a^i on the example y well you know,a^i is the prediction on the I've trained example,which is sigmoid of z^i,which is equal to sigmoid of W transpose x^i plus b\n",
    "\n",
    "在之前的视频中你 已经看到如何计算导数，和把梯度下降法，应用到logistic回归的一个训练样本上，现在我们想要把它 应用在m个训练样本上，首先 时刻记住有关于，成本函数J(w,b)的定义，它是这样一个平均值，1/m 从i=i到m求和，这个损失函数L 当你的算法，在样本(x,y)输出了a^i，你知道 a^i是训练样本的预测值，也就是sigmoid(z^i)，等于sigmoid作用于w的转置 乘上x^i 加上b\n",
    "\n",
    "\n",
    "ok so what we show in the previous slide is for,any single training example how to compute the derivatives,when you have just one training example,great so dw1 dw2 and db with now the superscript i,to denote the corresponding values you get,if you are doing what we did on the previous slide,but just using the one training example (x^i,y^i),excuse me I missing i there as well so,now you know the overall cost functions with the sum,was really the average of the 1 over m term of the individual losses,so it turns out that the derivative respect to say w1 of the overall cost function,is also going to be the average of derivatives,respect to w1 of the individual loss terms\n",
    "\n",
    "我们在前面的幻灯中展示的是，对于任意单个训练样本 如何计算导数，当你只有一个训练样本，dw1 dw2和db添上上标i，表示相应的值，如果你做的是和上一张幻灯中演示的那种情况，但只使用了一个 训练样本(x^i,y^i)，抱歉 我这里少了个i 现在你知道，全局成本函数 是一个求和，实际上是1到m项 损失函数和的平均，它表明 全局成本函数 对w1的导数，也同样是各项损失函数，对w1导数的平均\n",
    "\n",
    "\n",
    "but previously we have already shown how to compute this term,as say (dw1)^i right which we you know on the previous slide,show how the computes on a single training example,so what you need to do is really compute these own derivatives,as we showed on the previous training example,and average them and this will give you the overall gradient,that you can use to implement straight into gradient decent,so I know there was a lot of details,but let's take all of this up,and wrap this up into a concrete algorithms,and what you should implement together,just logistic regression with gradient descent working\n",
    "\n",
    "但之前我们 已经演示了如何计算这项，也就是我所写的 即之前幻灯中演示的，如何对单个训练样本进行计算，所以你真正需要做的是计算这些导数，如我们在之前的训练样本上做的，并且求平均 这会得到全局梯度值，你能够把它直接应用到梯度下降算法中，所以 这里有很多细节，但让我们把这些，装进一个具体的算法，同时你需要一起应用的，就是logistic回归和和梯度下降法\n",
    "\n",
    "\n",
    "so just what you can do,let's initialize J equals 0,um... dw1 equals 0 dw2 equals 0 db equals 0,and what we're going to do,is use a for loop over the training set, and compute the derivatives to respect each training example,and then add them up all right,so see as we do it for i equals 1 through m,so m is the number of training examples,we compute z^i equals w transpose x^i plus b um...,the prediction a^i is equal to sigmoid of z^i,and then you know let's let's add up J,J plus equals (y^i)log(a^i) um...,plus 1 minus y I log 1 minus a^i,and then put a negative sign in front of the whole thing\n",
    "\n",
    "因此你能做的，让我们初始化J=0，dw1等于0 dw2等于0 db等于0，并且我们将要做的是使用一个，for循环遍历训练集 同时，计算相应的每个训练样本的导数，然后把它们加起来，好了 如我们所做的让i 等于1到m，m正好是训练样本个数，我们计算z^i等于 w的转置乘上x^i 加上b，a^i的预测值等于σ(z^i)，然后你知道 让我们加上J，J加等于(y^i)log(a^i)，加上(1-y^i)log(1-a^i)，然后加上一个负号在整个公式的前面\n",
    "\n",
    "\n",
    "\n",
    "and then as we saw earlier we have dz^i,or it is equal to a^i minus y^i and dw,gets plus equals (x_1)^i dz^i dw2 plus,equals (x_2)^i dz^i or and i'm doing this calculation,assuming that you have just the two features,so the n is equal to 2,otherwise you do this for dw1 dw2 dw_3 and so on,and db plus equals dz^i,and I guess that's the end of the for loop,and then finally having done this for all m training examples,you will still need to divide by m,because we're computing averages,so dw1 divide equals m dw2 divide equals m,db divide equals m in order to compute averages,and so with all of these calculations you've just computed,the derivative of the cost function J,with respect to each parameters w1 w2 and b\n",
    "\n",
    "我们之前见过 我们有dz^i，或者它等于a^i减去y^i dw，加等于(x_1)^i乘上dz^i dw2加，等于(x_2)^i乘上dz^i 或者我正在做这个计算，假设你只有两个特征，所以n等于2，否则你做这个从dw1 dw2 dw_3 一直下去，同时db加等于dz^i，我想 这是for循环的结束，最终对所有的m个训练样本都进行了这个计算，你还需要除以m，因为我们计算平均值，因此dw1/=m dw2/=m，db/=m 可以计算平均值，随着所有这些计算 你已经计算了，损失函数J，对各个参数w1 w2和b的导数\n",
    "\n",
    "\n",
    "just come to details what we're doing,we're using dw1 and dw2 and db to as accumulators right,so that after this computation you know,dw1 is equal to the derivative of your overall cost function,with respect to w1,and similarly for dw2 and db so notice that,dw1 and dw2 do not have a superscript i,because we're using them in this code as accumulators,to sum over the entire training set whereas in contrast dz^i,here this was um... dz with respect to just one single training example,that is why that has a superscript i to refer to,the one training example i that's computed on,and so having finished all these calculations,to implement one step of gradient descent you implement,w1 gets updated as w1 minus a learning rate times dw1,w2 gives updates as w2 minus learning rate times dw2,and b gets update as b minus learning rate times db,where dw1 dw2 and db where you know as computed,and finally J here would also,be a correct value for your cost function\n",
    "\n",
    "回顾我们正在做的细节，我们使用dw1 dw2和db作为累加器，所以在这些计算之后，你知道dw1等于 你的全局成本函数，对w1的导数，对dw2和db也是一样 同时注意，dw1和dw2没有上标i，因为我们在这代码中 把它们作为累加器，去求取整个训练集上的和 相反地 dz^(i)，这是对应于 单个训练样本的dz，上标i指的是，对应第i个训练样本的计算，完成所有这些计算后，应用一步梯度下降，使得w1获得更新 即w1减去学习率乘上dw1，w2更新 即w2减去 学习率乘上dw2，同时更新b b减去学习率乘上db，这里dw1 dw2和db 如你知道的被计算，最终这里的J也会，是你成本函数的正确值\n",
    "\n",
    "\n",
    "\n",
    "so everything on the slide implements,just one single step of gradient descent,and so you have to repeat everything on this slide multiple times,in order to take multiple steps of gradient descent,in case these details seem too complicated,again don't worry too much about it,for now hopefully all this will be clearer,when you go and implement this in the programming assignment,but it turns out there are two weaknesses with the calculation,as with as with implements here,which is that to implement logistic regression this way,you need to write two for loops the first for loop,is a small loop over the m training examples,and the second for loop is,a for loop over all the features over here,right so in this example we just had two features,so n is equal to 2 and n_x equals 2\n",
    "\n",
    "所以幻灯片上的所有东西，只应用了一次  梯度下降法，因此你需要 重复以上内容很多次，以应用多次梯度下降，看起来这些细节 似乎很复杂，但目前不要担心太多，会清晰起来的，只要你在编程作业里 实现这些之后，但它表明计算中有两个缺点，当应用在这里的时候，就是说应用此方法到在logistic回归，你需要编写两个for循环 第一个for循环是，遍历m个训练样本的小循环，第二个for循环，是遍历所有特征的for循环，这个例子中 我们只有2个特征，所以n等于2 nx等于2\n",
    "\n",
    "---\n",
    "\n",
    "but if you have more features,you end up writing your dw1 dw2,and you have similar computations for dw_3,and so on down to dw_n,so seems like you need to,have a for loop over the features over all n features,when you're implementing deep learning algorithms,you'll find that having explicit for loops in your code,makes your algorithm run less efficiency,and so in the deep learning area,would move to a bigger and bigger data sets,and so being able to implement your algorithms,without using explicit for loops,is really important and will help you,to scale to much bigger data sets,so it turns out that there are set of techniques called vectorization,techniques that allows you to get rid of these explicit for loops in your code\n",
    "\n",
    "但如果你有更多特征，你开始编写你的dw1 dw2，类似地计算dw3，一直到dwn，看来你需要，一个for循环 遍历所有n个特征，当你应用深度学习算法，你会发现 在代码中显式地使用for循环，会使算法很低效，同时在深度学习领域，会有越来越大的数据集，所以能够应用你的算法，完全不用显式for循环的话，会是重要的 会帮助你，处理更大的数据集，有一门向量化技术，帮助你的代码 摆脱这些显式的for循环\n",
    "\n",
    "\n",
    "\n",
    "I think in the pre deep learning era,that's before the rise of deep learning,vectorization was a nice to have,you could sometimes do it to speed a vehicle,and sometimes not but in the deep learning era vectorization,that is getting rid of for loops like this,and like this has become really important,because we're more and more training on very large datasets,and so you really need your code to be very efficient\n",
    "\n",
    "我想 在深度学习时代的早期，也就是深度学习兴起之前，向量化是很棒的，有时候用来加速运算，但有时候也未必能够 但是在深度学习时代，用向量化来摆脱for循环，已经变得相当重要，因为我们越来越多地 训练非常大的数据集，你的代码变需要得非常高效\n",
    "\n",
    "\n",
    "so in the next few videos we'll talk aboutvectorization,and how to implement all this without using even a single for loop,so of this I hope you have a sense,of how to implement logistic regression,or gradient descent for logistic regression,um... things will be clearer when you implement the program exercise,but before actually doing the program exercise,let's first talk about vectorization,so then you can implement this whole thing,implement a single iteration of gradient descent,without using any for loop\n",
    "\n",
    "接下来的几个视频 我们会谈到向量化，以及如何应用这些 做到一个for循环都不需要使用，我希望你理解了，如何应用logistic回归，对logistic回归应用梯度下降法，在你进行编程练习后 应该会掌握得更清楚，但在做编程练习之前，让我们先谈谈“向量化”，然后你可以做到全部的这些，实现一个梯度下降法的迭代，而不需要for循环\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    " \n",
    "**PS: 欢迎扫码关注公众号：「SelfImprovementLab」！专注「深度学习」，「机器学习」，「人工智能」。以及 「早起」，「阅读」，「运动」，「英语 」「其他」不定期建群 打卡互助活动。**\n",
    "\n",
    "<center><img src=\"http://upload-images.jianshu.io/upload_images/1157146-cab5ba89dfeeec4b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\"></center>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
