{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursera | Andrew Ng (01-week-2-2.9)—Logistic Regression Gradient Descent\n",
    "\n",
    "该系列仅在原课程基础上部分知识点添加个人学习笔记，或相关推导补充等。如有错误，还请批评指教。在学习了 Andrew Ng 课程的基础上，为了更方便的查阅复习，将其整理成文字。因本人一直在学习英语，所以该系列以英文为主，同时也建议读者以英文为主，中文辅助，以便后期进阶时，为学习相关领域的学术论文做铺垫。- ZJ\n",
    "    \n",
    ">[Coursera 课程](https://www.coursera.org/specializations/deep-learning) |[deeplearning.ai](https://www.deeplearning.ai/) |[网易云课堂](https://mooc.study.163.com/smartSpec/detail/1001319001.htm)\n",
    "\n",
    "---\n",
    "   **转载请注明作者和出处：ZJ 微信公众号-「SelfImprovementLab」**\n",
    "   \n",
    "   [知乎](https://zhuanlan.zhihu.com/c_147249273)：https://zhuanlan.zhihu.com/c_147249273\n",
    "   \n",
    "   [CSDN]()：\n",
    "   \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "欢迎回来 在本节我们将讨论(字幕来源：网易云课堂)\n",
    "welcome back, in this video we'll talk about\n",
    "\n",
    "怎样计算偏导数\n",
    "how to compute derivatives for you\n",
    "\n",
    "来实现logistic回归的梯度下降法\n",
    "to implement gradient descent for logistic regression.\n",
    "\n",
    "它的核心关键点是 其中有几个重要的公式\n",
    "the key take aways will be what you need to implement that\n",
    "\n",
    "用来实现\n",
    "are the key equations you need\n",
    "\n",
    "logistic回归的梯度下降法\n",
    "in order to implement gradient descent for logistic regression.\n",
    "\n",
    "\n",
    "\n",
    "但是在本节视频中 我将使用导数流程图\n",
    "but in this video I want to do this computation\n",
    "\n",
    "来计算梯度\n",
    "using the computation graph.\n",
    "\n",
    "必须承认\n",
    "I have to admit using the computation graph\n",
    "\n",
    "用导数流程图来计算\n",
    "is a little bit of an overkill\n",
    "\n",
    "logistic回归的梯度下降 有点大材小用了\n",
    "for deriving gradient descent for logistic regression.\n",
    "\n",
    "但是我认为 以这种方式来讲解\n",
    "but I want to start explaining things this way\n",
    "\n",
    "可以更好地理解梯度下降\n",
    "to get you familiar with these ideas.\n",
    "\n",
    "从而在讨论神经网络时\n",
    "so that hopefully you'll make a bit more sense\n",
    "\n",
    "可以更深刻而全面地 理解神经网络\n",
    "when we talk about full fledged neural networks\n",
    "\n",
    "接下来开始学习 logistic回归的梯度下降法\n",
    "so let's dive into gradient descent for logistic regression.\n",
    "\n",
    "回想一下logistic回归的公式\n",
    "to recap we had set up logistic regression as follows\n",
    "\n",
    "y帽定义如下 y帽的定义是这样的\n",
    "your predictions y hat is defined as follows\n",
    "\n",
    "z则是这样 现在只考虑单个样本的情况\n",
    "where z is that and if we focus on just one example for now\n",
    "\n",
    "关于该样本的损失函数\n",
    "then the loss or respect to that one\n",
    "\n",
    "定义如下\n",
    "example is defined as follows\n",
    "\n",
    "其中a是logistic回归的输出\n",
    "where a is the output of the logistic regression\n",
    "\n",
    "y是样本的基本真值标签值\n",
    "and y is the ground truth label\n",
    "\n",
    "现在写出该样本的偏导数流程图\n",
    "so let's write this out as a computation graph\n",
    "\n",
    "假设样本只有两个 特征x1和x2\n",
    "and for this example let's say we have only two features x1 and x2.\n",
    "\n",
    "为了计算z\n",
    "so in order to compute z,\n",
    "\n",
    "我们需要输入参数w1、w2和b\n",
    "we'll need to input w1 w2 and b\n",
    "\n",
    "还有样本特征值x1 x2\n",
    "in addition to the feature values x1 x2\n",
    "\n",
    "因此用来计算z的 偏导数计算公式\n",
    "so these things in a computation graph get used to compute\n",
    "\n",
    "z等于w1*x1+w2*x2+b\n",
    "z which is w1 x1 plus w2 x2 plus b.\n",
    "\n",
    "如图用矩形括起来了  然后计算y帽\n",
    "draw a rectangle box around that and then we compute y hat\n",
    "\n",
    "即a 等于sigma(z) 也就是偏导数流程图的下一步\n",
    "or a equals Sigma of Z that's the next step in a computation graph\n",
    "\n",
    "最后计算L(a,y)\n",
    "and then finally we compute L(a,y)\n",
    "\n",
    "我不会再写出这个公式了\n",
    "and I won't copy the formula again\n",
    "\n",
    "因此在logistic回归中 我们需要做的是\n",
    "so in logistic regression what we want to do\n",
    "\n",
    "变换参数w和b的值\n",
    "is to modify the parameters w and b\n",
    "\n",
    "来最小化损失函数\n",
    "in order to reduce this loss\n",
    "\n",
    "在前面 我们已经前向传播步骤\n",
    "we've described before propagation steps\n",
    "\n",
    "在单个训练样本上 计算损失函数\n",
    "of how you actually compute the loss on a single training example\n",
    "\n",
    "现在让我们来 讨论怎样向后\n",
    "now let's talk about how you can go backwards to\n",
    "\n",
    "计算偏导数\n",
    "talk to compute the derivatives\n",
    "\n",
    "这是整洁版本的图\n",
    "here's the cleaned up version of the diagram\n",
    "\n",
    "要想计算损失函数L的导数\n",
    "because what we want to do is compute derivatives respect to this loss\n",
    "\n",
    "首先我们要\n",
    "the first thing we want to do we're going backwards\n",
    "\n",
    "向前一步 先计算损失函数的导数\n",
    "is to compute the derivative of this loss with\n",
    "\n",
    "这里有个标记\n",
    "respect to the script over there\n",
    "\n",
    "关于变量a的导数\n",
    "with respect to this variable a\n",
    "\n",
    "在代码中\n",
    "and so in the code\n",
    "\n",
    "你只需要使用da来表示这个变量\n",
    "you know you just use da right to denote this variable\n",
    "\n",
    "事实上 如果你熟悉微积分\n",
    "and it turns out that if you are familiar with calculus\n",
    "\n",
    "这个结果是 -y/a\n",
    "you can show that this ends up being negative y over a\n",
    "\n",
    "加上 (1-y)/(1-a)\n",
    "plus one minus y over one minus a\n",
    "\n",
    "损失函数导数的计算公式\n",
    "and the way you do that is you take the formula\n",
    "\n",
    "就是这样 如果你熟悉微积分\n",
    "for the loss and if you are familiar with calculus\n",
    "\n",
    "你计算的 关于变量a的导数\n",
    "you can compute the derivative with respect to the variable\n",
    "\n",
    "就是这个式子\n",
    "lowercase a and you get this formula\n",
    "\n",
    "如果你不熟悉微积分\n",
    "but if you're not familiar of calculus\n",
    "\n",
    "也不必太担心\n",
    "don't worry about it\n",
    "\n",
    "我们会列出本课程涉及的所有求导公式\n",
    "we'll provide the derivative formulas you need\n",
    "\n",
    "因此 如果你非常熟悉微积分\n",
    "through out this course so if you are a expert in calculus\n",
    "\n",
    "我们鼓励你\n",
    "you'll encourage you to look up\n",
    "\n",
    "通过上一张幻灯片中的损失函数公式\n",
    "the formula for the loss from their previous slide\n",
    "\n",
    "使用微积分 直接求出变量a的导数\n",
    "and try to get director for respect to a using you know calculus\n",
    "\n",
    "如果你不太了解微积分\n",
    "but if you don't know enough calculus to do that\n",
    "\n",
    "也不用太担心\n",
    "don't worry about it\n",
    "\n",
    "现在计算出da\n",
    "now having computed this quantity or da\n",
    "\n",
    "最终结果关于变量a的导数\n",
    "the derivative of your final output variable respect to a\n",
    "\n",
    "现在可以再向后一步 计算dz\n",
    "you can then go backwards and it turns out that you can show dz\n",
    "\n",
    "dz是代码中的变量名\n",
    "which this is the Python code variable name\n",
    "\n",
    "dz是损失函数关于z的导数\n",
    "this is going to be you know the derivative of the loss\n",
    "\n",
    "对于dL\n",
    "with respect to z or for L you can really write\n",
    "\n",
    "可以写成dL(a,y)\n",
    "the loss including a and y explicitly as parameters\n",
    "\n",
    "两种形式都可以\n",
    "or not right give either type of notation\n",
    "\n",
    "都是等价的\n",
    "is equally acceptable they can show that\n",
    "\n",
    "结果等于 a-y\n",
    "this is equal to a minus y\n",
    "\n",
    "给熟悉微积分的人解释一下\n",
    "just a couple comments only for those of you did a experts in calculus\n",
    "\n",
    "如果你对微积分不熟悉\n",
    "if you're not explain calculus\n",
    "\n",
    "也不用担心\n",
    "don't worry about it\n",
    "\n",
    "实际上 dL/dz\n",
    "but it turns out that this right dL dz\n",
    "\n",
    "等于(dL/da)乘上\n",
    "this can be expressed as dL da times\n",
    "\n",
    "da/dz 而da/dz\n",
    "da dz and it turns out that da dz\n",
    "\n",
    "等于a*(1-a)\n",
    "this turns out to be a times 1 minus a\n",
    "\n",
    "而dL/da 在前面已经计算过了\n",
    "and dL da we are previously worked out over here\n",
    "\n",
    "因此将这两项dL/da\n",
    "and so if you take these two quantities dL da\n",
    "\n",
    "即这部分 和da/dz\n",
    "which is this term together with da dz\n",
    "\n",
    "这部分进行相乘\n",
    "which is this term and just take these two things\n",
    "\n",
    "最终的公式\n",
    "and multiply them you can show that you\n",
    "\n",
    "化简成 (a-y)\n",
    "the equation simplifies the a minus y\n",
    "\n",
    "这个推导的过程\n",
    "so that's how you derive it\n",
    "\n",
    "就是我之前提到过的“链式法则”\n",
    "and this is really the chain rule that\n",
    "\n",
    "如果你对微积分熟悉\n",
    "I briefly clue to you before ok\n",
    "\n",
    "放心地去计算\n",
    "so feel free to go through that calculation yourself\n",
    "\n",
    "整个求导过程\n",
    "if you are knowledgeable calculus\n",
    "\n",
    "如果不熟悉微积分 你只需要知道\n",
    "but if you aren't all you need to know is that\n",
    "\n",
    "dz 等于 (a-y)\n",
    "you can compute dz as a minus y\n",
    "\n",
    "已经计算好了\n",
    "and it already done the calculus for you\n",
    "\n",
    "现在向后传播的最后一步\n",
    "and then the final step in back propagation\n",
    "\n",
    "计算看看\n",
    "is to go back to compute\n",
    "\n",
    "w和b需要如何变化\n",
    "how much you need to change w and b\n",
    "\n",
    "特别地 关于w1的导数 dL/dw1\n",
    "so in particular you can show that the derivative respect to w1\n",
    "\n",
    "写成 dw1\n",
    "and will call this DW 1 that\n",
    "\n",
    "等于x1*dz\n",
    "this is equal to x1 times dz\n",
    "\n",
    "同样地 dw2\n",
    "and then similarly dw 2\n",
    "\n",
    "w2的变化量 dw2等于 x2*dz\n",
    "which is how much you want to change w2 is x2 times dz\n",
    "\n",
    "b 不好意思 应该是db 等于dz\n",
    "and b excuse me db is equal to dz\n",
    "\n",
    "因此关于单个样本的\n",
    "so if you want to do gradient descents\n",
    "\n",
    "梯度下降法\n",
    "with respect to just this one example\n",
    "\n",
    "你所需要做的就是这些事情\n",
    "what you will do is the following\n",
    "\n",
    "使用这个公式计算dz\n",
    "you would use this formula to compute dz\n",
    "\n",
    "使用这个计算dw1、dw2\n",
    "and then use these formulas to compute dw1 dw2\n",
    "\n",
    "还有db 然后\n",
    "and db and then you perform these\n",
    "\n",
    "更新w1为 w1减去学习率乘以dw1\n",
    "updates w1 gets updated w1 - learning rate alpha\n",
    "\n",
    "类似地 更新w2\n",
    "rate alpha times dw1 w2 gets updated\n",
    "\n",
    "更新b 为b减去学习率乘以db\n",
    "similarly and B gets set as b - the learning rate times db\n",
    "\n",
    "这就是单个样本实例的\n",
    "and so this will be one step of gradicent descent\n",
    "\n",
    "一次梯度更新步骤\n",
    "with respect to a single example\n",
    "\n",
    "现在你已经知道了\n",
    "so you've seen how to compute derivatives\n",
    "\n",
    "怎样计算导数 并且实现了单个训练样本的\n",
    "and implement gradient descent for logistic regression\n",
    "\n",
    "logistic回归的梯度下降法\n",
    "with respect to a single training example\n",
    "\n",
    "但是训练logistic回归模型\n",
    "but to train logistic regression model you have\n",
    "\n",
    "不仅仅只有一个训练样本\n",
    "not just one training example\n",
    "\n",
    "而是有m个训练样本的整个训练集\n",
    "given entire training set of m training examples\n",
    "\n",
    "因此在下一节视频中\n",
    "so in the next video\n",
    "\n",
    "这些想法如何应用到\n",
    "let's see how you can take these ideas and apply them\n",
    "\n",
    "整个训练样本集中\n",
    "to learning not just from one example\n",
    "\n",
    "而不仅仅只是单个样本上\n",
    "but from an entire training set\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "2.10\n",
    "\n",
    "\n",
    "在之前的视频中你 已经看到如何计算导数(字幕来源：网易云课堂)\n",
    "in the previous video you saw how to compute derivatives\n",
    "\n",
    "和把梯度下降法\n",
    "and implement gradient descent\n",
    "\n",
    "应用到logistic回归的一个训练样本上\n",
    "with respect to just one training example for logistic regression\n",
    "\n",
    "现在我们想要把它 应用在m个训练样本上\n",
    "now we want to do it for m training examples to get started\n",
    "\n",
    "首先 时刻记住有关于\n",
    "let's remind ourselves that the definition of\n",
    "\n",
    "成本函数J(w,b)的定义\n",
    "the cost function J cost function (w,b)\n",
    "\n",
    "它是这样一个平均值\n",
    "which you care about is this average right\n",
    "\n",
    "1/m 从i=i到m求和\n",
    "1 over m sum from i equals 1 through m\n",
    "\n",
    "这个损失函数L 当你的算法\n",
    "you know the loss when your algorithm\n",
    "\n",
    "在样本(x,y)输出了a^i\n",
    "output a^i on the example y well you know\n",
    "\n",
    "你知道 a^i是训练样本的预测值\n",
    "a^i is the prediction on the I've trained example\n",
    "\n",
    "也就是sigmoid(z^i)\n",
    "which is sigmoid of z^i\n",
    "\n",
    "等于sigmoid作用于w的转置 乘上x^i 加上b\n",
    "which is equal to sigmoid of W transpose x^i plus b\n",
    "\n",
    "我们在前面的幻灯中展示的是\n",
    "ok so what we show in the previous slide is for\n",
    "\n",
    "对于任意单个训练样本 如何计算导数\n",
    "any single training example how to compute the derivatives\n",
    "\n",
    "当你只有一个训练样本\n",
    "when you have just one training example\n",
    "\n",
    "dw1 dw2和db添上上标i\n",
    "great so dw1 dw2 and db with now the superscript i\n",
    "\n",
    "表示相应的值\n",
    "to denote the corresponding values you get\n",
    "\n",
    "如果你做的是和上一张幻灯中演示的那种情况\n",
    "if you are doing what we did on the previous slide\n",
    "\n",
    "但只使用了一个 训练样本(x^i,y^i)\n",
    "but just using the one training example (x^i,y^i)\n",
    "\n",
    "抱歉 我这里少了个i 现在你知道\n",
    "excuse me I missing i there as well so\n",
    "\n",
    "全局成本函数 是一个求和\n",
    "now you know the overall cost functions with the sum\n",
    "\n",
    "实际上是1到m项 损失函数和的平均\n",
    "was really the average of the 1 over m term of the individual losses\n",
    "\n",
    "它表明 全局成本函数 对w1的导数\n",
    "so it turns out that the derivative respect to say w1 of the overall cost function\n",
    "\n",
    "也同样是各项损失函数\n",
    "is also going to be the average of derivatives\n",
    "\n",
    "对w1导数的平均\n",
    "respect to w1 of the individual loss terms\n",
    "\n",
    "但之前我们 已经演示了如何计算这项\n",
    "but previously we have already shown how to compute this term\n",
    "\n",
    "也就是我所写的 即之前幻灯中演示的\n",
    "as say (dw1)^i right which we you know on the previous slide\n",
    "\n",
    "如何对单个训练样本进行计算\n",
    "show how the computes on a single training example\n",
    "\n",
    "所以你真正需要做的是计算这些导数\n",
    "so what you need to do is really compute these own derivatives\n",
    "\n",
    "如我们在之前的训练样本上做的\n",
    "as we showed on the previous training example\n",
    "\n",
    "并且求平均 这会得到全局梯度值\n",
    "and average them and this will give you the overall gradient\n",
    "\n",
    "你能够把它直接应用到梯度下降算法中\n",
    "that you can use to implement straight into gradient decent\n",
    "\n",
    "所以 这里有很多细节\n",
    "so I know there was a lot of details\n",
    "\n",
    "但让我们把这些\n",
    "but let's take all of this up\n",
    "\n",
    "装进一个具体的算法\n",
    "and wrap this up into a concrete algorithms\n",
    "\n",
    "同时你需要一起应用的\n",
    "and what you should implement together\n",
    "\n",
    "就是logistic回归和和梯度下降法\n",
    "just logistic regression with gradient descent working\n",
    "\n",
    "因此你能做的\n",
    "so just what you can do\n",
    "\n",
    "让我们初始化J=0\n",
    "let's initialize J equals 0\n",
    "\n",
    "dw1等于0 dw2等于0 db等于0\n",
    "um... dw1 equals 0 dw2 equals 0 db equals 0\n",
    "\n",
    "并且我们将要做的是使用一个\n",
    "and what we're going to do\n",
    "\n",
    "for循环遍历训练集 同时\n",
    "is use a for loop over the training set\n",
    "\n",
    "计算相应的每个训练样本的导数\n",
    " and compute the derivatives to respect each training example\n",
    "\n",
    "然后把它们加起来\n",
    "and then add them up all right\n",
    "\n",
    "好了 如我们所做的让i 等于1到m\n",
    "so see as we do it for i equals 1 through m\n",
    "\n",
    "m正好是训练样本个数\n",
    "so m is the number of training examples\n",
    "\n",
    "我们计算z^i等于 w的转置乘上x^i 加上b\n",
    "we compute z^i equals w transpose x^i plus b um...\n",
    "\n",
    "a^i的预测值等于σ(z^i)\n",
    "the prediction a^i is equal to sigmoid of z^i\n",
    "\n",
    "然后你知道 让我们加上J\n",
    "and then you know let's let's add up J\n",
    "\n",
    "J加等于(y^i)log(a^i)\n",
    "J plus equals (y^i)log(a^i) um...\n",
    "\n",
    "加上(1-y^i)log(1-a^i)\n",
    "plus 1 minus y I log 1 minus a^i\n",
    "\n",
    "然后加上一个负号在整个公式的前面\n",
    "and then put a negative sign in front of the whole thing\n",
    "\n",
    "我们之前见过 我们有dz^i\n",
    "and then as we saw earlier we have dz^i\n",
    "\n",
    "或者它等于a^i减去y^i dw\n",
    "or it is equal to a^i minus y^i and dw\n",
    "\n",
    "加等于(x_1)^i乘上dz^i dw2加\n",
    "gets plus equals (x_1)^i dz^i dw2 plus\n",
    "\n",
    "等于(x_2)^i乘上dz^i 或者我正在做这个计算\n",
    "equals (x_2)^i dz^i or and i'm doing this calculation\n",
    "\n",
    "假设你只有两个特征\n",
    "assuming that you have just the two features\n",
    "\n",
    "所以n等于2\n",
    "so the n is equal to 2\n",
    "\n",
    "否则你做这个从dw1 dw2 dw_3 一直下去\n",
    "otherwise you do this for dw1 dw2 dw_3 and so on\n",
    "\n",
    "同时db加等于dz^i\n",
    "and db plus equals dz^i\n",
    "\n",
    "我想 这是for循环的结束\n",
    "and I guess that's the end of the for loop\n",
    "\n",
    "最终对所有的m个训练样本都进行了这个计算\n",
    "and then finally having done this for all m training examples\n",
    "\n",
    "你还需要除以m\n",
    "you will still need to divide by m\n",
    "\n",
    "因为我们计算平均值\n",
    "because we're computing averages\n",
    "\n",
    "因此dw1/=m dw2/=m\n",
    "so dw1 divide equals m dw2 divide equals m\n",
    "\n",
    "db/=m 可以计算平均值\n",
    "db divide equals m in order to compute averages\n",
    "\n",
    "随着所有这些计算 你已经计算了\n",
    "and so with all of these calculations you've just computed\n",
    "\n",
    "损失函数J\n",
    "the derivative of the cost function J\n",
    "\n",
    "对各个参数w1 w2和b的导数\n",
    "with respect to each parameters w1 w2 and b\n",
    "\n",
    "回顾我们正在做的细节\n",
    "just come to details what we're doing\n",
    "\n",
    "我们使用dw1 dw2和db作为累加器\n",
    "we're using dw1 and dw2 and db to as accumulators right\n",
    "\n",
    "所以在这些计算之后\n",
    "so that after this computation you know\n",
    "\n",
    "你知道dw1等于 你的全局成本函数\n",
    "dw1 is equal to the derivative of your overall cost function\n",
    "\n",
    "对w1的导数\n",
    "with respect to w1\n",
    "\n",
    "对dw2和db也是一样 同时注意\n",
    "and similarly for dw2 and db so notice that\n",
    "\n",
    "dw1和dw2没有上标i\n",
    "dw1 and dw2 do not have a superscript i\n",
    "\n",
    "因为我们在这代码中 把它们作为累加器\n",
    "because we're using them in this code as accumulators\n",
    "\n",
    "去求取整个训练集上的和 相反地 dz^(i)\n",
    "to sum over the entire training set whereas in contrast dz^i\n",
    "\n",
    "这是对应于 单个训练样本的dz\n",
    "here this was um... dz with respect to just one single training example\n",
    "\n",
    "上标i指的是\n",
    "that is why that has a superscript i to refer to\n",
    "\n",
    "对应第i个训练样本的计算\n",
    "the one training example i that's computed on\n",
    "\n",
    "完成所有这些计算后\n",
    "and so having finished all these calculations\n",
    "\n",
    "应用一步梯度下降\n",
    "to implement one step of gradient descent you implement\n",
    "\n",
    "使得w1获得更新 即w1减去学习率乘上dw1\n",
    "w1 gets updated as w1 minus a learning rate times dw1\n",
    "\n",
    "w2更新 即w2减去 学习率乘上dw2\n",
    "w2 gives updates as w2 minus learning rate times dw2\n",
    "\n",
    "同时更新b b减去学习率乘上db\n",
    "and b gets update as b minus learning rate times db\n",
    "\n",
    "这里dw1 dw2和db 如你知道的被计算\n",
    "where dw1 dw2 and db where you know as computed\n",
    "\n",
    "最终这里的J也会\n",
    "and finally J here would also\n",
    "\n",
    "是你成本函数的正确值\n",
    "be a correct value for your cost function\n",
    "\n",
    "所以幻灯片上的所有东西\n",
    "so everything on the slide implements\n",
    "\n",
    "只应用了一次  梯度下降法\n",
    "just one single step of gradient descent\n",
    "\n",
    "因此你需要 重复以上内容很多次\n",
    "and so you have to repeat everything on this slide multiple times\n",
    "\n",
    "以应用多次梯度下降\n",
    "in order to take multiple steps of gradient descent\n",
    "\n",
    "看起来这些细节 似乎很复杂\n",
    "in case these details seem too complicated\n",
    "\n",
    "但目前不要担心太多\n",
    "again don't worry too much about it\n",
    "\n",
    "会清晰起来的\n",
    "for now hopefully all this will be clearer\n",
    "\n",
    "只要你在编程作业里 实现这些之后\n",
    "when you go and implement this in the programming assignment\n",
    "\n",
    "但它表明计算中有两个缺点\n",
    "but it turns out there are two weaknesses with the calculation\n",
    "\n",
    "当应用在这里的时候\n",
    "as with as with implements here\n",
    "\n",
    "就是说应用此方法到在logistic回归\n",
    "which is that to implement logistic regression this way\n",
    "\n",
    "你需要编写两个for循环 第一个for循环是\n",
    "you need to write two for loops the first for loop\n",
    "\n",
    "遍历m个训练样本的小循环\n",
    "is a small loop over the m training examples\n",
    "\n",
    "第二个for循环\n",
    "and the second for loop is\n",
    "\n",
    "是遍历所有特征的for循环\n",
    "a for loop over all the features over here\n",
    "\n",
    "这个例子中 我们只有2个特征\n",
    "right so in this example we just had two features\n",
    "\n",
    "所以n等于2 nx等于2\n",
    "so n is equal to 2 and n_x equals 2\n",
    "\n",
    "但如果你有更多特征\n",
    "but if you have more features\n",
    "\n",
    "你开始编写你的dw1 dw2\n",
    "you end up writing your dw1 dw2\n",
    "\n",
    "类似地计算dw3\n",
    "and you have similar computations for dw_3\n",
    "\n",
    "一直到dwn\n",
    "and so on down to dw_n\n",
    "\n",
    "看来你需要\n",
    "so seems like you need to\n",
    "\n",
    "一个for循环 遍历所有n个特征\n",
    "have a for loop over the features over all n features\n",
    "\n",
    "当你应用深度学习算法\n",
    "when you're implementing deep learning algorithms,\n",
    "\n",
    "你会发现 在代码中显式地使用for循环\n",
    "you'll find that having explicit for loops in your code\n",
    "\n",
    "会使算法很低效\n",
    "makes your algorithm run less efficiency\n",
    "\n",
    "同时在深度学习领域\n",
    "and so in the deep learning area\n",
    "\n",
    "会有越来越大的数据集\n",
    "would move to a bigger and bigger data sets\n",
    "\n",
    "所以能够应用你的算法\n",
    "and so being able to implement your algorithms\n",
    "\n",
    "完全不用显式for循环的话\n",
    "without using explicit for loops\n",
    "\n",
    "会是重要的 会帮助你\n",
    "is really important and will help you\n",
    "\n",
    "处理更大的数据集\n",
    "to scale to much bigger data sets\n",
    "\n",
    "有一门向量化技术\n",
    "so it turns out that there are set of techniques called vectorization\n",
    "\n",
    "帮助你的代码 摆脱这些显式的for循环\n",
    "techniques that allows you to get rid of these explicit for loops in your code\n",
    "\n",
    "我想 在深度学习时代的早期\n",
    "I think in the pre deep learning era\n",
    "\n",
    "也就是深度学习兴起之前\n",
    "that's before the rise of deep learning\n",
    "\n",
    "向量化是很棒的\n",
    "vectorization was a nice to have\n",
    "\n",
    "有时候用来加速运算\n",
    "you could sometimes do it to speed a vehicle\n",
    "\n",
    "但有时候也未必能够 但是在深度学习时代\n",
    "and sometimes not but in the deep learning era vectorization\n",
    "\n",
    "用向量化来摆脱for循环\n",
    "that is getting rid of for loops like this\n",
    "\n",
    "已经变得相当重要\n",
    "and like this has become really important\n",
    "\n",
    "因为我们越来越多地 训练非常大的数据集\n",
    "because we're more and more training on very large datasets\n",
    "\n",
    "你的代码变需要得非常高效\n",
    "and so you really need your code to be very efficient\n",
    "\n",
    "接下来的几个视频 我们会谈到向量化\n",
    "so in the next few videos we'll talk aboutvectorization\n",
    "\n",
    "以及如何应用这些 做到一个for循环都不需要使用\n",
    "and how to implement all this without using even a single for loop\n",
    "\n",
    "我希望你理解了\n",
    "so of this I hope you have a sense\n",
    "\n",
    "如何应用logistic回归\n",
    "of how to implement logistic regression\n",
    "\n",
    "对logistic回归应用梯度下降法\n",
    "or gradient descent for logistic regression\n",
    "\n",
    "在你进行编程练习后 应该会掌握得更清楚\n",
    "um... things will be clearer when you implement the program exercise\n",
    "\n",
    "但在做编程练习之前\n",
    "but before actually doing the program exercise\n",
    "\n",
    "让我们先谈谈“向量化”\n",
    "let's first talk about vectorization\n",
    "\n",
    "然后你可以做到全部的这些\n",
    "so then you can implement this whole thing\n",
    "\n",
    "实现一个梯度下降法的迭代\n",
    "implement a single iteration of gradient descent\n",
    "\n",
    "而不需要for循环\n",
    "without using any for loop\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    " \n",
    "**PS: 欢迎扫码关注公众号：「SelfImprovementLab」！专注「深度学习」，「机器学习」，「人工智能」。以及 「早起」，「阅读」，「运动」，「英语 」「其他」不定期建群 打卡互助活动。**\n",
    "\n",
    "<center><img src=\"http://upload-images.jianshu.io/upload_images/1157146-cab5ba89dfeeec4b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\"></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
