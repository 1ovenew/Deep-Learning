{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursera | Andrew Ng (01-week-2-2.X)—xxxxxx\n",
    "\n",
    " 该系列仅在原课程基础上部分知识点添加个人学习笔记，或相关推导补充等。如有错误，还请批评指教。在学习了 Andrew Ng 课程的基础上，为了更方便的查阅复习，将其整理成文字。因本人一直在学习英语，所以该系列以英文为主，同时也建议读者以英文为主，中文辅助，以便后期进阶时，为学习相关领域的学术论文做铺垫。- ZJ\n",
    "    \n",
    ">[Coursera 课程](https://www.coursera.org/specializations/deep-learning) |[deeplearning.ai](https://www.deeplearning.ai/) |[网易云课堂](https://mooc.study.163.com/smartSpec/detail/1001319001.htm)\n",
    "\n",
    "---\n",
    "   **转载请注明作者和出处：微信公众号-「SelfImprovementLab」**\n",
    "   \n",
    "   [知乎](https://zhuanlan.zhihu.com/c_147249273)：https://zhuanlan.zhihu.com/c_147249273\n",
    "   \n",
    "   [CSDN]()：\n",
    "   \n",
    "   [简书]()：\n",
    "\n",
    "---\n",
    "\n",
    "(字幕来源：网易云课堂)\n",
    "\n",
    "Hello, and welcome back.In this week we're going to go over the basics of neural network programming.It turns out that when you implement a neural network,there are some techniques that are going to be really important.For example, if you have a training set of m training examples,\n",
    "you might be used to processing the training set by having a for loop step through your m training examples.But it turns out that when you're implementing a neural network,you usually want to process your entire training set without using an explicit for loop to loop over your entire training set.So, you'll see how to do that in this week's materials.\n",
    "\n",
    "大家好，欢迎回来,本周我们会学习神经网络编程的基础知识。当你要构建一个神经网络，有些技巧是相当重要的。例如 m 个样本的训练集，你可能会习惯性地去用一个 for 循环，来遍历这 m 个样本。但事实上，实现一个神经网络，如果你要遍历整个训练集，并不需要直接使用 for 循环。本周的课程你会学到如何做到。\n",
    "\n",
    "\n",
    "Another idea, when you organize the computation of, in your network,usually you have what's called a forward pass or forward propagation step,\n",
    "followed by a backward pass or what's called a backward propagation step.And so in this week's materials, you also get an introduction about why the computations in learning an neural network can be organized in this forward propagation and a separate backward propagation.For this week's materials,I want to convey these ideas using logistic regression in order to make the ideas easier to understand.But even if you've seen logistic regression before,I think that there'll be some new and interesting ideas for you to pick up in this week's materials.So with that, let's get started.\n",
    "\n",
    "还有就是，神经网络的计算过程中通常有一个正向过程，或者叫正向传播步骤，接着会有一个反向步骤，也叫做反向传播步骤。这周的学习材料中 我也会给你介绍为什么神经网络的计算过程可以分为 **前向传播** 和 **反向传播** 两个分开的过程。本周课程中，我会用 logistic 回归来阐述，以便于你能更好地理解。如果你之前学过 logistic 回归，我也认为，这周的学习材料也会带给你一些新的、有意思的想法，下面开始吧。\n",
    "\n",
    "\n",
    "Logistic regression is an algorithm for binary classification.So let's start by setting up the problem.Here's an example of a binary classification problem.You might have an input of an image, like that,and want to output a label to recognize this image as being either a cat, in which case you output 1,or not-cat in which case you output 0,and we're going to use y to denote the output label.Let's look at how an image is represented in a computer.\n",
    "\n",
    "logistic回归是一个用于二分分类的算法。我们从一个问题开始。这有一个二分分类问题的例子。假如你有一张图片作为输入，这样子的，你想输出识别此图的标签，如果是猫 输出1，如果不是 则输出0，我们用y来表示输出的结果标签。来看看一张图片 在计算机中是如何表示的。\n",
    "\n",
    "\n",
    "To store an image your computer stores three separate matrices corresponding to the red, green,and blue color channels of this image.So if your input image is 64 pixels by 64 pixels,then you would have 3 64 by 64 matrices corresponding to the red, green and blue pixel intensity values for your images.Although to make this little slide I drew these as much smaller matrices,so these are actually 5 by 4 matrices rather than 64 by 64.So to turn these pixel intensity values intoa feature vector, what we're going to do is unroll all of these pixel values into an input feature vector x.\n",
    "\n",
    "计算机保存一张图片 要保存三个独立矩阵，分别对应图片中的 红、绿、蓝三个颜色通道，如果输入图片是64×64像素的，就有三个 64×64 的矩阵 分别对应图片中 红、绿、蓝 三种像素的亮度。为了方便表示，这里我用三个小矩阵，它们是5×4的 并不是64×64的。要把这些像素亮度值，放进一个特征向量中，就要把这些像素值都提出来 放入一个特征向量 x。\n",
    "\n",
    "\n",
    "So to unroll all these pixel intensity values into feature vector,what we're going to do is define a feature vector x corresponding to this image as follows.We're just going to take all the pixel values 255, 231, and so on.255, 231, and so on until we've listed all the red pixels.\n",
    "And then eventually 255 134 255,134 and so on,until we get a long feature vector listing out all the red, green and blue pixel intensity values of this image.If this image is a 64 by 64 image, the total dimension of this vector x will be 64 by 64 by 3 because that's the total numbers we have in all of these matrixes.\n",
    "\n",
    "为了把这些像素值取出放入特征向量，就要像下面这样定义一个特征向量x 以表示这张图片。我们把所有的像素值都取出来 例如255、231这些255、231 等等 直到列完所有的红色像素，接着是255、134、255、134 等等。最后得到一个很长的特征向量 把图片中所有的红、绿、蓝像素强度值都列出来。如果图片是64×64的 那么向量x的总维度 就是64×64×3，因为这是三个矩阵的元素数量。\n",
    "\n",
    "\n",
    "Which in this case, turns out to be 12,288,that's what you get if you multiply all those numbers.And so we're going to use nx=12288  to represent the dimension of the input features x.And sometimes for brevity, I will also just use lowercase n to represent the dimension of this input feature vector.So in binary classification, our goal is to learn a classifier that can input an image represented by this feature vector x.And predict whether the corresponding label y is 1 or 0,that is, whether this is a cat image or a non-cat image.\n",
    "\n",
    "对于这个例子 数字是12288，把它们乘起来 这就是结果。我们用 nx=12288，来表示输入的特征向量x的维度。有时候为了简洁 我会直接用小写的 n，来表示输入的特征向量的维度。在二分分类问题中 目标是训练出一个分类器 它以图片的特征向量x作为输入。预测输出的结果标签y 是1还是0。也就是 预测图片中是否有猫\n",
    "\n",
    "\n",
    "\n",
    "Let's now lay out some of the notation that we'll use throughout the rest of this course.A single training example is represented by a pair (x, y),where x is nx-dimensional feature vector and y, the label, is either 0 or 1.Your training sets will comprise lower-case m training examples.And so your training sets will be written (x(1), y(1)) which is the input and output for your first training example (x(2), y(2)) for the second training example\n",
    "\n",
    "现在，我们看看在后面课程中需要用到的一些符号。用一对(x,y) 来表示一个单独的样本，x是nx维的特征向量，标签y 值为0或1，训练集由m个训练样本构成。(x^(1),y^(1))表示样本一的 输入和输出 (x^(2),y^(2))表示样本二 (x^(m),y^(m))表示最后一个样本m\n",
    "\n",
    "up to (x(m),y(m)) which is your last training example.\n",
    "\n",
    "这些一起就表示整个训练集\n",
    "And then that altogether is your entire training set.\n",
    "\n",
    "用小写的字母m\n",
    "So I'm going to use lowercase m\n",
    "\n",
    "来表示训练样本的个数\n",
    "to denote the number of training samples.\n",
    "\n",
    "有时候为了强调 这是训练样本的个数\n",
    "And sometimes to emphasize that this is the number of train examples,\n",
    "\n",
    "可以写作m=m_train\n",
    "I might write this as m = m train.\n",
    "\n",
    "当说到测试集时\n",
    "And when we talk about a test set,\n",
    "\n",
    "我们会用m 下标test\n",
    "we might sometimes use m subscript test\n",
    "\n",
    "来表示测试集的样本数\n",
    "to denote the number of test examples.\n",
    "\n",
    "所以 这是测试集的样本数\n",
    "So that's the number of test examples.\n",
    "\n",
    "最后 用更紧凑的符号表示训练集\n",
    "Finally, to put all of the training examples into a more compact notation,\n",
    "\n",
    "我们定义一个矩阵 用大写的X表示\n",
    "we're going to define a matrix, capital X.\n",
    "\n",
    "它由训练集中的x1、x2这些组成\n",
    "As defined by taking you training set inputs x1, x2 and so on\n",
    "\n",
    "像这样写成矩阵的列\n",
    "and stacking them in columns.\n",
    "\n",
    "现在我们把x^(1) 放进矩阵的第一列\n",
    "So we take x1 and put that as a first column of this matrix,\n",
    "\n",
    "x^(2)是第二列 …… xm是第m列\n",
    "x2, put that as a second column and so on down to xm,\n",
    "\n",
    "最后得到矩阵X\n",
    "then this is the matrix capital X.\n",
    "\n",
    "这个矩阵有m列\n",
    "So this matrix X will have m columns,\n",
    "\n",
    "m是训练集的样本数\n",
    "where m is the number of train examples\n",
    "\n",
    "这个矩阵的高度记为nx\n",
    "and the number of railroads, or the height of this matrix is nx.\n",
    "\n",
    "要注意的是 有时候矩阵X的定义\n",
    "Notice that in other causes, you might see the matrix capital X\n",
    "\n",
    "训练样本作为行向量堆叠 而不是这样列向量堆叠\n",
    "defined by stacking up the train examples in rows like so,\n",
    "\n",
    "x^(1)转置 …… x^(m)转置\n",
    "x1 transpose down to xm transpose.\n",
    "\n",
    "构建神经网络时\n",
    "It turns out that when you're implementing neural networks using\n",
    "\n",
    "用左边这个约定形式\n",
    "this convention I have on the left,\n",
    "\n",
    "会让构建过程简单得多\n",
    "will make the implementation much easier.\n",
    "\n",
    "现在回顾一下 X是一个nx×m矩阵\n",
    "So just to recap, X is a nx by m dimensional matrix,\n",
    "\n",
    "当你用Python实现的时候\n",
    "and when you implement this in Python,\n",
    "\n",
    "你会看到X.shape 这是一条Python命令\n",
    "you see that X.shape, that's the python command\n",
    "\n",
    "用来输出矩阵的维度 即(nx,m)\n",
    "for founding the shape of the matrix, that this an nx, m.\n",
    "\n",
    "表示X是一个nx×m矩阵\n",
    "That just means it is an nx by m dimensional matrix.\n",
    "\n",
    "这就是如何将训练样本 即输入x 用矩阵表示\n",
    "So that's how you group the training examples, input x into matrix.\n",
    "\n",
    "那输出标签y呢？\n",
    "How about the output labels y?\n",
    "\n",
    "同样 为了方便\n",
    "It turns out that\n",
    "\n",
    "构建一个神经网络\n",
    "to make your implementation of a neural network easier,\n",
    "\n",
    "将y标签也放到列中\n",
    "it would be convenient to also stack y in columns.\n",
    "\n",
    "我们定义Y 是y^(1) y^(2)\n",
    "So we're going to define capital Y to be equal to y1, y2\n",
    "\n",
    "一直到y(m)\n",
    "up to ym like so.\n",
    "\n",
    "这里的Y是一个1×m矩阵\n",
    "So Y here will be a 1 by m dimensional matrix.\n",
    "\n",
    "同样地 在Python里 Y.shape等于(1,m)\n",
    "And again, to use the Python notation then the shape of Y will be 1, m.\n",
    "\n",
    "表示这是一个1×m矩阵\n",
    "Which just means this is a 1 by m matrix.\n",
    "\n",
    "在后面的课程 要实现神经网络时\n",
    "And as you implement your neural network,\n",
    "\n",
    "你会发现\n",
    "later in this course, you find that\n",
    "\n",
    "好的惯例符号\n",
    "a useful convention would be\n",
    "\n",
    "能够将不同训练样本的数据联系起来\n",
    "to take the data associated with different training examples,\n",
    "\n",
    "这里说的数据 不仅有x和y 还会有之后其他的量\n",
    "and by data I mean either x or y, or other quantities you see later.\n",
    "\n",
    "将不同的训练样本数据\n",
    "But to take the stuff or the data\n",
    "\n",
    "取出来\n",
    "associated with different training examples\n",
    "\n",
    "放到不同的列上\n",
    "and to stack them in different columns,\n",
    "\n",
    "就像刚刚我们处理x和y那样\n",
    "like we've done here for both x and y.\n",
    "\n",
    "这门课中 在logistic回归\n",
    "So, that's the notation we we'll use for logistic regression\n",
    "\n",
    "和神经网络 要用到的符号就是这些了\n",
    "and for neural networks networks later in this course.\n",
    "\n",
    "如果你忘了这些符号的意义\n",
    "If you ever forget what a piece of notation means,\n",
    "\n",
    "比如什么是m 什么是n 或者其他\n",
    "like what is m or what is n or what is something else,\n",
    "\n",
    "我们也会在课程网站上 放上符号说明\n",
    "we've also posted on the course website a notation guide\n",
    "\n",
    "这样你就可以快速地查阅\n",
    "that you can use to quickly look up\n",
    "\n",
    "每个符号的意义\n",
    "what any particular piece of notation means.\n",
    "\n",
    "就这样吧 下个课程视频中\n",
    "So with that, let's go on to the next video\n",
    "\n",
    "我们以logistic回归作为开始\n",
    "where we'll start to fetch out logistic regression using this notation.\n",
    "\n",
    "\n",
    "---\n",
    " \n",
    "**PS: 欢迎扫码关注公众号：「SelfImprovementLab」！专注「深度学习」，「机器学习」，「人工智能」。以及 「早起」，「阅读」，「运动」，「英语 」「其他」不定期建群 打卡互助活动。**\n",
    "\n",
    "<img src=\"http://upload-images.jianshu.io/upload_images/1157146-cab5ba89dfeeec4b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
