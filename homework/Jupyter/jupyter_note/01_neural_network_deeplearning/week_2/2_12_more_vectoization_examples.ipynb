{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursera | Andrew Ng (01-week-2-2.12)—More Vectorization examples\n",
    "\n",
    "该系列仅在原课程基础上部分知识点添加个人学习笔记，或相关推导补充等。如有错误，还请批评指教。在学习了 Andrew Ng 课程的基础上，为了更方便的查阅复习，将其整理成文字。因本人一直在学习英语，所以该系列以英文为主，同时也建议读者以英文为主，中文辅助，以便后期进阶时，为学习相关领域的学术论文做铺垫。- ZJ\n",
    "    \n",
    ">[Coursera 课程](https://www.coursera.org/specializations/deep-learning) |[deeplearning.ai](https://www.deeplearning.ai/) |[网易云课堂](https://mooc.study.163.com/smartSpec/detail/1001319001.htm)\n",
    "\n",
    "---\n",
    "   **转载请注明作者和出处：ZJ 微信公众号-「SelfImprovementLab」**\n",
    "   \n",
    "   [知乎](https://zhuanlan.zhihu.com/c_147249273)：https://zhuanlan.zhihu.com/c_147249273\n",
    "   \n",
    "   [CSDN]()：\n",
    "   \n",
    "\n",
    "---\n",
    "更多向量化的例子\n",
    "\n",
    "\n",
    "在前面几个视频中 (字幕来源：网易云课堂)\n",
    "In the previous video\n",
    "\n",
    "\n",
    "，你们看到如何向量化\n",
    "you saw a few examples of how vectorization,\n",
    "\n",
    "，如何使用内置函数 避免使用显示for循环\n",
    "by using built in functions and by avoiding explicit for loops,\n",
    "\n",
    "，可以让程序运行速度显著加快\n",
    "allows you to speed up your code significantly.\n",
    "\n",
    "，我们再来看几个例子\n",
    "Let's look at a few more examples.\n",
    "\n",
    "，要记住 经验法则是\n",
    "The rule of thumb to keep in mind is,\n",
    "\n",
    "，当你编写新的网络时\n",
    "when you're programming your new networks,\n",
    "\n",
    "，或者你做的只是回归\n",
    "or when you're programming just a regression,\n",
    "\n",
    "，那么一定要尽量避免for循环\n",
    "whenever possible avoid explicit for-loops.\n",
    "\n",
    "，能不用就不用\n",
    "And it's not always possible to never use a for-loop,\n",
    "\n",
    "，如果你可以使用一个内置函数\n",
    "but when you can use a built in function\n",
    "\n",
    "，或者找出其他办法去计算循环\n",
    "or find some other way to compute whatever you need,\n",
    "\n",
    "\n",
    "，通常会比直接用for循环更快\n",
    "you'll often go faster than if you have an explicit for-loop.\n",
    "\n",
    "\n",
    "，我们来看另一个例子\n",
    "Let's look at another example.\n",
    "\n",
    "\n",
    "，如果你想计算\n",
    "If ever you want to compute\n",
    "\n",
    "，一个向量u 作为矩阵A\n",
    "a vector u as the product of the matrix A,\n",
    "\n",
    "\n",
    "，和另一个向量v的乘积 矩阵乘法的定义就是\n",
    "and another vector v, then the definition of our matrix multiply\n",
    "\n",
    "\n",
    "，就是u_i等于 A_ij v_j 对j求和\n",
    "is that your u_i is equal to sum over j, A_ij, v_j.\n",
    "\n",
    "\n",
    "，这是u_i的定义\n",
    "That's how you define u_i.\n",
    "\n",
    "\n",
    "，所以这些计算的非向量表示\n",
    "And so the non-vectorized implementation of this\n",
    "\n",
    "，就是令u等于np.zeros(n,1)\n",
    "would be to set u equals NP.Zeros, it would be n by 1.\n",
    "\n",
    "，对i循环  对j循环\n",
    "For I, and so on. For j, and so on..\n",
    "\n",
    "，然后u[i]+=A[i][j]·v[j]\n",
    "And then u[i]+=A[i][j]·v[j]\n",
    "\n",
    "，现在 这是一个双重for循环 对指标i和j循环\n",
    "So now, this is two for-loops, looping over both I and j.\n",
    "\n",
    "，所以这是一个非向量化的版本\n",
    "So, that's a non-vectorized version,\n",
    "\n",
    "，这个向量化实现就是说u=np.dot(A,v)\n",
    "the vectorized implementation which is to say u equals np dot (A,v)\n",
    "\n",
    "\n",
    "，而在右边的实现 就是向量化版本\n",
    "And the implementation on the right, the vectorized version,\n",
    "\n",
    "，消除了两个不同的for循环\n",
    "now eliminates two different for-loops,\n",
    "\n",
    "\n",
    "，速度会更快\n",
    "and it's going to be way faster.\n",
    "\n",
    "\n",
    "，我们再看一个例子\n",
    "Let's go through one more example.\n",
    "\n",
    "，假设你内存里已经有一个向量v\n",
    "Let's say you already have a vector, v, in memory\n",
    "\n",
    "\n",
    "，如果你想做指数运算\n",
    "and you want to apply the exponential operation\n",
    "\n",
    "\n",
    "，作用到向量v的每个元素\n",
    "on every element of this vector v.\n",
    "\n",
    "，你可以令u等于那个向量 这是e^(v1)\n",
    "So you can put u equals the vector, that's e to the v1,\n",
    "\n",
    "，e^(v2) 一直到e^(vn)\n",
    "e to the v2, and so on, down to e to the vn.\n",
    "\n",
    "，所以这是一个非向量化的实现\n",
    "So this would be a non-vectorized implementation,\n",
    "\n",
    "，一开始你让u初始化成全0向量\n",
    "which is at first you initialize u to the vector of zeros.\n",
    "\n",
    "，然后你有一个for循环\n",
    "And then you have a for-loop\n",
    "\n",
    "，一次计算一个元素\n",
    "that computes the elements one at a time.\n",
    "\n",
    "\n",
    "，但事实上 python的numpy里面有很多内置函数\n",
    "But it turns out that Python and NumPy have many built-in functions\n",
    "\n",
    "\n",
    "，可以让你计算这些向量\n",
    "that allow you to compute these vectors\n",
    "\n",
    "，你只需要调用单个函数\n",
    "with just a single call to a single function.\n",
    "\n",
    "\n",
    "，所以我去实现的时候 会导入import numpy as np\n",
    "So what I would do to implement this is import numpy as np,\n",
    "\n",
    "\n",
    "，这样你就可以调用u=np.exp(v)\n",
    "and then what you just call u = np dot Exp(v)\n",
    "\n",
    "，要注意\n",
    "And so, notice that,\n",
    "\n",
    "\n",
    "，之前你有这个显式for循环\n",
    "whereas previously you had that explicit for-loop,\n",
    "\n",
    "\n",
    "，这里只需一行代码\n",
    "with just one line of code here,\n",
    "\n",
    "，v作为输入向量 u作为输出向量\n",
    "just v as an input vector u as an output vector,\n",
    "\n",
    "，你已经去掉了显式for循环\n",
    "you've gotten rid of the explicit for-loop,\n",
    "\n",
    "，右边的代码实现会快得多\n",
    "and the implementation on the right will be much faster\n",
    "\n",
    "，这需要一个显式的for循环\n",
    "that the one needing an explicit for-loop.\n",
    "\n",
    "，实际上 numpy库\n",
    "In fact, the NumPy library\n",
    "\n",
    "，有很多向量值函数\n",
    "has many of the vector value functions.\n",
    "\n",
    "\n",
    "，np.log会逐个元素计算log\n",
    "So np.log will compute the element-wise log,\n",
    "\n",
    "，np.Abs会计算绝对值\n",
    "np.Abs computes the absolute value,\n",
    "\n",
    "，np.maximum计算所有元素中的最大值\n",
    "np.maximum computes the element-wise maximum\n",
    "\n",
    "\n",
    "，求出v中所有元素和0之间相比的最大值\n",
    "to take the max of every element of v with 0.\n",
    "\n",
    "，v**2就是v中每个元素的平方\n",
    "v**2 just takes the element-wise square of each element of v.\n",
    "\n",
    "，1/v就是每个元素求倒数 等等\n",
    "1/v takes the element-wise inverse, and so on.\n",
    "\n",
    "，所以每当你想写一个for循环时\n",
    "So, whenever you are tempted to write a for-loop take a look,\n",
    "\n",
    "，应该看看可不可以调用numpy\n",
    "and see if there's a way to call a NumPy\n",
    "\n",
    "，用内置函数计算 而不是用for循环\n",
    "built-in function to do it without that for-loop.\n",
    "\n",
    "，所以我们看看学到的这些技巧 怎么运用到\n",
    "So, let's take all of these learnings and apply it to\n",
    "\n",
    "，logistic回归梯度下降算法实现中来\n",
    "our logistic regression gradient descent implementation,\n",
    "\n",
    "，看看是否可以去掉两个for循环中的一个\n",
    "and see if we can at least get rid of one of the two for-loops we had.\n",
    "\n",
    "，这是我们用来计算\n",
    "So here's our code for computing\n",
    "\n",
    "，logistic回归导数的程序\n",
    "the derivatives for logistic regression,\n",
    "\n",
    "，我们有两个for循环\n",
    "and we had two for-loops.\n",
    "\n",
    "，一个在上面这里 第二个是这里\n",
    "One was this one up here, and the second one was this one.\n",
    "\n",
    "，所以在我们的例子中 有n_x=2\n",
    "So in our example we had nx equals 2,\n",
    "\n",
    "，如果你的特征不止两个的话\n",
    "but if you had more features than just 2 features\n",
    "\n",
    "，你需要用for循环处理dw_1 dw_2 dw_3 等等\n",
    "then you'd need have a for-loop over dw_1, dw_2, dw_3, and so on.\n",
    "\n",
    "，所以这里其实有个for j=1 to n_x\n",
    "So its as if there's actually a for j equals 1, to n_x.\n",
    "\n",
    "，dw_j就更新了\n",
    "dw_j gets updated.\n",
    "\n",
    "，所以我想去掉这第二个for循环\n",
    "So we'd like to eliminate this second for-loop.\n",
    "\n",
    "，这就是我们上一张幻灯片做的\n",
    "That's what we'll do on this slide.\n",
    "\n",
    "，做法就是\n",
    "So the way we'll do so is that\n",
    "\n",
    "，这里我们不会显式地把dw_1 dw_2 等等初始化成0\n",
    "instead of explicitly initializing dw_1, dw_2, and so on to zeros,\n",
    "\n",
    "，我们要去掉这个循环 把dw变成一个向量\n",
    "we're going to get rid of this and instead make dw a vector.\n",
    "\n",
    "，我们令dw=np.Zeros\n",
    "So we're going to set dw equals np.Zeros,\n",
    "\n",
    "\n",
    "，然后我们把这个变成n_x×1维向量\n",
    "and let's make this a nx by 1, dimensional vector.\n",
    "\n",
    "，然后在这里我们不需要对单个分量用for循环\n",
    "Then, here, instead of this for loop over the individual components,\n",
    "\n",
    "，我们用这个向量值操作\n",
    "we'll just use this vector value operation,\n",
    "\n",
    "\n",
    "，就是dw+=x^(i) dz^(i)\n",
    "dw+=x^(i) dz^(i)\n",
    "\n",
    "，最后 我们不用这个\n",
    "And then finally, instead of this,\n",
    "\n",
    "，我们就用dw/=m\n",
    "we will just have dw divides equals m.\n",
    "\n",
    "，现在我们从两个for循环化简成一个for循环\n",
    "So now we've gone from having two for-loops to just one for-loop.\n",
    "\n",
    "，我们这里还有一个for循环\n",
    "We still have this one for-loop\n",
    "\n",
    "，对单独的训练例子循环\n",
    "that loops over the individual training examples.\n",
    "\n",
    "，所以我希望本视频能给你向量化的概念\n",
    "So I hope this video gave you a sense of vectorization.\n",
    "\n",
    "，同时去掉一个for循环之后\n",
    "And by getting rid of one for-loop\n",
    "\n",
    "，你的代码运行速度会大大加快\n",
    "your code will already run faster.\n",
    "\n",
    "，事实证明我们还可以做得更好\n",
    "But it turns out we could do even better.\n",
    "\n",
    "，所以下一个视频将谈论\n",
    "So the next video will talk about\n",
    "\n",
    "，如何进一步向量化logistic回归\n",
    "how to vectorize logistic regression even further.\n",
    "\n",
    "，你会看到一个非常惊人的结果\n",
    "And you see a pretty surprising result,\n",
    "\n",
    "\n",
    "，就是没有任何for循环\n",
    "that without using any for-loops,\n",
    "\n",
    "，对所有训练样本不用任何for循环\n",
    "without needing a for-loop over the training examples,\n",
    "\n",
    "，你可以写代码一次处理整个训练集\n",
    "you could write code to process the entire training sets.\n",
    "\n",
    "，基本上同时处理\n",
    "So, pretty much all at the same time.\n",
    "\n",
    "，我们来看看下一个视频\n",
    "So, let's see that in the next video.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### <font color=#0099ff>重点总结：</font>\n",
    "\n",
    "\n",
    "参考文献：\n",
    "\n",
    "[1]. 大树先生.[吴恩达Coursera深度学习课程 DeepLearning.ai 提炼笔记（1-2）– 神经网络基础](http://blog.csdn.net/junjun_zhao/article/details/78855589)\n",
    "\n",
    "\n",
    "---\n",
    " \n",
    "**PS: 欢迎扫码关注公众号：「SelfImprovementLab」！专注「深度学习」，「机器学习」，「人工智能」。以及 「早起」，「阅读」，「运动」，「英语 」「其他」不定期建群 打卡互助活动。**\n",
    "\n",
    "<center><img src=\"http://upload-images.jianshu.io/upload_images/1157146-cab5ba89dfeeec4b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\"></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
