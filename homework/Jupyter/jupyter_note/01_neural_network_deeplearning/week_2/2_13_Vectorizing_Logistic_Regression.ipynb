{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursera | Andrew Ng (01-week-2-2.13)—Vectorizing Logistic Regression\n",
    "\n",
    "该系列仅在原课程基础上部分知识点添加个人学习笔记，或相关推导补充等。如有错误，还请批评指教。在学习了 Andrew Ng 课程的基础上，为了更方便的查阅复习，将其整理成文字。因本人一直在学习英语，所以该系列以英文为主，同时也建议读者以英文为主，中文辅助，以便后期进阶时，为学习相关领域的学术论文做铺垫。- ZJ\n",
    "    \n",
    ">[Coursera 课程](https://www.coursera.org/specializations/deep-learning) |[deeplearning.ai](https://www.deeplearning.ai/) |[网易云课堂](https://mooc.study.163.com/smartSpec/detail/1001319001.htm)\n",
    "\n",
    "---\n",
    "   **转载请注明作者和出处：ZJ 微信公众号-「SelfImprovementLab」**\n",
    "   \n",
    "   [知乎](https://zhuanlan.zhihu.com/c_147249273)：https://zhuanlan.zhihu.com/c_147249273\n",
    "   \n",
    "   [CSDN]()：\n",
    "   \n",
    "\n",
    "---\n",
    "向量化 Logistic 回归\n",
    "\n",
    "\n",
    "\n",
    "我们已经讨论过向量化(字幕来源：网易云课堂)\n",
    "We have talked about how vectorization\n",
    "\n",
    "是如何显著地加速你的代码\n",
    "lets you speed up your code significantly.\n",
    "\n",
    "在这次视频中\n",
    "In this video, we'll talk about\n",
    "\n",
    "我们将会谈及向量化是如何 实现在logistic回归的上面的\n",
    "how you can vectorize the implementation of logistic regression,\n",
    "\n",
    "这样就能同时处理整个训练集\n",
    "so they can process an entire training set,\n",
    "\n",
    "来实现梯度下降法的一步迭代\n",
    "that is implement a single iteration of gradient descent\n",
    "\n",
    "\n",
    "针对整个训练集的一步迭代\n",
    "with respect to an entire training set\n",
    "\n",
    "不需要使用任何显式for循环\n",
    "without using even a single explicit for loop.\n",
    "\n",
    "对于这一项技术 我特别兴奋\n",
    "I'm super excited about this technique,\n",
    "\n",
    "并且当我们后面谈及神经网络时\n",
    "and when we talk about neural networks later\n",
    "\n",
    "11\n",
    "00:00:26,158 --> 00:00:29,557\n",
    "也可以完全不用显式for循环\n",
    "without using even a single explicit for loop.\n",
    "\n",
    "12\n",
    "00:00:29,945 --> 00:00:33,042\n",
    "那我们开始吧 我们先回顾\n",
    "Let's get started. Let's first examine\n",
    "\n",
    "13\n",
    "00:00:33,080 --> 00:00:35,928\n",
    "logistic回归的正向传播步骤\n",
    "the forward propagation steps of logistic regression.\n",
    "\n",
    "14\n",
    "00:00:35,965 --> 00:00:37,814\n",
    "如果你有m个训练样本\n",
    "So, if you have m training examples,\n",
    "\n",
    "15\n",
    "00:00:37,856 --> 00:00:40,536\n",
    "那么对第一个样本进行预测\n",
    "then to make a prediction on the first example,\n",
    "\n",
    "16\n",
    "00:00:40,579 --> 00:00:41,787\n",
    "你需要这样计算\n",
    "you need to compute that,\n",
    "\n",
    "17\n",
    "00:00:41,835 --> 00:00:45,742\n",
    "计算出z 运用这个熟悉的公式\n",
    "compute z. I'm using this familiar formula,\n",
    "\n",
    "18\n",
    "00:00:45,785 --> 00:00:47,566\n",
    "然后计算激活函数\n",
    "then compute the activations,\n",
    "\n",
    "19\n",
    "00:00:47,609 --> 00:00:49,316\n",
    "计算在第一个样本的y帽\n",
    "you compute y hat in the first example.\n",
    "\n",
    "20\n",
    "00:00:49,352 --> 00:00:52,308\n",
    "然后继续去对第二个训练样本做一个预测\n",
    "Then to make a prediction on the second training example,\n",
    "\n",
    "21\n",
    "00:00:52,382 --> 00:00:53,995\n",
    "你需要这样计算\n",
    "you need to compute that.\n",
    "\n",
    "22\n",
    "00:00:54,047 --> 00:00:57,207\n",
    "然后去对第三个样本做一个预测\n",
    "Then, to make a prediction on the third example,\n",
    "\n",
    "23\n",
    "00:00:57,244 --> 00:00:59,161\n",
    "你需要这样计算 以此类推\n",
    "you need to compute that, and so on.\n",
    "\n",
    "24\n",
    "00:00:59,194 --> 00:01:01,269\n",
    "你可能需要这样做上m次\n",
    "And you might need to do this m times,\n",
    "\n",
    "25\n",
    "00:01:01,301 --> 00:01:03,159\n",
    "如果你有m个样本\n",
    "if you have M training examples.\n",
    "\n",
    "26\n",
    "00:01:03,629 --> 00:01:04,986\n",
    "可以看出\n",
    "So, it turns out,\n",
    "\n",
    "27\n",
    "00:01:05,024 --> 00:01:08,683\n",
    "为了执行正向传播步骤\n",
    "that in order to carry out the forward propagation step,\n",
    "\n",
    "28\n",
    "00:01:08,714 --> 00:01:13,118\n",
    "需要对m个训练样本 都计算出预测结果\n",
    "that is to compute these predictions on our m training examples,\n",
    "\n",
    "29\n",
    "00:01:13,141 --> 00:01:14,839\n",
    "但有一个办法可以\n",
    "there is a way to do so,\n",
    "\n",
    "30\n",
    "00:01:14,871 --> 00:01:17,462\n",
    "不需要任何一个显式的for循环\n",
    "without needing an explicit for loop.\n",
    "\n",
    "31\n",
    "00:01:18,047 --> 00:01:19,535\n",
    "让我们看看如何做到\n",
    "Let's see how you can do it.\n",
    "\n",
    "32\n",
    "00:01:19,592 --> 00:01:24,072\n",
    "首先记得我们曾定义过一个矩阵大写X\n",
    "First, remember that we defined a matrix capital X\n",
    "\n",
    "33\n",
    "00:01:24,114 --> 00:01:26,304\n",
    "来作为你的训练输入\n",
    "to be your training inputs,\n",
    "\n",
    "34\n",
    "00:01:26,345 --> 00:01:30,414\n",
    "像这样子在不同的列中堆叠在一起\n",
    "stacked together in different columns like this.\n",
    "\n",
    "35\n",
    "00:01:30,436 --> 00:01:33,224\n",
    "这就是一个矩阵\n",
    "So, this is a matrix,\n",
    "\n",
    "36\n",
    "00:01:33,282 --> 00:01:38,363\n",
    "这是一个nx×m的矩阵\n",
    "that is a NX by M matrix.\n",
    "\n",
    "37\n",
    "00:01:38,417 --> 00:01:41,961\n",
    "我现在写的是Python numpy形式\n",
    "So, I'm writing this as a Python numpy shape,\n",
    "\n",
    "38\n",
    "00:01:42,015 --> 00:01:48,878\n",
    "这只是意味 X是一个nx × m的矩阵\n",
    "this just means that X is a NX by m dimensional matrix.\n",
    "\n",
    "39\n",
    "00:01:48,921 --> 00:01:55,054\n",
    "现在我首先想要做的是 告诉你如何计算z^(1) z^(2)\n",
    "Now, the first thing I want to do is show how you can compute z^(1), z^(2),\n",
    "\n",
    "40\n",
    "00:01:55,107 --> 00:01:56,645\n",
    "以及z3 等等\n",
    "Z3 and so on,\n",
    "\n",
    "41\n",
    "00:01:56,710 --> 00:01:58,570\n",
    "全都在一个步骤中\n",
    "all in one step,\n",
    "\n",
    "42\n",
    "00:01:58,617 --> 00:02:00,734\n",
    "事实上仅用了一行代码\n",
    "in fact, with one line of code.\n",
    "\n",
    "43\n",
    "00:02:00,776 --> 00:02:09,380\n",
    "所以我要先构建一个 1×m 的矩阵\n",
    "So, I'm going to construct a 1 by M matrix\n",
    "\n",
    "44\n",
    "00:02:09,428 --> 00:02:13,096\n",
    "实际上就是一个行向量  然后当我计算z^(1)\n",
    "that's really a row vector while I'm going to compute z^(1),\n",
    "\n",
    "45\n",
    "00:02:13,151 --> 00:02:15,126\n",
    "z^(2) 等等\n",
    "z^(2), and so on,\n",
    "\n",
    "46\n",
    "00:02:15,200 --> 00:02:18,223\n",
    "一直到z^(m) 都是在同一时间内\n",
    "down to ZM, all at the same time.\n",
    "\n",
    "47\n",
    "00:02:18,260 --> 00:02:23,695\n",
    "结果发现它可以表达成\n",
    "It turns out that this can be expressed as W transpose\n",
    "\n",
    "48\n",
    "00:02:23,747 --> 00:02:29,338\n",
    "w的转置 乘以大写矩阵X 加上这个向量b\n",
    "to capital matrix X plus and then this vector B,\n",
    "\n",
    "49\n",
    "00:02:29,354 --> 00:02:31,147\n",
    "b b 等等\n",
    "B and so on.\n",
    "\n",
    "50\n",
    "00:02:31,194 --> 00:02:33,581\n",
    "b 这个东西\n",
    "B, where this thing,\n",
    "\n",
    "51\n",
    "00:02:33,622 --> 00:02:38,965\n",
    "这个 b b b b 这个东西就是一个1×m的向量 或者\n",
    "this B, B, B, B, b thing is a 1xM vector or\n",
    "\n",
    "52\n",
    "00:02:39,034 --> 00:02:45,576\n",
    "1×m 的矩阵 或者说是一个m维的行向量\n",
    "1xM matrix or that is as a M dimensional row vector.\n",
    "\n",
    "53\n",
    "00:02:45,629 --> 00:02:50,031\n",
    "希望你比较熟悉矩阵乘法\n",
    "So hopefully there you are with matrix multiplication.\n",
    "\n",
    "54\n",
    "00:02:50,074 --> 00:03:02,037\n",
    "那么就会发现 W转置·x^(1) x^(2) 等等 一直到x^(m)\n",
    "You might see that W transpose X1, x2 and so on to XM,\n",
    "\n",
    "55\n",
    "00:03:02,401 --> 00:03:05,538\n",
    "这个w转置可以是一个行向量\n",
    "that W transpose can be a row vector.\n",
    "\n",
    "56\n",
    "00:03:05,972 --> 00:03:10,214\n",
    "所以w转置会是一个这样的行向量\n",
    "So this W transpose will be a row vector like that.\n",
    "\n",
    "57\n",
    "00:03:10,261 --> 00:03:17,984\n",
    "所以第一个项求的是w转置乘x^(1)\n",
    "And so this first term will evaluate to W transpose X1,\n",
    "\n",
    "58\n",
    "00:03:18,021 --> 00:03:23,882\n",
    "w转置乘x^(2) 等等 点点点\n",
    "W transpose X2 and so on, dot, dot, dot,\n",
    "\n",
    "59\n",
    "00:03:23,934 --> 00:03:29,675\n",
    "w转置乘x^(m) 然后我们加上第二项b\n",
    "W transpose XM, and then we add this second term B,\n",
    "\n",
    "60\n",
    "00:03:29,727 --> 00:03:31,049\n",
    "b b 等等\n",
    "B, B, and so on,\n",
    "\n",
    "61\n",
    "00:03:31,095 --> 00:03:33,893\n",
    "最后是给每个元素加上b\n",
    "you end up adding B to each element.\n",
    "\n",
    "62\n",
    "00:03:33,926 --> 00:03:37,498\n",
    "最后得到一个1×m向量\n",
    "So you end up with another 1xm vector.\n",
    "\n",
    "63\n",
    "00:03:37,535 --> 00:03:39,160\n",
    "这是第一个元素\n",
    "Well that's the first element,\n",
    "\n",
    "64\n",
    "00:03:39,204 --> 00:03:40,361\n",
    "这是第二个元素等等\n",
    "that's the second element and so on,\n",
    "\n",
    "65\n",
    "00:03:40,408 --> 00:03:42,223\n",
    "这是第n个元素\n",
    "and that's the nth element.\n",
    "\n",
    "66\n",
    "00:03:42,270 --> 00:03:45,271\n",
    "如果你参考上面的定义\n",
    "And if you refer to the definitions above,\n",
    "\n",
    "67\n",
    "00:03:45,341 --> 00:03:50,937\n",
    "第一个元素恰恰是z^(1)的定义\n",
    "this first element is exactly the definition of z^(1).\n",
    "\n",
    "68\n",
    "00:03:50,986 --> 00:03:56,138\n",
    "第二个元素恰恰是z^(2)的定义 等等\n",
    "The second element is exactly the definition of z^(2) and so on.\n",
    "\n",
    "69\n",
    "00:03:57,276 --> 00:04:03,073\n",
    "X是把所有训练样本堆叠起来得到的\n",
    "So just as X was once obtained, when you took your training examples\n",
    "\n",
    "70\n",
    "00:04:03,119 --> 00:04:06,695\n",
    "一个挨着一个 横向堆叠\n",
    "and  stacked them next to each other, stacked them horizontally.\n",
    "\n",
    "71\n",
    "00:04:06,725 --> 00:04:11,675\n",
    "我将会把大写Z定义为这个 在这里\n",
    "I'm going to define capital Z to be this where\n",
    "\n",
    "72\n",
    "00:04:11,724 --> 00:04:15,100\n",
    "你用小写z表示 横向排在一起\n",
    "you take the lowercase Z's and stack them horizontally.\n",
    "\n",
    "73\n",
    "00:04:15,137 --> 00:04:18,614\n",
    "所以当你将对应于不同训练样本的\n",
    "So when you stack the lower case X's\n",
    "\n",
    "74\n",
    "00:04:18,656 --> 00:04:20,682\n",
    "小写x横向堆叠起来时\n",
    "corresponding to a different training examples,\n",
    "\n",
    "75\n",
    "00:04:20,729 --> 00:04:24,234\n",
    "你得到了这个变量 大写X\n",
    "horizontally you get this variable capital X\n",
    "\n",
    "76\n",
    "00:04:24,282 --> 00:04:27,382\n",
    "小写z变量 也是同样的处理\n",
    "and the same way when you take these lowercase Z variables,\n",
    "\n",
    "77\n",
    "00:04:27,424 --> 00:04:28,790\n",
    "把它们横向地堆叠起来\n",
    "and stack them horizontally,\n",
    "\n",
    "78\n",
    "00:04:28,844 --> 00:04:34,044\n",
    "你就得到了这个变量 大写Z\n",
    "you get this variable capital Z.\n",
    "\n",
    "79\n",
    "00:04:34,449 --> 00:04:37,245\n",
    "结果发现 为了计算这个\n",
    "And it turns out, that in order to implement this,\n",
    "\n",
    "80\n",
    "00:04:37,287 --> 00:04:46,124\n",
    "numpy的指令为大写Z = np.dot(w.T ..\n",
    "the numpy command is capital Z equals NP dot w dot T,\n",
    "\n",
    "81\n",
    "00:04:46,171 --> 00:04:50,326\n",
    "那是w的转置 .. ,x) + b\n",
    "that's w transpose X and then plus b.\n",
    "\n",
    "82\n",
    "00:04:50,363 --> 00:04:53,831\n",
    "这里有个Python巧妙的地方\n",
    "Now there is a subtlety in Python,\n",
    "\n",
    "83\n",
    "00:04:53,884 --> 00:04:56,400\n",
    "在这个地方b是一个实数\n",
    "which is at here b is a real number\n",
    "\n",
    "84\n",
    "00:04:56,467 --> 00:04:59,639\n",
    "或者你可以说是1×1的矩阵\n",
    "or if you want to say you know 1x1 matrix,\n",
    "\n",
    "85\n",
    "00:04:59,686 --> 00:05:01,109\n",
    "就是一个普通的实数\n",
    "is just a normal real number.\n",
    "\n",
    "86\n",
    "00:05:01,167 --> 00:05:06,216\n",
    "但是 当你把向量加上这个实数时\n",
    "But, when you add this vector to this real number,\n",
    "\n",
    "87\n",
    "00:05:06,252 --> 00:05:08,972\n",
    "Python会自动的把实数b\n",
    "Python automatically takes this real number B\n",
    "\n",
    "88\n",
    "00:05:09,013 --> 00:05:12,501\n",
    "扩展成一个1×m的行向量\n",
    "and expands it out to this 1XM row vector.\n",
    "\n",
    "89\n",
    "00:05:12,537 --> 00:05:16,160\n",
    "所以这个操作看上去有一点神秘\n",
    "So in case this operation seems a little bit mysterious,\n",
    "\n",
    "90\n",
    "00:05:16,219 --> 00:05:19,881\n",
    "在Python中这叫做广播(broadcasting)\n",
    "this is called broadcasting in Python,\n",
    "\n",
    "91\n",
    "00:05:19,933 --> 00:05:22,398\n",
    "目前你不用对此感到顾虑\n",
    "and you don't have to worry about it for now,\n",
    "\n",
    "92\n",
    "00:05:22,441 --> 00:05:24,758\n",
    "我们会在下一节视频中 更多地谈及它\n",
    "we'll talk about it some more in the next video.\n",
    "\n",
    "93\n",
    "00:05:25,735 --> 00:05:28,200\n",
    "再说回来 只要用一行的代码\n",
    "But the takeaway is that with just one line of code,\n",
    "\n",
    "94\n",
    "00:05:28,274 --> 00:05:29,333\n",
    "运用这行代码\n",
    "with this line of code,\n",
    "\n",
    "95\n",
    "00:05:29,376 --> 00:05:31,289\n",
    "你可以计算大写Z\n",
    "you can calculate capital Z\n",
    "\n",
    "96\n",
    "00:05:31,346 --> 00:05:35,160\n",
    "而大写Z是一个1×m的矩阵\n",
    "and capital Z is going to be a 1XM matrix\n",
    "\n",
    "97\n",
    "00:05:35,227 --> 00:05:37,702\n",
    "包含所有的小写z\n",
    "that contains all of the lower cases Z's.\n",
    "\n",
    "98\n",
    "00:05:37,749 --> 00:05:40,594\n",
    "小写z^(1)一直到小写z^(m)\n",
    "Lowercase z^(1) through lower case ZM.\n",
    "\n",
    "99\n",
    "00:05:40,687 --> 00:05:46,297\n",
    "这就是Z 那么变量a是怎样的呢\n",
    "So that was Z, how about these values a.\n",
    "\n",
    "我们接下去要做的\n",
    "What we like to do next,\n",
    "\n",
    "是找到一个办法来计算a^(1)\n",
    "is find a way to compute A1,\n",
    "\n",
    "a^(2)等等一直到a^(m)\n",
    "A2 and so on to AM,\n",
    "\n",
    "都在同一时间完成\n",
    "all at the same time,\n",
    "\n",
    "就像把小x堆叠起来形成X一样\n",
    "and just as stacking lowercase X's resulted in capital X\n",
    "\n",
    "将小z横向堆叠成大Z\n",
    "and stacking horizontally lowercase Z's resulted in capital Z,\n",
    "\n",
    "堆叠小写a 就会形成一个新的变量\n",
    "stacking lower case A, is going to result in a new variable,\n",
    "\n",
    "我们把它定义为大写A\n",
    "which we are going to define as capital A.\n",
    "\n",
    "在编程作业中\n",
    "And in the program assignment,\n",
    "\n",
    "你能看到如何对一个向量进行sigmoid函数操作\n",
    "you see how to implement a vector valued sigmoid function,\n",
    "\n",
    "所以sigmoid函数\n",
    "so that the sigmoid function,\n",
    "\n",
    "把大写Z当做一个变量进行输入\n",
    "inputs this capital Z as a variable\n",
    "\n",
    "然后非常高效的输出大写A\n",
    "and very efficiently outputs capital A.\n",
    "\n",
    "你仔细看看编程作业里的细节\n",
    "So you see the details of that in the programming assignment.\n",
    "\n",
    "总的来说\n",
    "So just to recap,\n",
    "\n",
    "我们在这张幻灯片所看到的是 不需要for循环\n",
    "what we've seen on this slide is that instead of needing to loop over\n",
    "\n",
    "就可以从m个训练样本 一次性计算出小写z和小写a\n",
    "M training examples to compute lowercase z and lowercase a,\n",
    "\n",
    "而你运行这些只需要一行代码\n",
    "one of the time, you can implement this one line of code,\n",
    "\n",
    "在同一时间计算所有的z\n",
    "to compute all these Z's at the same time.\n",
    "\n",
    "这一行的代码\n",
    "And then, this one line of code,\n",
    "\n",
    "实现的是\n",
    "with appropriate implementation of\n",
    "\n",
    "用小写sigma同时计算所有小写a\n",
    "lowercase Sigma to compute all the lowercase A's all at the same time.\n",
    "\n",
    "所以这就是\n",
    "So this is how you implement\n",
    "\n",
    "正向传播一步迭代的向量化实现\n",
    "a vectorize implementation of the forward propagation\n",
    "\n",
    "同时处理所有m个训练样本\n",
    "for all M training examples at the same time.\n",
    "\n",
    "概括一下 你刚刚看到如何使用向量化\n",
    "So to summarize, you've just seen how you can use vectorization\n",
    "\n",
    "高效计算激活函数\n",
    "to very efficiently compute all of the activations,\n",
    "\n",
    "同时输出所有小a\n",
    "all the lowercase a's at the same time.\n",
    "\n",
    "接下来 你会发现同样可以用向量化\n",
    "Next, it turns out, you can also use vectorization very efficiently\n",
    "\n",
    "来高效地计算反向传播\n",
    "to compute the backward propagation,\n",
    "\n",
    "并以此来计算梯度\n",
    "to compute the gradients.\n",
    "\n",
    "我们在下一次视频中 将看到它如何实现\n",
    "Let's see how you can do that, in the next video.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### <font color=#0099ff>重点总结：</font>\n",
    "\n",
    "\n",
    "参考文献：\n",
    "\n",
    "[1]. 大树先生.[吴恩达Coursera深度学习课程 DeepLearning.ai 提炼笔记（1-2）– 神经网络基础](http://blog.csdn.net/junjun_zhao/article/details/78855589)\n",
    "\n",
    "\n",
    "---\n",
    " \n",
    "**PS: 欢迎扫码关注公众号：「SelfImprovementLab」！专注「深度学习」，「机器学习」，「人工智能」。以及 「早起」，「阅读」，「运动」，「英语 」「其他」不定期建群 打卡互助活动。**\n",
    "\n",
    "<center><img src=\"http://upload-images.jianshu.io/upload_images/1157146-cab5ba89dfeeec4b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\"></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
