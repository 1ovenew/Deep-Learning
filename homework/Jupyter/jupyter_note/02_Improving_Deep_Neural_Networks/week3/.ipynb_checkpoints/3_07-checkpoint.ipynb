{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch norm processes your data one mini batch at a time,but the test time you might need to process the examples one at a time.Let's see how you can adapt your network to do that.Recall that during training,here are the equations you'd use to implement batch norm.Within a single mini batch,you'd sum over that mini batch of the ZI values to compute the mean.So here, you're just summing over the examples in one mini batch.I'm using M to denote the number of examples in the mini batchnot in the whole training set.Then, you compute the variance and then you compute Z norm byscaling by the mean and standard deviation with Epsilon added for numerical stability.And then Z tilde is taking Z norm and rescaling by gamma and beta.So, notice that mu and sigma squared which you need for this scaling calculationare computed on the entire mini batch.But the test time you might not have a mini batch of6428 or 2056 examples to process at the same time.So, you need some different way of coming up with mu and sigma squared.And if you have just one example,taking the mean and variance of that one example, doesn't make sense.So what's actually done in order to apply your neural network at test timeis to come up with some separate estimate of mu and sigma squared.And in typical implementations of batch norm,what you do is estimate this using an exponentially weighted averagewhere the average is across the mini batches.So, to be very concrete here's what I mean.Let's pick some layer L and let's say you're going through mini batches X1, X2together with the corresponding values of Y and so on.So, when training on X1 for that layer L, you get some mu L.And in fact, I'm going to write this as mu for the first mini batch and that layer.And then when you train on the second mini batch for that layer and that mini batch,you end up with some second value of mu.And then for the third mini batch in this hidden layer,you end up with some third value for mu.So just as we saw how to use an exponentially weighted averageto compute the mean of Theta one, Theta two, Theta threewhen you were trying to compute an exponentially weighted average of the current temperature,you would do that to keep track of what'sthe latest average value of this mean vector you've seen.So that exponentially weighted average becomesyour estimate for what the mean of the Zs is for that hidden layer and similarly,you use an exponentially weighted average to keep track ofthese values of sigma squared that you see on the first mini batch in that layer,sigma square that you see on second mini batch and so on.So you keep a running average of the mu and the sigma squared that you'reseeing for each layer as you train the neural network across different mini batches.Then finally at test time,what you do is in place of this equation,you would just compute Z norm using whatever value your Z have,and using your exponentially weighted average of the mu and sigma squarewhatever was the latest value you have to do the scaling here.And then you would compute Z tilde on your one test exampleusing that Z norm that we just computed on the left  and using the beta andgamma parameters that you have learned during your neural network training process.So the takeaway from this is that during training timemu and sigma squared are computed on an entire mini batch of,say, 64, 28 or some number of examples.But at test time, you might need to process a single example at a time.So, the way to do that is to estimate mu and sigma squared from your training setand there are many ways to do that.You could in theory run your whole training set through your final networkto get mu and sigma squared.But in practice, what people usually do is implement an exponentially weighted averagewhere you just keep track of the mu and sigma squared values you're seeing during trainingand use an exponentially weighted average, also sometimes called the running average,to just get a rough estimate of mu and sigma squaredand then you use those values of mu and sigma squared at test timeto do the scaling you need of the hidden unit values Z.In practice, this process is pretty robustto the exact way you used to estimate mu and sigma squared.So, I wouldn't worry too much about exactly how you do thisand if you're using a deep learning framework,they'll usually have some default way to estimate the mu and sigma squaredthat should work reasonably well as well.But in practice, any reasonable way to estimate the mean andvariance of your hidden unit values Z should work fine at test.So, that's it for batch norm.And using it, I think you'll be able to train much deeper networksand get your learning algorithm to run much more quickly.Before we wrap up for this week,I want to share with you some thoughts on deep learning frameworks as well.Let's start to talk about that in the next video.\n",
    "\n",
    "\n",
    "Batch归一化将你的数据以mini-batch的形式逐一处理(字幕来源：网易云课堂)，但在测试时  你可能需要对每一个样本逐一处理，我们来看一下怎样调整你的网络来做到这一点，回想一下  在训练时，这些就是用来执行Batch归一化的等式，在一个mini-batch中，你将mini-batch的z(i)值求和  计算均值，所以这里你只要把一个mini-batch中的样本都加起来，我用m来表示这个mini-batch中的样本数量，而不是整个训练集，然后计算方差  再算z norm，即用均值和标准差来调整  加上ε是为了数值稳定性，z̃是用γ和β再次调整z norm，请注意用于调节计算的μ和σ²，是在整个mini-batch上进行计算，但是在测试时  你可能不能将一个mini-batch中的，6428个或2056个样本同时处理，因此你需要用其他方式来得到μ和σ²，而且如果你只有一个样本，一个样本的均值和方差没有意义，那么实际上  为了将你的神经网络运用于测试，就需要单独估算μ和σ²，在典型的Batch归一化运用中，你需要用一个指数加权平均来估算，这个平均数涵盖了所有mini-batch，接下来我会具体解释，我们选择L层  假设我们有mini-batch X^[1]  X^[2]，以及对应的y值等等，那么在为L层训练X^[1]时  你就得到了μ^[L]，我还是把它写做第一个mini-batch和这一层的μ吧，当你训练第二个mini-batch  在这一层和这个mini-batch中，你就会得到第二个μ值，然后在这一隐藏层的第三个mini-batch，你得到了第三个μ值，正如我们之前用指数加权平均，来计算θ_1  θ_2  θ_3的均值，当时是试着计算当前气温的指数加权平均，你会这样来追踪，你看到的这个均值向量的最新平均值，于是这个指数加权平均就成了，你对这一隐藏层的z均值的估值  同样的，你也可以用指数加权平均来追踪，你在这一层的第一个mini-batch中所见的σ²的值，以及第二个mini-batch中所见的σ²的值等等，因此在用不同的mini-batch训练神经网络的同时，能够得到你所查看的每一层的μ和σ²的平均数的实时数值，最后在测试时，对应这个等式，你只需要用你的z值来计算z norm，用μ和σ²的指数加权平均，用你手头的最新数值来做调整，然后你可以用左边我们刚算出来的z norm，和你在神经网络训练过程中得到的β和γ参数，来计算你那个测试样本的z̃值，总结一下就是  在训练时，μ和σ²是在整个mini-batch上计算出来的，包含了像是64或28或其他一定数量的样本，但在测试时  你可能需要逐一处理样本，方法是根据你的训练集估算μ和σ²，估算的方式有很多种，理论上你可以在最终的网络中运行整个训练集，来得到μ和σ²，但在实际操作中  我们通常运用指数加权平均，来追踪在训练过程中你看到的μ和σ²的值，还可以用指数加权平均  有时也叫做流动平均，来粗略估算μ和σ²，然后用测试中μ和σ²的值，来进行你所需的隐藏单元z值的调整，在实践中  不管你用什么方式估算μ和σ²，这套过程都是比较稳健的，因此我不会太担心你具体的操作方式，而且如果你使用的是某种深度学习框架，通常会有默认的估算μ和σ²的方式，应该一样会起到比较好的效果，但在实践中  任何合理的估算你的隐藏单元z值的，均值和方差的方式  在测试中应该都会有效，Batch归一化就讲到这里，使用Batch归一化  你能够训练更深的网络，让你的学习算法运行速度更快，在结束这周的课程前，我还想和你们分享一些关于深度学习框架的想法，让我们在下一段视频中一起讨论这个话题，"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
