{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, the classification examples we've talked about have usedbinary classification, where you had two possible labels, 0 or 1.Is it a cat, is it not a cat?What if we have multiple possible classes?There's a generalization of logistic regression called Softmax regressionthat lets you make predictions where you're trying to recognize one of C orone of multiple classes, rather than just recognize two classes.Let's take a look.Let's say that instead of just recognizing catsyou want to recognize cats, dogs, and baby chicks.So I'm going to call cats class 1, dogs class 2, baby chicks class 3.And if none of the above, then there's an other ora none of the above class, which I'm going to call class 0.So here's an example of the images and the classes they belong to.That's a picture of a baby chick, so the class is 3.Cats is class 1, dog is class 2,I guess that's a koala, so that's none of the above,so that is class 0. Class 3 and so on.So the notation we're going to use is, I'm going to use capital C todenote the number of classes you're trying to categorize your inputs into.And in this case, you have four possible classes,including the other or the none of the above class.So when you have four classes, the numbers indexing your classeswould be 0 through capital C minus one.So in other words, that would be zero, one, two or three.In this case, we're going to build a neural network  where the output layer has four,or in this case the variable capital alphabet C output units.So n, the number of units output layer which is layer Lis going to equal to 4 or in general this is going to equal to C.And what we want is for the number of units in the output layer to tell uswhat is the probability of each of these four classes.So the first node here is supposed to output,or we want it to output the probability that is the other class, given the input x,This will output probability there's a cat, given an x.This will output probability that there's a dog, given an x.That will output the probability...I'm just going to abbreviate baby chick to bcSo probability of a baby chick, abbreviated to bc, given the input x.So here, the output labels y hat is going to be a four by one dimensional vector,because it now has to output four numbers, giving you these four probabilities.And because probably it should sum to one,the four numbers in the output y hat, they should sum to one.The standard model for getting your network to do this uses what's calleda Softmax layer, and the output layer in order to generate these outputs.Let me write down the math,then you can come back and get some intuition about what the Softmax there is doing.So in the final layer of the neural network,you are going to compute as usual the linear part of the layers.So z, capital L, that's the z variable for the final layer.So remember this is layer capital L.So as usual you compute that as w^[L] times the activation of the previous layerplus the biases for that final layer.Now having computer z,you now need to apply what's called the Softmax activation function.So that activation function is a bit unusual for the Softmax layer,but this is what it does.First, we're going to computes a temporary variable,which we're going to call t, which is e to the z^[L].So this is applied element-wise.So z^[L] here, in our example, z^[L] is going to be four by oneThis is a four dimensional vector.So t itself e to the zL, that's an element-wise exponentiation.T will also be a 4 by 1 dimensional vector.Then the output a^[L],is going to be basically the vector t but normalized to sum to 1.So aL is going to be e to the z^[L]divided by sum from J equal 1 through 4,because we have four classes of t subscript i.So in other words of saying that a^[L] is also a four by one vector,and the i element of this four dimensional vector,let's write that, a^[L] subscript i that's going to be equal to t_i over sum of t_i, okay?In case this math isn't clear,we'll do an example in a minute that will make this clearer.So in case this math isn't clear,let's go through a specific example that will make this clearer.Let's say that you compute z^[L], andz^[L] is a four dimensional vector, let's say is 5, 2, -1, 3.What we're going to do is use this element-wise exponentiation tocompute this vector t.So t is going to be e to the 5, e to the 2, e to the -1, e to the 3.And if you plug that in the calculator, these are the values you get.E to the 5 is 148.4, e squared is about 7.4,to the -1 is 0.4, and e cubed is 20.1.And so, the way we go from the vector t to the vector a^[L] is justto normalize these entries to sum to one.So if you sum up the elements of t,if you just add up those 4 numbers you get 176.3.So finally, a^[L] is just going to be this vector t,as a vector, divided by 176.3.So for example, this first node here,this will output e to the 5 divided by 176.3.And that turns out to be 0.842.So saying that, for this image, if this is the value of z you get,the chance of it being class zero is 84.2%.And then the next nodes outputs e squared over 176.3,that turns out to be 0.042, so this is 4.2% chance.The next one is e to -1 over that, which is 0.002.And the final one is e cubed over that, which is 0.114.So it is 11.4% chance that this is class number three,which is the baby chick class, right?So there's a chance of it being class zero, class one, class two, class three.So the output of the neural network a^[L], this is also y hat,this is a 4 by 1 vectorwhere the elements of this 4 by 1 vector are going to be these four numbers that we just compute it.So this algorithm takes the vector z^[L] and mathes it to four probabilities that sum to 1.And if we summarize what we just did to math from z^[L] to a^[L],this whole computation--computing exponentiation toget this temporary variable t and then normalizing,we can summarize this into a Softmax activation function andsay a^[L] equals the activation function g applied to the vector z^[L].The unusual thing about this particular activation function is that,this activation function g, it takes us input a 4 by 1 vector andit outputs a 4 by 1 vector.So previously, our activation functions used to take in a single row value input.So for example, the Sigmoid and the ReLU activation functionsinput a real number and output a real number.The unusual thing about the Softmax activation function is,because it needs to normalize across the different possible outputs,it needs to take in a vector of input and then outputs a vector.So what other things that a Softmax classifier can represent?I'm going to show you some examples where you have inputs x_1, x_2.And these feed directly to a Softmax layerthat has three or four, or more output nodes that then outputs y hat.So I'm going to show you a neural network with no hidden layer,and all it does is compute z^[1] equals w^[1] times the input x plus b.And then the output a^[1], or y hatis just the Softmax activation function applied to z^[1].So in this neural network with no hidden layers,it should give you a sense of the types of things a Softmax function can represent.So here's one example with just raw inputs x_1 and x_2.A Softmax layer with C equals 3 output classes can representthis type of decision boundaries.Notice this is kind of several linear decision boundaries,but this allows it to separate out the data into three classes.And in this diagram, what we did waswe actually took the training set that's kind of shown in this figure andtrain the Softmax classifier with three output labels on the dataAnd then the color on this plot shows threshold in the outputs in the Softmax classifierand coloring in the input base on which one of the three outputs have the highest probability.So we can maybe kind of see that this is like a generalization of logistic regressionwith sort of linear decision boundaries, but with more than two classesinstead of class being just 0, 1, the class could be 0, 1, or 2.Here's another example of the decision boundary that the Softmax classifier can representwhen training on a data set with three classes.And here's another one.Right, so this is a... but one intuition is thatthe decision boundary between any two classes will be linear.That's why you see for examplethat decision boundary between the yellow and the red classes, that's the linear boundarybetween purple and red is another linear boundary,between the purple and yellow is another linear decision boundary.But it was able to use these different linear functionsin order to separate the space into three classes.Let's look at some examples with more classes.So it's an example with C equals 4so that the green class and Softmax can continue to representthese types of linear decision boundaries between multiple classes.So here's one more example with C equals 5 classes,and here's one last example with C equals 6.So this shows the type of things the Softmax classifier can dowhen there is no hidden layerOf course, even much deeper neural network with x andthen some hidden units, and then more hidden units, and so on,then you can learn even more complex non-linear decision boundariesto separate out multiple different classes.So I hope this gives you a sense of what a Softmax layer orthe Softmax activation function in the neural network can do.In the next video, let's take a look at how you can traina neural network that uses a Softmax layer.\n",
    "\n",
    "到目前为止 我们讲到过的分类的例子都使用了(字幕来源：网易云课堂)，二分分类  这种分类只有两种可能的标记 0或1，这是一只猫或者不是一只猫，如果我们有多种可能的类型的话呢？，有一种logistic回归的一般形式 叫做Softmax回归，能让你在试图识别某一分类时做出预测，或者说是多种分类中的一个  不只是识别两个分类，我们来一起看一下，假设你不单需要识别猫，而是想要识别猫  狗和小鸡，我把猫叫做类1  狗为类2  小鸡是类3，如果不属于以上任何一类  就分到\"其他\"或者说，\"以上均不符合\"这一类  我把它叫做类0，这里显示的图片及其对应的分类就是一个例子，这幅图片上是一只小鸡  所以是类3，猫是类1  狗是类2，我猜这是一只考拉  所以以上均不符合，那就是类0  下一个类3  以此类推，我们将会用符号表示  我会用大写C，来表示你的输入会被分入的类别总个数，在这个例子中  我们有4种可能的类别，包括\"其他\"或\"以上均不符合\"这一类，当有4个分类时  指示类别的数字，就是从0到大写C减1，换句话说  就是0  1  2  3，在这个例子中  我们将建立一个神经网络  其输出层有4个，或者说变量大写字母C个输出单元，因此n  即输出层也就是L层的单元数量，等于4  或者一般而言等于C，我们想要输出层单元的数字告诉我们，这4种类型中每一个的概率有多大，所以这里的第一个节点输出的应该是，或者说我们希望它输出\"其他\"类的概率  在输入x的情况下，这个会输出是猫的概率  在输入x的情况下，这个会输出是狗的概率  在输入x的情况下，这个会输出...，我把小鸡缩写为bc，小鸡即bc的概率  在输入x的情况下，因此这里的ŷ将是一个4*1维向量，因为它必须输出四个数字  给你这四种概率，因为它加起来应该等于1，输出的ŷ中的四个数字加起来应该等于1，让你的网络做到这一点的标准模型要用到，Softmax层  以及输出层来生成输出，让我把式子写下来，然后回过头来  就会对Softmax的作用有一点感觉了，在神经网络的最后一层，你将会像往常一样计算各层的线性部分，z 大写L  这是最后一层的z变量，记住这是大写L层，和往常一样  计算方法是w^[L]乘以上一层的激活值，再加上最后一层的偏差，算出了z之后，你需要应用Softmax激活函数，这个激活函数对于Softmax层而言有些不同，它的作用是这样的，首先我们要计算一个临时变量，我们把它叫做t  它等于e的z^[L]次方，这适用于每个元素，而这里的z^[L]  在我们的例子中  z^[L]是4*1的，四维向量，t就是e的z^[L]次方  这是对所有元素求幂，t也是一个4*1维向量，然后输出的a^[L]，基本上就是向量t  但是会归一化  使和为1，因此a^[L]等于e的z^[L]次方，除以j等于1到4时的总和，因为我们有4个类型的t下标i，换句话说  a^[L]也是一个4*1维向量，而这个四维向量的i元素，我把它写下来  a^[L]下标i等于t_i除以t_i之和，以防这里的计算不够清晰易懂，我们马上会举个例子来详细解释，以防计算不够清晰，我们来看一个例子  详细解释，假设你算出了z^[L]，z^[L]是一个四维向量  假设为5  2  -1  3，我们要做的就是用这个元素取幂方法，来计算向量t，因此t就是e^5 e^2 e^(-1) e^3，如果你按一下计算器就会得到以下值，e^5是148.4  e^2约为7.4，e^(-1)是0.4  e^3是20.1，我们从向量t得到向量a^[L]就只需，将这些项目归一化使总和为1，如果你把t的元素都加起来，把这四个数字加起来  得到176.3，最终a^[L]就等于向量t，作为一个向量  除以176.3，例如这里的第一个节点，它会输出e^5除以176.3，也就是0.842，这样说来  对于这张图片  如果这是你得到的z值，它是类0的概率就是84.2%，下一个节点输出e^2除以176.3，等于0.042  也就是4.2%的几率，下一个是e^(-1)除以它  即0.002，最后一个是e^3除以它  等于0.114，也就是11.4%的概率属于类3，也就是小鸡组  对吧，这就是它属于类0  类1  类2  类3的可能性，神经网络的输出a^[L]  也就是ŷ，是一个4乘1维向量，这个4乘1向量的元素就是我们算出来的这四个数字，这种算法通过向量z^[L]计算出总和为1的四个概率，如果我们总结一下从z^[L]到a^[L]的计算步骤，整个计算过程  从计算幂，到得出临时变量t  再归一化，我们可以将此概括为一个Softmax激活函数，设a^[L]等于向量z^[L]的激活函数g，这一激活函数的与众不同之处在于，这个激活函数g需要输入一个4*1维向量，然后输出一个4*1维向量，之前  我们的激活函数都是接受单行数值输入，例如Sigmoid和ReLU激活函数，输入一个实数  输出一个实数，Softmax激活函数的特殊之处在于，因为需要将所有可能的输出归一化，就需要输入一个向量  最后输出一个向量，那么Softmax分类器还可以代表其他的什么东西呢？，我来举几个例子  你有两个输入x_1  x_2，它们直接输入到Softmax层，它有三四个或者更多的输出节点  输出ŷ，我将向你展示一个没有隐藏层的神经网络，它所做的就是计算z^[1]=w^[1]*x（输入）+b，而输出的a^[1]或者说ŷ，就是z^[1]的Softmax激活函数，这个没有隐藏层的神经网络，应该能让你对Softmax函数能够代表的东西有所了解，这个例子中  原始输入只有x_1和x_2，一个C等于3个输出分类的Softmax层能够代表，这种类型的决策边界，请注意这是几条线性决策边界，但这使得它能够将数据分到3个类别中，在这张图表中  我们所做的是，选择这张图中显示的训练集，用数据的3种输出标签来训练Softmax分类器，图中的颜色显示了Softmax分类器的输出的阈值，输入的着色是基于三种输出中概率最高的那种，因此我们可以看到这是logistic回归的一般形式，有类似线性的决策边界  但有超过两个分类，分类不只有0和1  而是可以是0  1或2，这是另一个Softmax分类器可以代表的决策边界的例子，用有三个分类的数据集来训练，这里还有一个，对吧  但是直觉告诉我们，任何两个分类之间的决策边界都是线性的，这就是为什么你看到比如这里，黄色和红色分类之间的决策边界是线性边界，紫色和红色之间的也是线性边界，紫色和黄色之间的也是线性决策边界，但它能用这些不同的线性函数，来把空间分成三类，我们来看一下更多分类的例子，这个例子中C等于4，因此这个绿色分类和Softmax仍旧可以代表，多种分类之间的这些类型的线性决策边界，另一个例子是C等于5类，最后一个例子是C等于6，这显示了Softmax分类器在没有隐藏层的情况下，能够做到的事情，当然  更深的神经网络会有x，然后是一些隐藏单元  以及更多隐藏单元等等，你就可以学习更复杂的非线性决策边界，来区分多种不同分类，我希望你了解了神经网络中的Softmax层，或者Softmax激活函数有什么作用，下一个视频中  我们来看一下你该怎样训练，一个使用Softmax层的神经网络，"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
