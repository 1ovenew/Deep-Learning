{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursera | Andrew Ng (02-week3-3.11)—TensorFlow\n",
    "\n",
    "该系列仅在原课程基础上部分知识点添加个人学习笔记，或相关推导补充等。如有错误，还请批评指教。在学习了 Andrew Ng 课程的基础上，为了更方便的查阅复习，将其整理成文字。因本人一直在学习英语，所以该系列以英文为主，同时也建议读者以英文为主，中文辅助，以便后期进阶时，为学习相关领域的学术论文做铺垫。- ZJ\n",
    "    \n",
    ">[Coursera 课程](https://www.coursera.org/specializations/deep-learning) |[deeplearning.ai](https://www.deeplearning.ai/) |[网易云课堂](https://mooc.study.163.com/smartSpec/detail/1001319001.htm)\n",
    "\n",
    "---\n",
    "   **转载请注明作者和出处：ZJ 微信公众号-「SelfImprovementLab」**\n",
    "   \n",
    "   [知乎](https://zhuanlan.zhihu.com/c_147249273)：https://zhuanlan.zhihu.com/c_147249273\n",
    "   \n",
    "   [CSDN]()：\n",
    "   \n",
    "\n",
    "---\n",
    "\n",
    "3.11 TensorFlow\n",
    "\n",
    "Welcome to the last video for this week.There are many great, deep learning programming frameworks.One of them is TensorFlow.I'm excited to help you start to learn to use TensorFlow.What I want to do in this video is show you the basic structureof a TensorFlow program, and then leave you to practice, learn more details, andpractice them yourself in this week's programming exercise.This week's programming exercise will take some time to do soplease be sure to leave some extra time to do it.As a motivating problem,let's say that you have some cost function J that you want to minimize.And for this example, \n",
    "\n",
    "欢迎来到这周的最后一个视频(字幕来源：网易云课堂)，有很多很棒的深度学习编程框架，其中一个是TensorFlow，我很期待帮助你开始学习使用TensorFlow，我想在这个视频中向你展示TensorFlow程序的，基本结构  然后让你自己练习  学习更多细节，并运用到本周的编程练习中，这周的编程练习需要花些时间来做，所以请务必留出一些空余时间，先提一个启发性的问题，假设你有一个损失函数J需要最小化，在本例中  我将使用这个高度简化的损失函数，J(w) = w^2- 10w + 25，\n",
    "\n",
    "I'm going to use this highly simple cost functionJ(w) = w squared- 10w + 25.So that's the cost function.You might notice that this function is actually (w- 5) squared.If you expand out this quadratic,you get the expression above,and so the value of w that minimizes this is w = 5.But let's say we didn't know that, and you just have this function.Let's see how you can implement something in TensorFlow to minimize this.Because a very similar structure of program can be used totrain neural networkswhere you can have some complicated cost function J(w, b)depending on all the parameters of your neural network.\n",
    "\n",
    "\n",
    "这就是损失函数，也许你已注意到该函数其实就是w减5的平方，如果你把这个二次方的式子展开，就得到了上面的表达式，所以使它最小的w值是w等于5，但是假设我们不知道这点  你只有这个函数，我们来看一下怎样用TensorFlow将其最小化，因为一个非常类似的程序结构可以用来，训练神经网络，其中可以有一些复杂的损失函数J(w, b)，取决于你的神经网络的所有参数，然后类似地  你就能用TensorFlow，自动找到使损失函数最小的w和b的值，但是让我们先从左边这个更简单的例子入手，我在我的Jupyter笔记本中运行Python，为了启动TensorFlow  首先import numpy as np，而且我们习惯用import tensorflow as tf，\n",
    "\n",
    "\n",
    "And then, similarly, you'll be able to use TensorFlow toautomatically try to find values of w and b that minimize this cost function.But let's start with the simpler example on the left.So, I'm running Python in my Jupyter Notebookand to start up TensorFlow, you import numpy as npand it's idiomatic to use import tensorflow as tf.Next, let me define the parameter w.So in TensorFlow, you're going to use tf.Variable to define a parameter.dtype=tf.float32.And then let's define the cost function.So remember the cost function was w squared- 10w + 25.So let me use tf.add.So I'm going to have w squared + tf.multiply.So the second term was -10.w.And then I'm going to add that to 25.So let me put another tf.add over there.So that defines the cost J that we had.And then, I'm going to write train = tf.train.GradientDescentOptimizer.\n",
    "\n",
    "接下来  让我定义参数w，在TensorFlow中  你要用tf.Variable来定义参数，dtype=tf.float32，然后我们来定义损失函数，记得损失函数是w^2-10w+25，让我用tf.add，我们有w平方加tf.multiply，第二项是-10乘以w，然后我再加上25，让我在这里再加一个tf.add，这样就定义了我们的损失J，然后再写train = tf.train.GradientDescentOptimizer，让我们用0.01的学习率  目标是最小化损失，最后  下面的几行是惯用表达，init = tf.global_variables_initializer，然后session = tf.Sessions，这样就开启了一个TensorFlow session，session.run(init)来初始化全局变量，然后要让TensorFlow评估一个变量，我们要用到sess.run(w)，我们还什么都没有做，上面的这一行将w初始化为0，并定义损失函数，我们定义train为学习算法，\n",
    "\n",
    "Let's use a learning rate of 0.01 and the goal is to minimize the cost.And finally, the following few lines are quite idiomatic.init = tf.global_variables_initializerand then session = tf.Sessions.So it starts a TensorFlow session.Session.run init to initialize global variables.And then, for TensorFlow's to evaluate a variable,we're going to use sess.run w.We haven't done anything yet.So with this line above, initialize w to zeroand define a cost function.We define train to be our learning algorithmwhich uses a GradientDescentOptimizer to minimize the cost function.But we haven't actually run the learning algorithm yet, sosession.run, we evaluate w,and let me print session.run(w).So if we run that, it evaluates w to be equal to 0because we haven't run anything yet.Now, let's do session.run train.So what this will do is run one step of GradientDescent.And then let's evaluate the value of w after onestep of GradientDescent and print that.\n",
    "\n",
    "\n",
    "\n",
    "它用梯度下降法优化器使损失函数最小化，但实际上我们还没有运行学习算法，session.run我们评估了w，让我print session.run(w)，所以如果我们运行这个  它评估w等于0，因为我们什么都还没运行，现在让我们输入session.run(train)，它所做的就是运行一步梯度下降法，接下来在运行了一步梯度下降法后，让我们评估一下w的值  再print，在一步梯度下降法之后  w现在是0.1，现在我们运行梯度下降1000次迭代  .run(train)，然后print(session.run(w))，它运行了梯度下降的1000次迭代，最后w变成了4.99999，记不记得我们说要使(w - 5)^2最小化，因此w的最优值是5  这个结果已经很接近了，希望这让你对TensorFlow程序的大致结构有了了解，当你做编程练习  使用更多TensorFlow代码时，我这里用到的一些函数你会熟悉起来，这里有地方要注意  w是我们想要优化的参数，因此将它称为变量，注意我们需要做的就是定义一个损失函数，使用这些add和multiply之类的函数，TensorFlow知道如何对add和mutiply，还有其他函数求导，这就是为什么你只需基本实现前向传播，它能弄明白如何做反向传播和梯度计算，因为它已经内置在add  multiply和平方函数中，\n",
    "\n",
    "So we do that after the one step of GradientDescent, w is now 0.1.Let's now run 1000 iterations of GradientDescent so .run(train).And lets then print(session.run w).So this would run a 1,000 iterations of GradientDescent,and at the end w ends up being 4.99999.Remember, we said that we're minimizing w- 5 squaredso the optimum value of w is 5 and it got very close to this.So hope this gives you a sense of the broad structure of a TensorFlow program.And as you do the programming exercise  and play with more TensorFlow codes yourself,some of these functions that I'm using here will become more familiar.Some things to notice about this, w is the parameter we are trying to optimizeso we're going to declare that as a variable.And notice that all we had to do was define a cost functionusing these add and multiply and so on functions.And TensorFlow knows automatically how to take derivativeswith respect to the add and multiply as well as other functions.Which is why you only had to implement basically forward prop andit can figure out how to do the back prop or the gradient computation.Because that's already built in to the add and multiply as well as the squaring functions.By the way, in case this notation seems really ugly,TensorFlow actually has overloaded the computation forthe usual plus, minus, and so on.\n",
    "\n",
    "\n",
    "\n",
    "对了  要是觉得这种写法不好看的话，TensorFlow其实还重载了，一般的加减运算等等，因此你也可以把cost写成更好看的形式，把这个标成注释  重新运行  得到了同样的结果，一旦w被称为TensorFlow变量，平方  乘法和加减运算都重载了，因此你不必使用上面这种不好看的句法，TensorFlow还有一个特点我想告诉你，那就是这个例子将w的一个固定函数最小化了，如果你想要最小化的函数是训练集函数又如何呢？，不管你有什么训练数据x，当你训练神经网络时  训练数据x会改变，那么如何把训练数据加入TensorFlow程序呢？，我会定义t和x，把它想做扮演训练数据的角色，事实上训练数据有x和y  但这个例子中只有x，把x定义为placeholder，类型是float32，让它成为[3, 1]数组，\n",
    "\n",
    "\n",
    "So you could also just write this nicer format for the cost andcomment that out and rerun this and get the same result.So once w is declared to be a TensorFlow variable,the squaring, multiplication, adding, and subtraction operations are overloaded.So you don't need to use this uglier syntax that I had above.Now, there's just one more feature of TensorFlow that I want to show you,which is this example minimize a fix function of w.What if the function you want to minimize is the function of your training set?So whatever you have some training data, x andwhen you're training a neural network the training data x can change.So how do you get training data into a TensorFlow program?So I'm going to define t and xwhich is think of this as playing a role of training dataor really the training data with both x and y, but we only have x in this example.So this is going to define x with placeholderand it's going to be a type float32and let's make this a [3,1] array.And what I'm going to do iswhereas the cost here have fixed coefficients in front of the three terms in this quadraticwas 1 times w squared- 10*w + 25.\n",
    "\n",
    "我要做的就是，因为cost这个二次方程的三个项前有固定的系数，它是1*w^2-10*w+25，我们可以把这些数字1  -10和25变成数据，我要做的就是把cost，替换成cost = x[0][0]*w*2 + x[1][0]*w + x[2][0]，嗯  再乘1，现在x变成了控制，这个二次函数系数的数据，这个placeholder函数告诉TensorFlow，你稍后会为x提供数值，让我们再定义一个数组  coefficient = np.array，[1.], [-10.]还有最后一个值是[25.]，这就是我们要接入x的数据，最后我们需要用某种方式把这个系数数组接入变量x，做到这一点的句法是  在训练这一步中，要提供给x的数值，我在这里设置feed_dict = x:coefficients，然后改一下这个  我要复制粘贴到这里，好了  希望没有句法错误，我们重新运行它  希望得到和之前一样的结果，现在如果你想改变这个二次函数的系数，假设你把 [-10.] 变成[-20]，让我们把它改成100，现在这个函数就变成了(x-10)^2，如果我重新运行，希望我得到的使(x-10)^2最小化的w值是10，让我们看一下  很好，在梯度下降1000次迭代之后  我们得到接近10的w，在你做编程练习时见到更多的是，TensorFlow中的placeholder是一个你之后会赋值的变量，这种方式便于把训练数据加入损失方程，把数据加入损失方程用的是这个句法，当你运行训练迭代，用feed_dict来让x等于coefficients，\n",
    "\n",
    "\n",
    "We could turn these numbers 1- 10 and 25 into data.So what I'm going to do is replace the costwith cost = x[0][0]*w squared + x[1][0]*w + x[2][0].Well, times 1.So now x becomes sort of like data that controlsthe coefficients of this quadratic function.And this placeholder function tells TensorFlow thatx is something that you provide the values for later.So let's define another array, coefficient = np.array,[1.], [-10.] and yes, the last value was [25.].So that's going to be the data that we're going to plug into x.So finally we need a way to get this array coefficients into the variable xand the syntax to do that is just doing the training step.That the values for will need to be provided for x,I'm going to set here, feed_dict = x:coefficients,And I'm going to change this, I'm going to copy and paste put that there as well.All right, hopefully, I didn't have any syntax errors.Let's try re-running this and we get the same results hopefully as before.And now, if you want to change the coefficients of this quadratic function,let's say you take this [-10.] and change it to 20, [-20].\n",
    "\n",
    "And let's change this to 100.So this is now a function x- 10 squared.And if I re-run this,hopefully, I find that the value that minimizes x- 10 squared is w = 10.Let's see, cool, great andwe get w very close to 10 after running 1,000 integrations of GradientDescent.So what you see more of when you do that programming exercise is thata placeholder in TensorFlow is a variable whose value you assign later.And this is a convenient way to get your training data into the cost function.And the way you get your data into the cost function is with this syntaxwhen you're running a training iteration touse the feed_dict to set x to be equal to the coefficients here.And if you are doing mini batch Gradient Descentwhere on each iteration you need to plug in a different mini-batch,then on different iterations you use the feed_dict to feed in different subsets of your training sets.Different mini batches into where your cost function is expecting to see data.So hopefully this gives you a sense of what TensorFlow can do.\n",
    "\n",
    "\n",
    "\n",
    "如果你在做mini-batch梯度下降，在每次迭代时你需要插入不同的mini-batch，那么每次迭代  你就用feed_dict来喂入训练集的不同子集，把不同的mini-batch喂入损失函数需要数据的地方，希望这让你了解了TensorFlow能做什么，让它如此强大的是，你只需说明如何计算损失函数，它就能求导，而且用一两行代码就能，运用梯度优化器  Adam优化器或者其他优化器，这还是刚才的代码，我稍微整理了一下，尽管这些函数或变量看上去有点神秘，但你在做编程练习时多练习几次，就会熟悉起来了，还有最后一点我想提一下，这三行在TensorFlow里是符合表达习惯的，有些程序员会用这种形式来替代，作用基本上是一样的，把session设置为tf.Session()来开始session，然后用session来运行init，再用session评估例如w  再输出结果，但这个with结构也会在很多TensorFlow程序中用到，它的意思基本上和左边的相同，但是Python中的with命令更方便清理，以防在执行这个内循环时出现错误或例外，所以你也会在编程练习中看到这种写法，那么这个代码到底做了什么呢？，\n",
    "\n",
    "\n",
    "And the thing that makes this so powerful isall you need to do is specify how to compute the cost functionand then, it takes derivatives andit can apply a gradient optimizer or an Adam optimizer orsome other optimizer with just pretty much one or two lines of codes.So here's the code again.I've cleaned this up just a little bit.And in case some of these functions or variables seem a little bit mysterious to use,they will become more familiar after you've practiced with it a couple timesby working through programming exercise.Just one last thing I want to mention.These three lines of code are quite idiomatic in TensorFlow, andwhat some programmers will do is use this alternative format.Which basically does the same thing.Set session to tf.Session() to start the session,and then use the session to run init, andthen use the session to evaluate, say, w and then print the result.But this with construction is used in a number of TensorFlow programs as well.\n",
    "\n",
    "\n",
    "It more or less means the same thing as the thing on the left.But the with command in Python is a little bit better at cleaning upin case there's an error or exception while executing this inner loop.So you will see this in the programming exercise as well.So what is this code really doing?Let's focus on this equation.The heart of a TensorFlow program is something to compute a cost,and then TensorFlow automatically figures out the derivativesand how to minimize that costs.So what this equation or what this line of code is doingis allowing TensorFlow to construct a computation graph.And a computation graph does the following, it takes x[0][0],it takes w and then it goes w gets squared.And then x[0][0] gets multiplied with w squared,so you have x[0][0]*w squared, and so on, right?And eventually, you know, this gets built up to compute this xw,x[0][0]*w squared + x[1][0]*w + and so on.And so eventually, you get the cost function.\n",
    "\n",
    "\n",
    "And so the last term to be added would be x [2][0]it gets added to be the cost.I won't write all the formulas for the cost.And the nice thing about TensorFlow is that by implementingbasically forward propagation through this computation graph that computes the cost,TensorFlow already has that built in,all the necessary backward functions.So remember how training a deep neural network has a set of forward functionsand a set of backward functions.And programming frameworks like TensorFlow have already built-inthe necessary backward functions.Which is why by using the built-in functions to compute the forward function,it can automatically do the backward functions as well to implement back propagationthrough even very complicated functions and compute derivatives for you.So that's why you don't need to explicitly implement back prop.\n",
    "\n",
    "\n",
    "\n",
    "让我们看这个等式，TensorFlow程序的核心是计算损失的东西，然后TensorFlow自动算出导数，以及如何最小化损失，因此这个等式或者这行代码所做的，就是让TensorFlow建立计算图，计算图所做的就是取x[0][0]，取w  然后将它平方，然后x[0][0]和w^2相乘，你就得到了x[0][0]*w^2  以此类推，最终整个建立起来计算，x[0][0]*w^2 + x[1][0]*w +等等，最后你得到了损失函数，最后加上的一项应该是x [2][0]，加上去得到损失函数，我就不把所有方程写出来了，TensorFlow的优点在于  通过用这个计算损失的，计算图基本实现前向传播，TensorFlow已经内置了，所有必要的反向函数，回忆一下训练深度神经网络时有一组前向函数，和一组反向函数，而像TensorFlow之类的编程框架已经内置了，必要的反向函数，这也是为什么通过内置函数来计算前向函数，它也能自动用反向函数来实现反向传播，即便函数非常复杂 \n",
    "\n",
    "\n",
    "\n",
    "This is one of the things that makes the programming frameworkshelp you become really efficient.If you look at the TensorFlow documentation,I just want to point out that the TensorFlow documentationuses a slightly different notation than I did for drawing the computation graph.So it uses x[0][0] w.And then, rather than writing the value, like w squared,the TensorFlow documentation tends to just write the operation.So this would be a square operation,and then these two get combined in the multiplication operation and so on.And then, the final nodes, I guess that would bean addition operation where you add x[2][0] to find the final value.So for the purposes of this class, I thought that this notationfor the computation draft would be easier for you to understand.\n",
    "\n",
    "\n",
    "But if you look at the TensorFlow documentation,if you look at the computation graphs in the documentation,you see this alternative convention where the nodes are labeledwith the operations rather than with the value.But both of these representations represent basically the same computation graph.And there are a lot of things that you can with just one line of code in programming frameworks.For example, if you don't want to use GradientDescent,but instead you want to use the Adam Optimizerby changing this line of code, you can very quickly swap it,swap in a better optimization algorithm.So all the modern deep learning programming framework support things like this andmakes it really easy for you to code up even pretty complex neural networks.\n",
    "\n",
    "\n",
    "再帮你计算导数，这就是为什么你不需要明确实现反向传播，这是编程框架能帮你变得高效，的原因之一，如果你看TensorFlow的使用说明，我只想指出TensorFlow的说明，用了一套和我不太一样的符号来画计算图，它用了x[0][0]  w，然后它不是写出值  像这里的w^2，TensorFlow使用说明倾向于只写运算符，所以这里就是平方运算，而这两者一起指向乘法运算  以此类推，然后在最后的节点  我猜应该是，一个将x[2][0]加上去得到最终值的加法运算，为本课程起见  我认为计算图用，这种表示方式会更容易理解，但是如果你去看TensorFlow的使用说明，如果你看到说明里的计算图，你会看到另一种表示方式，节点都用运算来标记而不是值，但这两种呈现方式表达的是同样的计算图，在编程框架中你可以用一行代码做很多事情，\n",
    "\n",
    "So I hope this is helpful forgiving you a sense of the typical structure of a TensorFlow program.To recap the material from this week,you saw how to systematically organize the hyperparameter search process.We also talked about batch normalization andhow you can use that to speed up training of your neural networks.And finally, we talked about programming frameworks of deep learning.There are many great programming frameworks.And we had this last video focusing on TensorFlow.With that, I hope you enjoyed this week's programming exercise andthat helps you gain even more familiarity with these ideas.\n",
    "\n",
    "\n",
    "\n",
    "例如  你不想用梯度下降法，而是想用Adam优化器，你只要改变这行代码  就能很快换掉它，换成更好的优化算法，所有现代深度学习编程框架都支持这样的功能，让你很容易就能编写复杂的神经网络，我希望我帮助你，了解了TensorFlow程序典型的结构，概括一下这周的内容，你学习了如何系统化地组织超参数搜索过程，我们还讲了batch归一化，以及如何用它来加速神经网络的训练，最后我们讲了深度学习的编程框架，有很多很棒的编程框架，这最后一个视频我们重点讲了TensorFlow，有了它  我希望你享受这周的编程练习，帮助你更熟悉这些概念，\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### <font color=#0099ff>重点总结：</font>\n",
    "\n",
    "\n",
    "参考文献：\n",
    "\n",
    "[1]. 大树先生.[吴恩达Coursera深度学习课程 DeepLearning.ai 提炼笔记（2-3）-- 超参数调试 和 Batch Norm](http://blog.csdn.net/koala_tree/article/details/78234830)\n",
    "\n",
    "\n",
    "---\n",
    " \n",
    "**PS: 欢迎扫码关注公众号：「SelfImprovementLab」！专注「深度学习」，「机器学习」，「人工智能」。以及 「早起」，「阅读」，「运动」，「英语 」「其他」不定期建群 打卡互助活动。**\n",
    "\n",
    "<center><img src=\"http://upload-images.jianshu.io/upload_images/1157146-cab5ba89dfeeec4b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\"></center>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
