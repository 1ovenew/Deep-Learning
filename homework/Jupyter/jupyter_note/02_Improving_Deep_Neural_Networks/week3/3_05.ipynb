{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you have seen the equations for how toimplement Batch Norm for maybe a single hidden layer.Let's see how it fits into the training of a deep network.So, let's say you have a neural network like this,you've seen me say before thatyou can view each of the unit as computing two things.First, it computes zand then it applies the activation function to compute a.And so we can think ofeach of these circles as representing a two step computation.And similarly for the next layer,that is z2 1, and a2 1, and so on.So, if you were not applying Batch Norm,you would have an input X fit into the first hidden layer,and then first compute z1,and this is governed by the parameters z1 and b1.And then ordinarily, you would fit z1 into the activation function to compute a1.But what would do in Batch Norm is take this value z1,and apply Batch Norm,sometimes abbreviated BN to it,and that's going to be governed by parameters,Beta 1 and Gamma 1,and this will give you this new normalized value z1.And then you fit that to the activation function to get a1,which is G1 applied to z tilde 1.Now, you've done the computation for the first layer,where this Batch Norms that really occurs in between the computation from z and a.Next, you take this value a1 and use it to compute z2,and so this is now governed by w2, b2.And similar to what you did for the first layer,you would take z2 and apply it through Batch Norm, and we abbreviate it to BN now.This is governed by Batch Norm parameters specific to the next layer.So Beta 2, Gamma 2,and now this gives you z tilde 2,and you use that to compute a2 by applying the activation function, and so on.So once again, the Batch Norms that happens between computing z and computing a.And the intuition is that,instead of using the un-normalized value z,you can use the normalized value z tilde, that's the first layer.The second layer as well,instead of using the un-normalized value z2,you can use the mean and variance normalized values Z tilde 2.So the parameters of your network are going to be w1, b1.It turns out we'll get rid of the parametersBut for now, imagine the parameters are the usual w1，b1......wl, bl,and we have added to this new network,additional parameters，Beta 1, Gamma 1, Beta 2, Gamma 2,and so on,for each layer in which you are applying Batch Norm.For clarity, note that these Betas here,these have nothing to do with the hyperparameter betabut we'll see why in the next slide.that we had for momentumor for the computing the various exponentially weighted averages.You know,the authors of the Adam paperhad used Beta in their paper to denote that hyperparameter,the authors of the Batch Norm paperhad used Beta to denote this parameter,but these are two completely different Betas.I decided to stick with Beta on in both cases,in case you read the original papers.But the Beta 1,Beta 2, and so on,that Batch Norm tries to learn is a different Beta thanthe hyperparameter Beta used in momentum and the Adam and RMSprop algorithms.So now that these are the new parameters of your algorithm,you would then use whether optimization you want,such as creating descent in order to implement it.So for example, you might compute d Beta l for a given layer,and then update the parameters Beta,gets updated as Beta minus learning rate times D Beta L.And you can also use Adam or RMS prop or momentumin order to update the parameters Beta and Gamma,not just creating descent.And even though in the previous video,I had explained what the Batch Norm operation does,computes mean and variances and subtracts and divides by them.If they are using a Deep Learning Programming Framework,usually you won't have toimplement the Batch Norm step on Batch Norm layer yourself.So the probing frameworks,can be sub one line of code.So for example, in the TensorFlow framework,you can implement Batch Normalization, you know, with this function.We'll talk more about probing frameworks later,but in practice you might not end up needing to implement all these details yourself,but self-aware of knowing how it worksso that you can get a better understanding of what your code is doing.But implementing Batch Norm is often，you know,something like one line of code in the deep learning frameworks.Now, so far, we've talked about Batch Normas if you were training on your entire training site at the other timeas if you are using Batch gradient descent.In practice,Batch Norm is usually applied with mini-batches of your training set.So the way you actually apply Batch Norm isyou take your first mini-batch and compute z1.Same as we did on the previous slide using the parameters w1,b1and then you take just this mini-batch andcompute mean and variance of the Z1 on just this mini batchand then goes to the second mini-batch x2,and then Batch Norm would subtract by the mean and divide by the standard deviationand then re-scale by Beta 1, Gamma 1, to give you z1,and all this is on the first mini-batch,then you apply the activation function to get A1,and then you compute z2 using w2,b2, and so on.So you do all this in order to***************and you do something similarwhere you will now compute z1 on the second mini-batchand then use Batch Norm to compute z1 tilde.And so here in this Batch Norm step,You would be normalizing z tilde using just the data in your second mini-batch,so does Batch Norm step here.Let's look at the examples in your second mini-batch,computing the mean and variances of the z1's on just that mini-batch andre-scaling by Beta and Gamma to get z tilde, and so on.And you do this with a third mini-batch, and keep training.Now, there's one detail to the parameterization that I want to clean up,which is previously, I said that the parameters was wl, bl,for each layer as well as Beta l, and Gamma l.Now notice that the way Z was computed is as follows,zl= wl x a of l - 1 + b of l. But what Batch Norm does,is it is going to look at the mini-batchand normalize zl to first of mean 0 and standard variance,and then a rescale by Beta and Gamma.But what that means is that,whatever is the value of bl is actually going to just get subtracted out,because during that Batch Normalization step,you are going to compute the means of the zl's and subtract the mean.And so adding any constant to all of the examples in the mini-batch,it doesn't change anything.Because any constant you add will get cancelled out by the mean subtractions step.So, if you're using Batch Norm,you can actually that parameter,or if you want, think of it as setting it permanently to 0.So then the parameterization becomes zl is just wl x al - 1,And then you compute zl normalized,and we compute z tilde = Gamma zl+ Beta,you end up using this parameter Beta Lin order to decide what's the mean of z tilde l.Which is why guess post in this layer.So just to recap,because Batch Norm zeroes out the mean of these zl values in the layer,there's no point having this parameter bl,and so you must get rid of it,and instead is sort of replaced by Beta l,which is a parameter that controls that ends up affecting the shift or the biased terms.Finally, remember that the dimension of zl,because if you're doing this on one example,it's going to be nl by 1,and so bl had a dimension, nl by one,if nl was the number of hidden units in layer l.And so the dimension of Beta l and Gamma lis also going to be nl by 1 because that's the number of hidden units you have.You have nl hidden units, and so Beta l and Gamma l are used toscale the mean and variance of each of the hidden unitsto whatever the network wants to set them to.So, let's pull all together and describe howyou can implement gradient descent using Batch Norm.Assuming you're using mini-batch gradient descent,it runs for t = 1 to the number of many batches.You would implement forward prop on mini-batch xtand doing forward prop in each hidden layer,use Batch Norm to replace zl with z tilde l.And so then it ensures that within that mini-batch,the value z end up with some normalized mean and varianceand the values and the version of the normalized mean and variance is this z tilde l.And then, you use back prop to compute dw,db,for all the values of l,d Beta, d Gamma.Although, technically, since you have got to get rid of b,this actually now goes away.And then finally, you update the parameters.So, w gets updated as w minus the learning rate times dw, as usual,Beta gets updated as Beta minus learning rate times dβ,and similarly for Gamma.And if you have computed the gradient as follows,you could use gradient descent.That's what I've written down here,but this also works with gradient descent with momentum,or RMSprop, or Adam.Where instead of taking this gradient descent update mini-batchyou could use the updates given by these other algorithmsas we discussed in the previous week's videos.Some of these other optimization algorithms as well can be usedto update the parameters Beta and Gamma that Batch Norm added to algorithm.So, I hope that gives you a sense ofhow you could implement Batch Norm from scratch if you wanted to.If you're using one of the Deep Learning Programming frameworkswhich we will talk more about later,hopefully you can just call someone else's implementation inthe Programming framework which will make using Batch Norm much easier.Now, in case Batch Norm still seems a little bit mysteriousif you're still not quite sure why it speeds up training so dramatically,let's go to the next video and talk more aboutwhy Batch Norm really works and what it is really doing.\n",
    "\n",
    "\n",
    "你已经看到那些等式(字幕来源：网易云课堂)，它可以在单一隐藏层上进行Batch归一化，接下来 让我们看看它是怎样在深度网络训练中拟合的吧，假设你有一个这样的神经网络，我之前说过，你可以认为每个单元负责计算两件事，第一 它先计算z，然后应用其到激活函数中在计算a，所以我们可以认为，每个圆圈代表着两步的计算过程，同样的 对于下一层而言，那就是z^[2]_1和a^[2]_1等，所以 如果你没有应用Batch归一化，你会把拟合到第一隐藏层，然后首先计算z^[1]，这是由w^[1]和b^[1]两个参数控制得，接着 通常而言 你会把z^[1]拟合到激活函数以计算a^[1]，但Batch归一化的做法是将z^[1]值，进行Batch归一化，简称BN，此过程将由，β^[1]和γ^[1]两参数控制，这一步操作会给你一个新的规范化的z^[1]值，然后将其输入激活函数中 得到a^[1]，即g^[1]（z̃^[1]），现在 你已在第一层进行了计算，此时 这项Batch归一化发生在z的计算和a之间，接下来 你需要应用a^[1]值来计算z^[2]，此过程是由w^[1]和b^[1]控制的，与你在第一层所做的类似，你会将z^[2] 进行Batch归一化 我们现在简称BN，这是由下一层的Batch归一化参数所管控的的，即β^[2]和γ^[2]，现在 你得到z̃^[2]，再通过激活函数计算出a^[2]  等等，所以 需强调的是 Batch归一化是发生在计算z和a之间的，直觉就是，与其应用没有归一化的z值，不如用归一过的z̃ 这是第一层，第二层同理，与其应用没有规范过的z^[2]值，不如用经方差和均值归一后的z̃^[2]，所以 你网络的参数就会是w^[1] b^[1]，我们将要去掉这些参数，但现在 想像参数是w^[1] b^[l]到w^[l] b^[l]，我们将另一些参数，加入到此新网络中，β^[1] β^[2] γ^[1] γ^[2] 等等，对于应用Batch归一化的每一层而言，需要澄清的是 请注意 这里的这些β，和超参数β没有任何关系，下一张幻灯片中会解释原因，后者是用于momentum，或计算各个指数的加权平均值，Adam论文的作者，在论文里用β代表超参数，Batch归一化论文的作者，则使用β代表此参数，但这是两个完全不同的β，我在两种情况下都决定使用β，以便你阅读那些原创的论文，但β^[1] β^[2] 等等，Batch归一化试图去学习β和，用于momentum、the Adam、RMS prop算法中的β不同，所以现在 这是你算法的新参数，接下来你可以使用想用的任一种优化法，比如 创造下降来应用它，举个例子 对于给定层 你会计算dβ^[l]，接着 更新参数β为β^[l]，即为β^[l]-αdβ^[l]，你也可以使用Adam或 RMS prop或 momentum，以更新参数β和γ，并不只用创造下降法，即使在之前的视频中，我已经解释过Batch归一化是怎么操作的，计算均值和方差 减去 再被它们除，如果它们使用的是深度学习编程框架，通常 你不必自己，把Batch归一化步骤应用于Batch归一化层，因此 探究框架，可写成一行代码，比如说 在TensorFlow框架中，你可以用这个函数来实现Batch归一化，我们会稍后讲解，但实践中 你不必自己操作所有这些具体的细节，但知道它是如何作用的，这样 你会更好的理解代码的作用，但在深度学习框架中 Batch归一化的过程，经常是类似一行代码的东西，所以 到目前为止 我们已经讲了Batch归一化，就像你在整个训练站点上训练一样，或就像你正在使用Batch梯度下降，实践中，Batch归一化通常和训练集的mini-batch一起使用，你应用Batch归一化的方式就是，你用第一个mini-batch然后计算z^[1]，这和上张幻灯片上我们所做的一样  应用参数w^[1] b^[1]，使用这个mini-batch，在其上计算z^[1]的均值和方差，接着 继续第二个mini-batch x^，接着Batch归一化会减去均值 除以标准差，由β^[1] γ^[1]重新缩放 这样就得到了z^[1]，而所有的这些都是在第一个mini-batch的基础上，你再应用激活函数得到a^[1]，然后用w^[2] b^[2]计算z^[2] 等等，所以你做的这一切都是为了，在第一个mini-batch上进行一步梯度下降法，做类似的工作，你会在第二个mini-batch上计算z^[1]，然后用Batch归一化来计算z^[1] tilde，所以在Batch归一化的此步中，你用第二个mini-batch中的数据使z̃归一化，这里的Batch归一化步骤也是如此，让我们来看看在第二个mini-batch中的例子，在mini-batch上计算z^[1]的均值和方差，重新缩放的β和γ得到z̃等等，然后在第三个mini-batch上同样这样做 继续训练，现在 我想澄清此参数化的一个细节，先前 我说过每层的参数是w^[l] b^[1]，还有β^[l]和γ^[l]，请注意计算z的方式如下，z^[l]=w^[l]a^[l-1]+b^[l] 但Batch归一化做的是，它要看这个mini-batch，先将z^[l]归一化 结果为均值0和标准方差，再由β和γ重缩放，但这意味着，无论b^[l]的值是多少 都是要被减去的，因为在Batch归一化的过程中，你要计算z^[l]的均值 再减去平均值，在此例的mini-batch中增加任何常数，数值都不会改变，因为加上的任何常数都将会被均值减法所抵消，所以 如果你在使用Batch归一化，其实你可以消除这个参数，或者你也可以 暂时把它设置为0，那么 参数化变成z^[l]=w^[l]a^[l-1]，然后你计算归一化的z^[l]，z̃=γ^[l]z^[l]+β^[l]，你最后会用参数β^[l]，以便决定z̃^[l]的取值，这就是原因，所以 总结一下，因为Batch归一化0超过了此层z^[l]的均值，b^[l]这个参数没有意义，所以你必须去掉它，由β^[l]替代，这是个控制参数 会影响转移或偏置条件，最后 请记住z^[l]的维数，因为在这个例子中，维数会是(n^[l] 1)，b^[l]的尺寸 (n^[l] 1)，如果 是l层隐藏单元的数量，那β^[l]和γ^[l]的维度，也是(n^[l] 1) 因为这是你有的隐藏层的数量，你有n^[l] 隐藏单元 所以β^[l]和γ^[l]用来，将每个隐藏层的均值和方差缩放为，网络想要的值，让我们总结一下，关于如何用Batch归一化来应用梯度下降法，假设你在使用mini-batch梯度下降法，你运行同t等于1到batch数量的for循环，你会应用正向prop于mini-batch x^，每个隐藏层都应用正向prop，用Batch归一化替代z^[l] 为z̃^[l]，接下来 它确保在这个mini-batch中，z值有归一化的均值和方差，归一化均值和方差是z̃^[l]，然后 你用反向prop计算dw^[l]  db^[l]，及l的所有值dβ^[l] dγ^[l]，尽管 严格来说 因为你要去掉b，这部分其实已经去掉了，最后 你更新这些参数，w^[l]=w^[l]- αdw^[l] 和以前一样，β^[l]=β^[l]- αdβ^[l]，对于γ也是如此，如果你已将梯度计算如下，你就可以使用梯度下降法了，这就是我写到这里的，但这也适用于有momentum、RMSprop、Adam的梯度下降法，与其使用梯度下降法更新mini-batch，你可以用这些其它的算法来更新，我们在之前几星期视频中讨论过的，也可以应用其它的一些优化算法，来更新由Batch归一化添加到算法中的β和γ参数，我希望 你能学会，如何从头开始应用Batch归一化 如果你想的话，如果你使用深度学习编程框架之一，我们之后会谈到，希望 你可以直接叫别人应用于，编程框架 这会使Batch归一化的使用变得很容易，现在 以防Batch归一化仍然看起来有些神秘，尤其是你还不清楚为什么其能如此显著的加速训练，我们下一个视频中 会谈到，Batch归一化为何效果如此显著 它到底在做什么，"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
