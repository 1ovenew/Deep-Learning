{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursera | Andrew Ng (02-week1)—深度学习的实用层面\n",
    "\n",
    "在吴恩达深度学习视频以及大树先生的博客提炼笔记基础上添加个人理解，原大树先生博客可查看该链接地址[大树先生的博客](http://blog.csdn.net/koala_tree)- ZJ\n",
    "    \n",
    ">[Coursera 课程](https://www.coursera.org/specializations/deep-learning) |[deeplearning.ai](https://www.deeplearning.ai/) |[网易云课堂](https://mooc.study.163.com/smartSpec/detail/1001319001.htm)\n",
    "\n",
    "   [CSDN]()：\n",
    "\n",
    "---\n",
    "\n",
    "## <font color=#0099ff> 1.1 Train_dev_test sets (训练_开发_测试_数据集)\n",
    "    \n",
    "**1. 训练、验证、测试集**\n",
    "\n",
    "对于一个需要解决的问题的样本数据，在建立模型的过程中，我们会将问题的 data 划分为以下几个部分：\n",
    "\n",
    "- **训练集（train set）**：用训练集对算法或模型进行训练过程；\n",
    "\n",
    "- **验证集（development set**）：利用验证集或者又称为简单交叉验证集（hold-out cross validation set）进行交叉验证，**选择出最好的模型**；\n",
    "\n",
    "- **测试集（test set**）：最后利用测试集**对模型进行测试**，获取模型运行的**无偏估计**。\n",
    "\n",
    "**小数据时代**\n",
    "\n",
    "在小数据量的时代，如：100、1000、10000 的数据量大小，可以将 data 做以下划分：\n",
    "\n",
    "- 无验证集的情况：70% / 30%；\n",
    "- 有验证集的情况：60% / 20% / 20%；\n",
    "\n",
    "通常在小数据量时代，以上比例的划分是非常合理的。\n",
    "\n",
    "**大数据时代**\n",
    "\n",
    "但是在如今的大数据时代，对于一个问题，我们拥有的 data 的数量可能是百万级别的，所以验证集和测试集所占的比重会趋向于变得更小。\n",
    "\n",
    "验证集的目的是为了验证不同的算法哪种更加有效，所以验证集只要足够大能够验证大约 2-10 种算法哪种更好就足够了，不需要使用 20% 的数据作为验证集。如百万数据中抽取 1 万的数据作为验证集就可以了。\n",
    "\n",
    "测试集的主要目的是评估模型的效果，如在单个分类器中，往往在百万级别的数据中，我们选择其中 1000 条数据足以评估单个模型的效果。\n",
    "\n",
    "- 100万数据量：98% / 1% / 1%；\n",
    "\n",
    "- 超百万数据量：99.5% / 0.25% / 0.25%（或者99.5% / 0.4% / 0.1%）\n",
    "\n",
    "**Notation**\n",
    "\n",
    "- 建议验证集要和训练集来自于同一个分布，可以使得机器学习算法变得更快；\n",
    "- 如果不需要用无偏估计来评估模型的性能，则可以不需要测试集。\n",
    "\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180115092111600?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "## <font color=#0099ff>1.2 Bias_Variance (偏差_方差)\n",
    "    \n",
    "**偏差、方差**\n",
    "\n",
    "对于下图中两个类别分类边界的分割： \n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180115101715219?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "从图中我们可以看出，在**欠拟合（underfitting）**的情况下，出现**高偏差（high bias）**的情况；在**过拟合（overfitting）**的情况下，出现**高方差（high variance）**的情况。\n",
    "\n",
    "在 bias-variance trade off 的角度来讲，我们**利用训练集对模型进行训练就是为了使得模型在 train 集上使 bias 最小化，避免出现underfitting 的情况**；\n",
    "\n",
    "但是如果模型设置的太复杂，虽然在 train 集上 bias 的值非常小，模型甚至可以将所有的数据点正确分类，但是当将训练好的模型应用在dev 集上的时候，却出现了较高的错误率。这是因为模型设置的太复杂则没有排除一些 train 集数据中的噪声，使得模型出现 overfitting 的情况，在dev 集上出现高 variance 的现象。\n",
    "\n",
    "所以对于 bias 和 variance 的权衡问题，对于模型来说是一个十分重要的问题。\n",
    "\n",
    "例子：\n",
    "\n",
    "几种不同的情况： \n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180115111539939?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "以上为在人眼判别误差在 0% 的情况下，该最优误差通常也称为“贝叶斯误差”，如果“贝叶斯误差”大约为 15%，那么图中第二种情况就是一种比较好的情况。\n",
    "\n",
    "High bias and high variance 的情况\n",
    "\n",
    "上图中第三种 bias 和 variance 的情况出现的可能如下： \n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180115110508344?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "没有找到边界线，但却在部分数据点上出现了过拟合，则会导致这种高偏差和高方差的情况。\n",
    "\n",
    "虽然在这里二维的情况下可能看起来较为奇怪，出现的可能性比较低；但是在高维的情况下，出现这种情况就成为可能。\n",
    "    \n",
    "\n",
    "**学会如何通过分析训练集训练算法产生的误差，和验证集验证算法产生的误差，来诊断算法是否存在高偏差或高方差，是否两个值都高  或者两个值都不高，根据算法偏差和方差的具体情况，决定接下来你要做的工作。**\n",
    "\n",
    "    \n",
    "---\n",
    "## <font color=#0099ff>1.3 Basic \"recipe\" for machine learning (机器学习基础)\n",
    " \n",
    "Recap:\n",
    "\n",
    "**如何通过训练误差和验证误差，判断算法偏差或方差是否偏高** \n",
    "\n",
    "**机器学习的基本方法**\n",
    "\n",
    "在训练机器学习模型的过程中，解决 **High bias** 和 **High variance** 的过程：\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180115141429892?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "1.是否存在 High bias ? \n",
    "\n",
    "- 增加网络结构，如增加隐藏层数目；\n",
    "- 训练更长时间；\n",
    "- (寻找合适的网络架构，使用更大的 NN 结构)；\n",
    "\n",
    "2.是否存在 High variance？ \n",
    "\n",
    "- 获取更多的数据；\n",
    "- 正则化（ regularization）；\n",
    "- (寻找合适的网络结构)；\n",
    "\n",
    "\n",
    "---\n",
    "## <font color=#0099ff>1.4 Regularization (正则化)\n",
    "\n",
    "\n",
    "正则化（regularization）\n",
    "\n",
    "利用正则化来解决 High variance 的问题，正则化是在 Cost function 中加入一项正则化项，惩罚模型的复杂度。\n",
    "\n",
    "**Logistic regression**\n",
    "\n",
    "加入正则化项的代价函数： \n",
    "\n",
    "<center>$J(w,b)=\\dfrac{1}{m}\\sum\\limits_{i=1}^{m}L(\\hat y^{(i)},y^{(i)})+\\dfrac{\\lambda}{2m}||w||_{2}^{2}$</center>\n",
    "\n",
    "上式为逻辑回归的 $L_2$正则化。\n",
    "\n",
    "- L2 正则化：$\\dfrac{\\lambda}{2m}||w||_{2}^{2} = \\dfrac{\\lambda}{2m}\\sum\\limits_{j=1}^{n_{x}} w_{j}^{2}=\\dfrac{\\lambda}{2m}w^{T}w$\n",
    "\n",
    "- L1正则化：$\\dfrac{\\lambda}{2m}||w||_{1}=\\dfrac{\\lambda}{2m}\\sum\\limits_{j=1}^{n_{x}}|w_{j}|$\n",
    "\n",
    "\n",
    "其中 λ 为正则化因子。\n",
    "\n",
    "注意：`lambda` 在python中属于保留字，所以在编程的时候，用“lambd”代表这里的正则化因子λ。\n",
    "\n",
    "**Neural network**\n",
    "\n",
    "加入正则化项的代价函数： \n",
    "\n",
    "<center>$J(w^{[1]},b^{[1]},\\cdots,w^{[L]},b^{[L]})=\\dfrac{1}{m}\\sum\\limits_{i=1}^{m}l(\\hat y^{(i)},y^{(i)})+\\dfrac{\\lambda}{2m}\\sum\\limits_{l=1}^{L}||w^{[l]}||_{F}^{2}$</center>\n",
    "\n",
    "其中 $||w^{[l]}||_{F}^{2}=\\sum\\limits_{i=1}^{n^{[l-1]}}\\sum\\limits_{j=1}^{n^{[l]}}(w_{ij}^{[l]})^{2}$ ，因为 w 的大小为 $(n^{[l-1]},n^{[l]})$ ，该矩阵范数被称为“**Frobenius norm**”\n",
    "\n",
    "**Weight decay**\n",
    "\n",
    "在加入正则化项后，梯度变为：\n",
    "\n",
    "<center>$dW^{[l]} = (form\\_backprop)+\\dfrac{\\lambda}{m}W^{[l]}$</center>\n",
    "\n",
    "则梯度更新公式变为：\n",
    "\n",
    "<center>$W^{[l]}:= W^{[l]}-\\alpha dW^{[l]}$</center>\n",
    "\n",
    "代入可得：\n",
    "\n",
    "<center>$W^{[l]}:= W^{[l]}-\\alpha [ (form\\_backprop)+\\dfrac{\\lambda}{m}W^{[l]}]\\\\ = W^{[l]}-\\alpha\\dfrac{\\lambda}{m}W^{[l]} -\\alpha(form\\_backprop)\\\\=(1-\\dfrac{\\alpha\\lambda}{m})W^{[l]}-\\alpha(form\\_backprop)$</center>\n",
    "\n",
    "\n",
    "其中，$(1-\\dfrac{\\alpha\\lambda}{m})$为一个<1的项，会给原来的$W^{[l]}$一个衰减的参数，所以 L2 范数正则化也被称为“**权重衰减**（**Weight decay**）”。\n",
    "\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180115150755178?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180115154536980?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "## <font color=#0099ff> 1.5 Why regularization reduces overfitting? 为什么正则化可以减少过拟合？\n",
    "\n",
    "**为什么正则化可以减小过拟合**\n",
    "\n",
    "假设下图的神经网络结构属于过拟合状态： \n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180115162315097?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "对于神经网络的 Cost function： \n",
    "\n",
    "<center>$J(w^{[1]},b^{[1]},\\cdots,w^{[L]},b^{[L]})=\\dfrac{1}{m}\\sum\\limits_{i=1}^{m}L(\\hat y^{(i)},y^{(i)})+\\dfrac{\\lambda}{2m}\\sum\\limits_{l=1}^{L}||w^{[l]}||_{F}^{2}$</center>\n",
    "\n",
    "加入正则化项，直观上理解，正则化因子λ设置的足够大的情况下，为了使代价函数最小化，权重矩阵W就会被设置为接近于0的值。则相当于消除了很多神经元的影响，那么图中的大的神经网络就会变成一个较小的网络。\n",
    "\n",
    "当然上面这种解释是一种直观上的理解，但是实际上隐藏层的神经元依然存在，但是他们的影响变小了，便不会导致过拟合。\n",
    "\n",
    "数学解释：\n",
    "\n",
    "假设神经元中使用的激活函数为 $g(z)=tanh(z)$，在加入正则化项后： \n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180115164637286?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "当$λ$增大，导致$W^{[l]}$减小，$Z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]}$便会减小，由上图可知，在 z 较小的区域里，$tanh(z)$函数近似线性，所以每层的函数就近似线性函数，整个网络就成为一个简单的近似线性的网络，从而不会发生过拟合。\n",
    "\n",
    "\n",
    "---\n",
    "## <font color=#0099ff>1.6  Dropout regularization (Dropout 正则化)\n",
    "\n",
    " Dropout 正则化\n",
    " \n",
    "Dropout（随机失活）就是在神经网络的 Dropout 层，为每个神经元结点设置一个随机消除的概率，对于保留下来的神经元，我们得到一个节点较少，规模较小的网络进行训练。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180116072211635?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "**实现Dropout的方法：反向随机失活（Inverted dropout）**\n",
    "\n",
    "首先假设对 layer 3 进行dropout：\n",
    "\n",
    "```\n",
    "keep_prob = 0.8  # 设置神经元保留概率\n",
    "d3 = np.random.rand(a3.shape[0], a3.shape[1]) < keep_prob\n",
    "a3 = np.multiply(a3, d3)\n",
    "a3 /= keep_prob\n",
    "```\n",
    "\n",
    "这里解释下为什么要有最后一步：`a3 /= keep_prob`\n",
    "\n",
    "依照例子中的 `keep_prob = 0.8 `，那么就有大约 20% 的神经元被删除了，也就是说 $a^{[3]}$ 中有 20% 的元素被归零了，在下一层的计算中有 $Z^{[4]}=W^{[4]}\\cdot a^{[3]}+b^{[4]}$，所以为了不影响 $Z^{[4]}$ 的期望值，所以需要 $W^{[4]}\\cdot a^{[3]}$ 的部分除以一个 `keep_prob`。\n",
    "\n",
    "Inverted dropout 通过对“`a3 /= keep_prob`”,则保证无论 keep_prob 设置为多少，都不会对 $Z^{[4]}$ 的期望值产生影响。\n",
    "\n",
    "**Notation**：在测试阶段不要用 dropout，因为那样会使得预测结果变得随机。\n",
    "\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180116084102250?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "---\n",
    "## <font color=#0099ff>1.7 Understanding dropout  (理解 dropout )\n",
    "    \n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180116103909022?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)    \n",
    "    \n",
    "理解 Dropout\n",
    "\n",
    "另外一种对于 Dropout 的理解。\n",
    "\n",
    "这里我们以单个神经元入手，单个神经元的工作就是接收输入，并产生一些有意义的输出，但是加入了 Dropout 以后，输入的特征都是有可能会被随机清除的，所以该神经元不会再特别依赖于任何一个输入特征，也就是说不会给任何一个输入设置太大的权重。\n",
    "\n",
    "所以通过传播过程，dropout 将产生和 L2 范数相同的**收缩权重**的效果。\n",
    "\n",
    "对于不同的层，设置的`keep_prob`也不同，一般来说神经元较少的层，会设 `keep_prob `\n",
    "=1.0，神经元多的层，则会将` keep_prob `设置的较小。\n",
    "\n",
    "**缺点：**\n",
    "\n",
    "dropout 的一大缺点就是其使得 Cost function不能再被明确的定义，以为每次迭代都会随机消除一些神经元结点，所以我们无法绘制出每次迭代 $J(W,b)$下降的图，如下：\n",
    "\n",
    "<center>![这里写图片描述](http://img.blog.csdn.net/20180116104255115?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)</center>\n",
    "\n",
    "**使用 Dropout：**\n",
    "\n",
    "关闭 dropout 功能，即设置 `keep_prob = 1.0`；\n",
    "运行代码，确保 $J(W，b)$ 函数单调递减；\n",
    "再打开 dropout 函数。\n",
    "\n",
    "    \n",
    "---\n",
    "## <font color=#0099ff>1.8 Other regularization method (其他正则化方法)\n",
    "    \n",
    "\n",
    "其他正则化方法\n",
    "\n",
    "- 数据扩增（Data augmentation）：通过图片的一些变换，得到更多的训练集和验证集； \n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180116133849825?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "- Early stopping：在交叉验证集的误差上升之前的点停止迭代，避免过拟合。这种方法的缺点是无法同时解决 bias 和 variance 之间的最优。 \n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180116133537960?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "---\n",
    "## <font color=#0099ff>1.9 Normalizing inputs (正则化输入)\n",
    "\n",
    "归一化输入\n",
    "\n",
    "对数据集特征 x1,x2 归一化的过程： \n",
    "\n",
    " ![这里写图片描述](http://img.blog.csdn.net/20180116142658241?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast) \n",
    "\n",
    "- 计算每个特征所有样本数据的均值：$\\mu = \\dfrac{1}{m}\\sum\\limits_{i=1}^{m}x^{(i)}$；\n",
    "- 减去均值得到对称的分布：$x : =x-\\mu$；\n",
    "- 归一化方差：$\\sigma^{2} = \\dfrac{1}{m}\\sum\\limits_{i=1}^{m}x^{(i)^{2}}$, $x = x/\\sigma^{2}$\n",
    "  \n",
    "**使用归一化的原因：**\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180116143822477?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "由图可以看出不使用归一化和使用归一化前后 Cost function 的函数形状会有很大的区别。\n",
    "\n",
    "在不使用归一化的代价函数中，如果我们设置一个较小的学习率，那么很可能我们需要很多次迭代才能到达代价函数全局最优解；\n",
    "\n",
    "如果使用了归一化，那么无论从哪个位置开始迭代，我们都能以相对很少的迭代次数找到全局最优解。\n",
    "\n",
    "\n",
    "---\n",
    "## <font color=#0099ff>1.10 vanishing/exploding gradients (梯度消失与梯度爆炸)\n",
    "\n",
    "梯度消失与梯度爆炸\n",
    "\n",
    "如下图所示的神经网络结构，以两个输入为例：\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180116163343722?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "这里我们首先假定$g(z)=z$，$b[l]=0$，所以对于目标输出有：\n",
    "\n",
    "<center>$\\hat y = W^{[L]}W^{[L-1]}\\cdots W^{[2]}W^{[1]}X$</center>\n",
    "\n",
    "- $W^{[l]}$ 的值大于1的情况：\n",
    "\n",
    "\t如：$W^{[l]}=\\left[ \\begin{array}{l}  \n",
    "1.5 & 0 \\\\ 0 & 1.5\\end{array} \\right]$  ，那么最终，$\\hat y = W^{[L]}\\left[ \\begin{array}{l}\n",
    "1.5 & 0 \\\\\\ 0 & 1.5\\end{array} \\right]^{L-1}X$ ，激活函数的值将以指数级递增；\n",
    "\n",
    "- $W^{[l]}$ 的值小于1的情况：\n",
    "\n",
    "\t如：$W^{[l]}=\\left[ \\begin{array}{l}  \n",
    "0.5 & 0 \\\\ 0 & 0.5\\end{array} \\right]$  ，那么最终，$\\hat y = W^{[L]}\\left[ \\begin{array}{l}\n",
    "0.5 & 0 \\\\\\ 0 & 0.5\\end{array} \\right]^{L-1}X$ ,激活函数的值将以指数级递减。\n",
    "\n",
    "\n",
    "上面的情况对于导数也是同样的道理，所以在计算梯度时，根据情况的不同，梯度函数会以指数级递增或者递减，导致训练导数难度上升，梯度下降算法的步长会变得非常非常小，需要训练的时间将会非常长。\n",
    "\n",
    "在梯度函数上出现的以指数级递增或者递减的情况就分别称为梯度爆炸或者梯度消失。\n",
    "\n",
    "\n",
    "## <font color=#0099ff>1.11 Weight initialization for deep networks (神经网络的权重初始化)\n",
    "\n",
    "利用初始化缓解梯度消失和爆炸问题\n",
    "\n",
    "以一个单个神经元为例子： \n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180116181626601?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "由上图可知，当输入的数量n较大时，我们希望每个wi的值都小一些，这样它们的和得到的z也较小。\n",
    "\n",
    "这里为了得到较小的 $w_i$，设置 $Var(w_{i})=\\dfrac{1}{n}$，这里称为 **Xavier initialization**。 \n",
    "\n",
    "对参数进行初始化：\n",
    "\n",
    "```\n",
    "WL = np.random.randn(WL.shape[0],WL.shape[1])* np.sqrt(1/n)\n",
    "```\n",
    "\n",
    "这么做是因为，如果激活函数的输入x近似设置成均值为0，标准方差1的情况，输出z也会调整到相似的范围内。虽然没有解决梯度消失和爆炸的问题，但其在一定程度上确实减缓了梯度消失和爆炸的速度。\n",
    "\n",
    "**不同激活函数的 Xavier initialization：**\n",
    "\n",
    "激活函数使用 Relu：$Var(w_{i})=\\dfrac{2}{n}$\n",
    "激活函数使用 tanh：$Var(w_{i})=\\dfrac{1}{n}$\n",
    "\n",
    "其中 n 是输入的神经元个数，也就是 n[l−1]。\n",
    "\n",
    "\n",
    "**有时调优该超级参数效果一般，这并不是我想调优的首要超级参数，当发现调优过程中产生的问题，虽然调优该参数能起到一定作用，但考虑到相比调优其它超级参数的重要性，我通常把它的优先级放得比较低。**\n",
    "\n",
    "**设置的权重矩阵 既不要增长过快 也不要太快下降到 0，从而训练出一个，权重或梯度不会增长或消失过快的深度网络，在训练深度网络时，这也是一个加快训练速度的技巧。**\n",
    "\n",
    "## <font color=#0099ff>1.12 Numerical approximation of gradients(梯度的数值逼近)\n",
    "\n",
    "梯度的数值逼近\n",
    "\n",
    "使用**双边误差**的方法去**逼近导数**： \n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180117085640996?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "由图可以看出，双边误差逼近的误差是0.0001，先比单边逼近的误差0.03，其精度要高了很多。\n",
    "\n",
    "涉及的公式：\n",
    "\n",
    "- 双边导数： \n",
    "\n",
    "<center>$f'(\\theta) = \\lim\\limits_{\\varepsilon  \\to 0}=\\dfrac{f(\\theta+\\varepsilon)-(\\theta-\\varepsilon)}{2\\varepsilon}$</center>\n",
    "\n",
    "误差：$O(\\varepsilon^{2})$\n",
    "\n",
    "- 单边导数： \n",
    "\n",
    "<center>$f'(\\theta) = \\lim\\limits_{\\varepsilon  \\to 0}=\\dfrac{f(\\theta+\\varepsilon)-(\\theta)}{\\varepsilon}$\n",
    "\n",
    "误差：$O(\\varepsilon)$</center>\n",
    "\n",
    "\n",
    "**重点是要记住双边误差公式的结果更准确**\n",
    "\n",
    "## <font color=#0099ff>1.13  Gradient Checking (梯度检验)\n",
    "\n",
    "梯度检验\n",
    "\n",
    "下面用前面一节的方法来进行梯度检验。\n",
    "\n",
    "**连接参数**\n",
    "\n",
    "因为我们的神经网络中含有大量的参数：$W^{[1]},b^{[1]},\\cdots,W^{[L]},b^{[L]}$，为了做梯度检验，需要将这些参数全部连接起来，reshape成一个大的向量 θ。\n",
    "\n",
    "同时对 $dW^{[1]},db^{[1]},\\cdots,dW^{[L]},db^{[L]}$ 执行同样的操作。 \n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180117095536767?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "**进行梯度检验**\n",
    "\n",
    "进行如下图的梯度检验：\n",
    "\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180117100600370?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "判断$d\\theta_{approx}\\approx d\\theta$是否接近。\n",
    "\n",
    "判断公式： \n",
    "\n",
    "<center>$ \\dfrac {||d\\theta_{approx}-d\\theta||_{2}}{||d\\theta_{approx}||_{2}+||d\\theta||_{2}}$</center>\n",
    "\n",
    "其中，“$||\\cdot ||_{2}$”表示欧几里得范数，它是误差平方之和，然后求平方根，得到的欧氏距离。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## <font color=#0099ff>1.14 Gradient Checking implementation notes (关于梯度检验实现的注记)\n",
    "    \n",
    "**实现梯度检验 Notes**\n",
    "\n",
    "- 不要在训练过程中使用梯度检验，只在 debug 的时候使用，使用完毕关闭梯度检验的功能；\n",
    "- 如果算法的梯度检验出现了错误，要检查每一项，找出错误，也就是说要找出哪个 $dθ_{approx}^{[i]}$ 与 $dθ$ 的值相差比较大；\n",
    "- 不要忘记了正则化项；\n",
    "- 梯度检验不能与 dropout 同时使用。因为每次迭代的过程中，dropout 会随机消除隐层单元的不同神经元，这时是难以计算 dropout 在梯度下降上的代价函数 $J$；\n",
    "- 在随机初始化的时候运行梯度检验，或许在训练几次后再进行。\n",
    "\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180117140032955?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "参考文献：\n",
    "\n",
    "[1]. 大树先生.[吴恩达Coursera深度学习课程 DeepLearning.ai 提炼笔记（1-4）-- 浅层神经网络](http://blog.csdn.net/koala_tree/article/details/78087711)\n",
    "\n",
    "---\n",
    " \n",
    "**PS: 欢迎扫码关注公众号：「SelfImprovementLab」！专注「深度学习」，「机器学习」，「人工智能」。以及 「早起」，「阅读」，「运动」，「英语 」「其他」不定期建群 打卡互助活动。**\n",
    "\n",
    "<center><img src=\"http://upload-images.jianshu.io/upload_images/1157146-cab5ba89dfeeec4b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\"></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
