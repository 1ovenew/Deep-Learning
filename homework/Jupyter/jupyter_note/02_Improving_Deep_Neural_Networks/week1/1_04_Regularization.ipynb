{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Coursera | Andrew Ng (02-week-1-1.4)—Regularization\n",
    "\n",
    "该系列仅在原课程基础上部分知识点添加个人学习笔记，或相关推导补充等。如有错误，还请批评指教。在学习了 Andrew Ng 课程的基础上，为了更方便的查阅复习，将其整理成文字。因本人一直在学习英语，所以该系列以英文为主，同时也建议读者以英文为主，中文辅助，以便后期进阶时，为学习相关领域的学术论文做铺垫。- ZJ\n",
    "    \n",
    ">[Coursera 课程](https://www.coursera.org/specializations/deep-learning) |[deeplearning.ai](https://www.deeplearning.ai/) |[网易云课堂](https://mooc.study.163.com/smartSpec/detail/1001319001.htm)\n",
    "\n",
    "---\n",
    "   **转载请注明作者和出处：ZJ 微信公众号-「SelfImprovementLab」**\n",
    "   \n",
    "   [知乎](https://zhuanlan.zhihu.com/c_147249273)：https://zhuanlan.zhihu.com/c_147249273\n",
    "   \n",
    "   [CSDN]()：\n",
    "   \n",
    "\n",
    "---\n",
    "\n",
    "1.4 Regularization (正则化)\n",
    "\n",
    "If you suspect your neural network is over fitting your data,that is you have a high variance problem,one of the first things you should try is probably regularization.The other way to address high variance is to get more training data, that's also quite reliable.But you can't always get more training data, orit could be expensive to get more data.But adding regularization will often help to prevent overfitting, orto reduce the errors in your network.So let's see how regularization works.Let's develop these ideas using logistic regression.Recall that for logistic regression, you try to minimize the cost function J,which is defined as this cost function.Some of your training examples of the losses of the individual predictions inthe different examples, where you recall that w and b in the logistic regression are the parameters.So w is an x-dimensional parameter vector, and b is a real number.And so to add regularization to the logistic regression, what you do is add to it,this thing, lambda, which is called the regularization parameter.\n",
    "\n",
    "\n",
    "I'll say more about that in a second.But lambda/2m times the norm of w squared.So here, the norm of w squared is just equal to sum from j equals 1 to nx of wj squared,or this can also be written w transpose w,it's just a square Euclidean norm of the parameter vector w.And this is called L2 regularization.Because here, you're using the Euclidean normals, justcalled the L2 norm with the parameter vector w.Now, why do you regularize just the parameter w?Why don't we add something here about b as well?In practice, you could do this, but I usually just omit this.Because if you look at your parameters, w is usually a pretty high dimensionalparameter vector, especially with a high variance problem.Maybe w just has a lot of parameters, soyou aren't fitting all the parameters well, whereas b is just a single number.So almost all the parameters are in w rather than b.And if you add this last term,in practice, it won't make much of a difference,because b is just one parameter over a very large number of parameters.\n",
    "\n",
    "\n",
    "In practice, I usually just don't bother to include it.But you can if you want.So L2 regularization is the most common type of regularization.You might have also heard of some people talk about L1 regularization.And that's when you add,  instead of this L2 norm,you instead add a term that is lambda/m of sum over of this.And this is also called the L1 norm of the parameter vector w,so the little subscript 1 down there, right?And I guess whether you put m or 2m in the denominator, is just a scaling constant.If you use L1 regularization, then w will end up being sparse.And what that means is that the w vector will have a lot of zeros in it.And some people say that this can help with compressing the model, becausethe set of parameters are zero, and you need less memory to store the model.Although, I find that, in practice, L1 regularization to make your model sparse,helps only a little bit.So I don't think it's used that much,at least not for the purpose of compressing your model.\n",
    "\n",
    "\n",
    "And when people train your networks,L2 regularization is just used much much more often.Sorry, just fixing up some of the notation here.So one last detail.Lambda here is called the regularization, Parameter.And usually, you set this using your development set,or using cross validation.When you try a variety of values and see what does the best,in terms of trading off between doing well in your training set versus alsosetting that two normal of your parameters to be small,which helps prevent over fitting.So lambda is another hyper parameter that you might have to tune.And by the way, for the programming exercises,lambda is a reserved keyword in the Python programming language.So in the programming exercise, we'll have lambd,without the a, so as not to clash with the reserved keyword in Python.So we use lambd to represent the lambda regularization parameter.\n",
    "\n",
    "\n",
    "So this is how you implement L2 regularization for logistic regression.How about a neural network?In a neural network, you have a cost, function that's a function ofall of your parameters, w[1], b[1] through w[L], b[L],where capital L is the number of layers in your neural network.And so the cost function is this, sum of the losses,summed over your m training examples.And says at regularization, you add lambda over 2m of sumover all of your parameters W, your parameter matrix is w,of their, that's called the squared norm.Where this norm of a matrix, meaning the squared normis defined as the sum of the i, sum of j, of each of the elements of that matrix, squared.And if you want the indices of this summation,this is sum from i=1 through n[l-1].Sum from j=1 through n[l],because w is a n[l-1] by n[l] dimensional matrix,where these are the number of hidden units in the number of units [l-1] in layer l.So this matrix norm, it turns out is called the Frobeniusnorm of the matrix, denoted with a F in the subscript.\n",
    "\n",
    "\n",
    "So for arcane linear algebra technical reasons,this is not called the l2 normal of a matrix.Instead, it's called the Frobenius norm of a matrix.I know it sounds like it would be more natural to just call the l2 norm of the matrix,but for really arcane reasons that you don't need to know,by convention, this is called the Frobenius norm.It just means the sum of square of elements of a matrix.So how do you implement gradient descent with this?Previously, we would compute dw using backprop,where backprop would give us the partial derivative of J with respect to w,or really w for any given [l].And then you update w[l], as w[l] minus the learning rate times d.So this is before we added this extra regularization term to the objective.Now that we've added this regularization term to the objective,what you do is you take dw and you add to it, lambda/m times w.And then you just compute this update, same as before.And it turns out that with this new definition of dw[l],this new dw[l] is still a correct definition of the derivative of your cost function,with respect to your parameters,now that you've added the extra regularization term at the end.And it's for this reason that L2 regularization is sometimes alsocalled weight decay.So if I take this definition of dw[l] and just plug it in here,then you see that the update is w[l] = w[l] timesthe learning rate alpha times the thing from backprop, +lambda of m times w[l].\n",
    "\n",
    "\n",
    "Throw the minus sign there.And so this is equal to w[l]- alpha, lambda / m times w[l]- alpha timesthe thing you got from backpop.And so this term shows that whatever the matrix w[l] is,you're going to make it a little bit smaller, right?This is actually as if you're taking the matrix w andyou're multiplying it by 1-alpha lambda/m.You're really taking the matrix w and subtracting alpha lambda/m times this.Like you're multiplying matrix w by this number,which is going to be a little bit less than 1.So this is why L2 norm regularization is also called weight decay.Because it's just like the ordinarily gradient descent, where you updatew by subtracting alpha times the original gradient you got from backprop.But now you're also multiplying w by this thing,which is a little bit less than 1.So the alternative name for L2 regularization is weight decay.\n",
    "\n",
    "\n",
    "I'm not really going to use that name, but the intuition forwhy it's called weight decay is that this first term here, is equal to this.So you're just multiplying the weight metrics by a number slightly less than 1.So that's how you implement L2 regularization in the neural network. Now, one question that peers sometimes ask me is, hey, Andrew,why does regularization prevent over-fitting?Let's take a quick look at the next video,and gain some intuition for how regularization prevents over-fitting.\n",
    "\n",
    "\n",
    "\n",
    "如果你怀疑神经网络过度拟合了数据(字幕来源：网易云课堂)，即存在高方差问题，那么最先想到的方法可能是正则化，另一个解决高方差的方法就是准备更多数据  这也是非常可靠的办法，但你可能无法时时准备足够多的训练数据，或者  获取更多数据的成本很高，但正则化通常有助于避免过度拟合，或减少网络误差，下面我们就来讲讲正则化的作用原理，我用逻辑回归来实现这些设想，求成本函数J的最小值，它是我们定义的成本函数，参数包含一些训练数据和不同数据中个体预测的损失，w和b是逻辑回归的两个参数，w是一个多维度参数矢量  b是一个实数，在逻辑回归函数中加入正则化，只需添加参数λ  也就是正则化参数，一会儿再详细讲，λ/2m乘以w平方的范数，w欧几里德范数的平方等于wj（j值从1到nx）平方的和，也可以表示为wTw，也就是向量参数w的欧几里德范数平方，此方法称为L2正则化，因为这里用了欧几里德法线，被称为向量参数W的L2范数，为什么只正则化参数w，为什么不再加上参数b呢，你可以这么做  只是我习惯省略不写，因为w通常是一个高维参数矢量，已经可以表达高偏差问题，W可能含有很多参数，\n",
    "\n",
    "\n",
    "\n",
    "我们不可能拟合所有参数 而b只是单个数字，所以w几乎涵盖所有参数  而不是b，如果加了参数b，其实也没什么太大影响，因为b只是众多参数中的一个，所以我通常省略不计，如果你想加上这个参数  完全没问题，L2正则化是最常见的正则化类型，你们可能听说过L1正则化，L1正则化加的不是L2范数，而是正则项λ/m乘以W范数从j=1到nx的和，也被称为参数w向量的L1范数，这里的下标是1，无论分母是m还是2m  它都是一个比例常量，如果用的是L1正则化  W最终会是稀疏的，也就是说W向量中有很多0，有人说这样有利于压缩模型，因为集合中参数均为0  存储该模型所占用的内存更少，实际上  虽然L1正则化使模型变得稀疏，却没有降低太多存储内存，所以我认为这并不是L1正则化的目的，至少不是为了压缩模型，人们在训练网络时，越来越倾向于使用L2正则化，不好意思  有几个符号要改一下，我们来看最后一个细节，λ是正则化参数，我们通常使用验证集或交叉验证，来配置这个参数，尝试各种各样的数据  寻找最好的参数，我们要考虑训练集之间的权衡，把参数正常值设置为较小值，这样可以避免过拟合，λ是另外一个需要调整的超级参数，\n",
    "\n",
    "\n",
    "\n",
    "顺便说一下  为了方便编写代码，在Python编程语言中  λ是一个保留字段，编写代码时  我们删掉a  写成lambd，以免与Python中的保留字段冲突，我们用lambd来代替lambda正则化参数，这就是在逻辑回归函数中实现L2正则化的过程，如何在神经网络中实现L2正则化呢，神经网络含有一个成本函数，该函数中包含从w[1]  b[1]到w[L] b[L]所有参数，字母L是神经网络所含的层数，因此成本函数等于，损失总和乘以训练数据m的总和，正则项为，λ/2m乘以参数矩阵W的总和，我们称||W[l]||2为范数平方，这个矩阵范数（即平方范数），被定义为矩阵中所有元素的平方求和，我们看下求和公式的具体参数，第一个求和符号其i值从1到n[l-1]，第二个其j值从1到n[l]，因为W是一个n[l-1] x n[l]的多维矩阵，n[l-1]表示隐藏单元的数量  n[l]表示l层单元的数量，该矩阵范数被称作“弗罗贝尼乌斯范数”，用下标F标注，鉴于线性代数中一些神秘晦涩的原因，我们不称之为“矩阵L2范数”，而称它为“弗罗贝尼乌斯范数”，矩阵L2范数听起来更自然，但鉴于一些大家无须知道的特殊原因，按照惯例  我们称之为“弗罗贝尼乌斯范数”，\n",
    "\n",
    "\n",
    "它表示一个矩阵中所有元素的平方和，如何使用该范数实现梯度下降呢，用backprop计算出dw的值，backprop会给出j对w的偏导数，实际上是w[l]，把w[l]替换为w[l]减去学习率乘以d，这就是之前我们额外增加的正则化项，既然已经增加了这个正则化项，现在我们要做的就是给dw加上这一项  λ/m乘以w，然后计算这个更新项，使用新定义的dw[l]，它的定义含有代价函数导数和，相关参数，以及最后添加的额外正则项，这也是L2正则化有时被称为，“权重衰减”的原因，我们用dw[l]的定义替换此处的dw[l]，可以看到  w[l]的定义被更新为，w[l]  学习率α乘以backprop  再加上λ/mw[l]，这儿写个减号，它等于w[1]减去αλ/m w[l]  然后减去，α乘以backpop的输出，该正则项说明  不论w[l]是什么，我们都试图让它变得更小，实际上 相当于我们给矩阵W，乘以了(1- αλ/m)倍的权重，矩阵W减去αλ/m倍的它，也就是用这个系数乘以矩阵W，该系数小于1，因此  L2范数正则化也被称为“权重衰减”，\n",
    "\n",
    "\n",
    "因为它就像一般的梯度下降，w被更新为少了α乘以backprop输出的最初梯度值，同时w也乘以了这个系数，这个系数小于1，因此L2正则化也被称为“权重衰减”，我不打算这么叫它，之所以叫它“权重衰退“是因为这两项相等，权重指标乘以了一个小于1的系数，以上就是在神经网络中应用L2正则的过程，有人会问我，为什么正则化可以预防过拟合，我们放在下节课讲，同时直观感受一下正则化是如何预防过拟合的，\n",
    "---\n",
    "\n",
    "### <font color=#0099ff>重点总结：</font>\n",
    "\n",
    "\n",
    "参考文献：\n",
    "\n",
    "[1]. 大树先生.[吴恩达Coursera深度学习课程 DeepLearning.ai 提炼笔记（2-1）-- 深度学习的实践方面](http://blog.csdn.net/koala_tree/article/details/78125697)\n",
    "\n",
    "\n",
    "---\n",
    " \n",
    "**PS: 欢迎扫码关注公众号：「SelfImprovementLab」！专注「深度学习」，「机器学习」，「人工智能」。以及 「早起」，「阅读」，「运动」，「英语 」「其他」不定期建群 打卡互助活动。**\n",
    "\n",
    "<center><img src=\"http://upload-images.jianshu.io/upload_images/1157146-cab5ba89dfeeec4b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\"></center>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
