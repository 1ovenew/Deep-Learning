{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   When you implement back propagation you'll find that there's a test called creating checking that can really help you make surethat your implementation of back prop is correct.Because sometimes you write all these equations and you're just not 100% sure if you've got all the details right and implementing back propagation. So in order to build up to gradient checking, let's first talk about how to numerically approximate computations of gradients, and in the next video, we'll talk about how you can implement gradient checking to make sure the implementation of backprop is correct. So lets take the function f and replot it here, and remember this is f(θ) equals θ cubed, and let's again start off some value of theta, let's say theta equals 1. Now instead of just nudging theta to the right to get theta plus epsilon, we're going to nudge it to the right and nudge it to the left to get theta minus epsilon, as well as θ+ε. So this is 1, this is 1.01, this is 0.99 where, again, epsilon is same as before, it is 0.01. It turns out that rather than taking this little triangle and computing the height over the width, you can get a much better estimate of the gradient if you take this point, f of theta minus epsilon and this point,and you instead compute the height over width of this bigger triangle.So for technical reasons which I won't go into, the height over width of this biggergreen triangle gives you a much better approximation to the derivative at theta.And you saw it yourself, taking just this lower triangle in the upper rightis as if you have two triangles, right?This one on the upper right and this one on the lower left. And you're kind of taking both of them into account, by using this bigger green triangle. So rather than a one sided difference, you're taking a two sided difference. So let's work out the math.This point here is f(θ+ε).This point here is f(θ-ε).So the height of this big green triangle is f(θ+ε)-f(θ-ε)And then the width, this is 1 epsilon, this is 2 epsilon.So the width of this green triangle is 2ε.So the height of the width is going to be first the height,so that's f(θ+ε)-f(θ-ε) divided by the width.So that was 2ε which we write that down here.And this should hopefully be close to g(θ).So plug in the values, remember f(θ) is theta cubed.So this is theta plus epsilon is 1.01.So I take a cube of that minus 0.99 take a cube of that, divided by 2 times 0.01.Feel free to pause the video and practice this in the calculator. You should get that this is 3.0001. Whereas from the previous slide, we saw that g(θ), this was 3 theta squared, so when theta was 1, this is 3,  g(θ)=3 θ2 so these two values are actually very close to each other. The approximation error is now 0.0001. Whereas on the previous slide, we've taken the one sided of difference, just theta and theta plus epsilon, we had gotten 3.0301 and so the approximation error was 0.03, rather than 0.0001. So with this two sided difference way of approximating the derivative you find that this is extremely close to 3. And so this gives you a much greater confidence that g(θ) is probably a correct implementation of the derivative of f. When you use this method for gradient checking and back propagation, this turns out to run twice as slow as you were to use a one-sided difference. It turns out that, in practice, I think it's worth it to use this other method,because it's just much more accurate.The little bit of optional theory for those of you that are a little bit more familiar of Calculus,it turns out that, and it's okay if you don't get what I'm about to say here.But it turns out that the formal definition of a derivative is for very small values of epsilonis  f(θ+ε)-f(θ-ε) / 2ε.And the formal definition of derivative is in the limits of exactlythat formula on the right, as epsilon goes as 0.And the definition of unlimited is something that you learned if you took a Calculus class,but I won't go into that here.And it turns out that for a non zero value of epsilon,you can show that the error of this approximation is on the order of epsilon squared,and remember epsilon is a very small number.So if epsilon is 0.01, which it is here,then epsilon squared is 0.0001.The big O notation means the error is actually some constant times this, but this is actually exactly our approximation error. So the big O constant happens to be 1. Whereas in contrast, if we were to use this formula, the other one,then the error is on the order of epsilon.And again, when epsilon is a number less than 1, then epsilon is actuallymuch bigger than epsilon squared, which is why this formula here is actuallymuch less accurate approximation than this formula on the left;which is why when doing gradient checking,we rather use this two-sided difference when you compute f(θ+ε)-f(θ-ε) and then divide by 2ε,rather than just one sided difference which is less accurate. If you didn't understand my last two comments, all of these things are on here, don't worry about it. That's really more for those of you that are a bit more familiar with Calculus, and with numerical approximations.But the takeaway is that this two-sided difference formula is much more accurate.And so that's what we're gonna use when we do gradient checking in the next video. So you've seen how by taking a two sided difference, you can numerically verify whether or not a function g, g(θ) that someone else gives youis a correct implementation of the derivative of a function f. Let's now see how we can use this to verify whether or not your back propagation implementation is correct oryou know, there might be a bug in there that you need to go in and tease out.\n",
    "在实施backprop时(字幕来源：网易云课堂)，有一个测试叫作梯度检验，它的作用是确保backprop正确实施，因为有时候 你虽然写下了这些方程式 却不能100%确定，执行backprop的所有细节都是正确的，为了逐渐实现梯度检验，我们首先说说如何对计算梯度做数值逼近，下节课 我们将讨论如何在backprop中执行梯度检验，以确保backprop正确实施，我们先画出函数f，标记为f(θ)   f(θ)=θ^3，先看下θ的值 假设θ=1，不增大θ的值 而是在θ右侧，设置一个θ+ε，在θ左侧 设置θ-ε，因此θ=1  θ+ε=1.01  θ-ε=0.99，跟以前一样 ε的值为0.01， 看下这个小三角形，计算高和宽的比值 就是更准确的坡度预估，选择f函数在θ-ε上的这个点，用这个较大三角形的高比上宽，技术上的原因我就不详细解释了，较大三角形的高宽比值更接近于θ的导数，把右上角的三角形下移，好像有了两个三角形，右上角有一个 左下角有一个，我们通过这个绿色大三角形同时考虑了这两个小三角形，所以我们得到的不是一个单边公差而是一个双边公差，我们写一下数据算式，这点的值是f(θ+ε)，这点的是f(θ-ε)，这个三角形的高度是f(θ+ε)-f(θ-ε)，这两个宽度都是ε，所以三角形的宽度是2ε，高宽比值为， f(θ+ε)-f(θ-ε) 除以宽度，宽度为2ε 结果为(f(θ+ε)-f(θ-ε))/(2ε )，它的期望值接近g(θ)，传入参数值 f(θ)=θ^3，θ+ε=1.01，(1.01)^3-(0.99)^3/2(0.01)，大家可以暂停视频 用计算器算算结果， 结果应该是3.0001，而前一张幻灯片上面是，当θ=1时 g(θ)=3，所以这两个g(θ)值非常接近，逼近误差为0.0001，前一张幻灯片，我们只考虑了单边公差  即从θ到θ+ε之间的误差，g(θ)的值为3.0301，逼近误差是0.03 不是0.0001，所以使用双边误差的方法更逼近导数，其结果接近于3，现在我们更加确信，g(θ)可能是一个f导数的正确实现，在梯度检验和反向传播中使用该方法时，最终 它与运行两次单边公差的速度一样，实际上 我认为这种方法还是非常值得使用的，因为它的结果更准确，这是一些你可能比较熟悉的微积分的理论，如果你不太明白我讲的这些理论也没关系，导数的官方定义是针对值很小的ε， f(θ+ε)-f(θ-ε) / 2ε，导数的官方定义是右边公式的极限，ε趋近于0，如果你上过微积分课 应该学过无穷尽的定义，我就不在这里讲了，对于一个非零的ε，它的逼近误差可以写成О(ε2)，ε值非常小，如果ε=0.01， ε2=0.0001，大写符号O的含义是指逼近误差其实是一些常量乘以ε2，但它的确是很准确的逼近误差，所以大写O的常量有时是1，然而 如果我们用另外一个公式，逼近误差就是О(ε)，当ε小于1时 实际上ε比ε2大很多，所以这个公式，近似值远没有左边公式的准确，所以在执行梯度检验时，我们使用双边误差 即(f(θ+ε)-f(θ-ε))/(2ε )，而不使用单边公差 因为它不够准确，如果你不理解上面两条结论 所有公式都在这儿，不用担心，如果你对微积分和数值逼近有所了解，这些信息已经足够多了，重点是要记住 双边误差公式的结果更准确，下节课我们做梯度检验时就会用到这个方法，今天我们讲了如何使用双边误差，来判断别人给你的函数g(θ)，是否正确实现了函数f的偏导，现在我们可以使用这个方法来检验，反向传播是否得以正确实施，如果不正确  它可能有bug需要你来解决，"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
