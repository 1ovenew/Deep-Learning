{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout does this seemingly crazy thing of randomly knocking out units on your network.Why does it work so well with a regularizer?Let's gain some better intuition.In the previous video,I gave this intuition that drop-out randomly knocks out units in your network.So it's as if on every iteration, you're working with a smaller neural network,and so using a smaller neural network seems like it should have a regularizing effect.Here's a second intuition which is,let's look at it from the perspective of a single unit.Let's say this one.Now, for this unit to do its job as for inputs and it needs to generate some meaningful output.Now with dropout,the inputs can get randomly eliminated.Sometimes those two units will get eliminated,sometimes a different unit will get eliminated.So, what this means is that this unit, which I'm circling in purple,it can't rely on any one feature,because any one feature could go away at random,or any one of its own inputs could go away at random.Some particular would be reluctant to put all of its bets on, say, just this input, right?The weights, we're reluctant to put too much weight on any one input,because it can go away.So this unit will be more motivated to spread out this wayand give you a little bit of weight to each of the four inputs to this unit.And by spreading all the weights,this will tend to have an effect of shrinking the squared norm of the weights.And so, similar to what we saw with L2 regularization,the effect of implementing dropout is that it shrinks the weights,and does some of those outer regularization that helps prevent over-fitting.But it turns out that dropout can formally be shown to be in an adaptive form without a regularization.But L2 penalty on different weights are different,depending on the size of the activations being multiplied that way.But to summarize,it is possible to show that dropout has a similar effect to L2 regularization.Only to L2 regularization applied to different ways can be a little bit differentand even more adaptive to the scale of different inputs.One more detail for when you're implementing dropout.Here's a network where you have three input features.This is seven hidden units here,seven, three, two, one.So, one of the parameters we had to choose was the keep_probwhich has a chance of keeping a unit in each layer.So, it is also feasible to vary keep_prob by layer.So for the first layer,your matrix W[1] will be three by seven.Your second weight matrix will be seven by seven.W[3] will be seven by three and so on.And so W[2] is actually the biggest weight matrix, rightbecause they're actually the largest set of parameters would be in W[2], which is seven by seven.So to prevent, to reduce over-fitting of that matrix,maybe for this layer,I guess this is layer two,you might have a keep_prob that's relatively low,say zero point five,whereas for different layers where you might worry less about over-fitting,you could have a higher keep_prob,maybe just zero point seven, maybe this is a 0.7And if a layers we don't worry about over-fitting at all,you can have a keep_prob of one point zero.So, for clarity, these are numbers I'm drawing on the purple boxes.These could be different keep_probs for different layers.Notice that the keep_prob of one point zero means that you're keeping every unit and so,you're really not using dropout for that layer.But for layers where you're more worried about over-fitting,really the layers with a lot of parameters,you can set keep_prob to be smaller to apply a more powerful form of dropout.It's kind of like cranking upthe regularization parameter lambda for L2 regularizationwhere you try to regularize some layers more than others.And technically, you can also apply dropout to the input layer,where you can have some chance of just maxing out one or more of the input features.Although in practice, usually don't do that that often.And so, keep_prob of one point zero is quite common for the input there.You can also use a very high value, maybe zero point nine,but it's much less likely that you want to eliminate half of the input features.So usually keep_prob,if you apply the lawwould be a number close to one, if you even apply dropout at all to the input layer.So just to summarize,if you're more worried about some layers overfitting than others,you can set a lower keep_prob for some layers than others.The downside is, this gives you even more hyper parameters to search for using cross-validation.One other alternative might be to have some layers where you apply dropoutand some layers where you don't apply dropoutand then just have one hyper parameter,which is the keep_prob for the layers for which you do apply dropout.And before we wrap up, just a couple implementational tips.Many of the first successful implementations of dropouts were to computer vision.So in computer vision, the input size is so big,you inputting all these pixels that you almost never have enough data.And so dropout is very frequently used by computer vision.And there's some computer vision researchers that pretty much always use it,almost as a default.But really the thing to remember is that dropout is a regularization technique,it helps prevent over-fitting.And so, unless my algorithm is over-fitting,I wouldn't actually bother to use dropout.So it's used somewhat less often than other application areas.There's just with computer vision,you usually just don't have enough data,so you're almost always overfitting,which is why there tends to be some computer vision researchers who swear by dropout.by the intuition, I was doesn't always generalize I think to other disciplines.One big downside of dropout is that the cost function J is no longer well-defined.On every iteration, you are randomly killing off a bunch of nodes.and so, if you are double checking the performance of gradient dissent,it's actually harder to double check thatright, you have a well-defined cost function J that is going downhill on every iteration.Because the cost function J that you're optimizing is actually less,less well-defined, or is certainly hard to calculate.So you lose this debugging tool to will a plot,a graph like this.So what I usually do is turn off dropout,you will set key prop equals one,and I run my code and make sure that it is monotonically decreasing J,and then turn on dropout and hope thatI didn't introduce bugs into my code during dropout.Because you need other ways, I guess,but not plotting these figures to make sure that your code is working to greatnessand it's working even with dropout.So with that, there's still a few more regularization techniques that are worth your knowing.Let's talk about a few more such techniques in the next video.\n",
    "\n",
    "Deopout可以随机删除网络中的神经单元  做法有点疯狂(字幕来源：网易云课堂)，它为什么可以通过正则化发挥这么大作用呢，我们来更直观地理解一下，上节课，我们已经对dropout随机删除网络中的神经单元有了一个直观了解，好像每次迭代之后  神经网络都会变得比以前更小，因此采用一个较小神经网络好像和使用正则化的效果是一样的，第二个直观认识是，我们从单个神经元入手，如图，这个单元的工作就是输入并生成一些有意义的输出，通过dropout，该单元的输入几乎被消除，有时这两个单元会被删除，有时会删除其它单元，就是说  我用紫色圈起来的这个单元，它不能依靠任何特征，因为特征都有可能被随机清除，或者说该单元的输入也都可能被随机清除，我不愿意把所有赌注都放在一个节点上，不愿意给任何一个输入加上太多权重，因为它可能会被删除，因此该单元将通过这种方式积极地传播开，并为单元的四个输入增加一点权重，通过传播所有权重，dropout将产生收缩权重的平方范数的效果，和我们之前讲过的L2正则化类似，实施dropout的结果是它会压缩权重，并完成一些预防过拟合的外层正则化，事实证明  dropout被正式地作为一种正则化的替代形式，L2对不同权重的衰减是不同的，它取决于倍增的激活函数的大小，总结一下，dropout的功能类似于L2正则化，与L2正则化不同的是 被应用的方式不同 dropout也会有所不同，甚至更适用于不同的输入范围，实施dropout的另一个细节是，这是一个拥有三个输入特征的网络，这是它的7个隐藏单元，7个  3个  2个和1个，其中一个要选择的参数是keep-prob，它代表每一层上保留单元的概率，所以不同层的keep-prob也可以变化，第一层，矩阵W[1]是3x7，第二个权重矩阵是7x7，W[3]是7x3  以此类推，W[2]是最大的权重矩阵，因为W[2]拥有最大参数集  即7x7，为了预防矩阵的过拟合，对于这一层，我认为这是第二层，它的keep-prob值应该相对较低，假设是0.5，对于其它层  过拟合的程度可能没那么严重，它们的keep-prob值可能高一些，可能是0.7 这里是0.7，如果在某一层  我们不必担心其过拟合的问题，那么  keep-prob可以为1，为了表达清楚  我用紫线笔把它们圈出来，每层keep-prob的值都可能不同，注意 keep-prob的值是1  意味着保留所有单元，并且不在这一层使用dropout，对于有可能出现过拟合，且含有诸多参数的层，我们可以把keep.prob设置成比较小的值   以便应用更强大的dropout，有点像在处理，L2正则化的正则化参数λ，我们尝试对某些层施行更多正则化，从技术上讲  我们也可以对输入层应用dropout，我们有机会删除一个或多个输入特征，虽然现实中  我们通常不这么做，keep-prob的值为1  是非常常用的输入值，也可以用更大的值  或许是1.9，但是消除一半的输入特征是不太可能的，如果我们遵守这个准则，keep-prob的值，会接近于1  即使你对输入层应用dropout，总结一下，如果你担心某些层比其它层更容易发生过拟合，可以把某些层的keep-prob值设置得比其它层更低，缺点是为了使用交叉验证  你要搜索更多的超级参数，另一种方案是在一些层上应用dropout，而有些层不用dropout，应用dropout的层只含有一个超级参数，就是keep-prob，结束前分享两个实施过程中的技巧，实施dropout  在计算机视觉领域有很多成功的第一次，计算视觉中的输入量非常大，输入了太多像素  以至于没有足够的数据，所以dropout在计算机视觉中应用得比较频繁，有些计算机视觉研究人员非常喜欢用它，几乎成了默认的选择，但是要牢记一点  dropout是一种正则化方法，它有助于预防过拟合，因此  除非算法过拟合，不然我是不会使用dropout 的，所以它在其它领域应用得比较少，主要存在于计算视觉领域，因为我们通常没有足够的数据，所以一直存在过拟合，这就是有些计算视觉研究人员如此钟情dropout函数的原因，直观上  我认为不能概括其它学科，dropout一大缺点就是代价函数J不再被明确定义，每次迭代  都会随机移除一些节点，如果再三检查梯度下降的性能，实际上是很难进行复查的，定义明确的代价函数J每次迭代后都会下降，因为我们所优化的代价函数J实际上并没有明确定义，或者在某种程度上很难计算，所以我们失去了调试工具，来绘制这样的图片，我通常会关闭dropout函数，将keep.prop的值设为1，运行代码  确保J函数单调递减，然后再打开dropout函数，在dropout过程中  代码并未引入bug，我觉得你也可以尝试其它方法，虽然我们并没有关于这些方法性能的数据统计，但是你可以把它们与dropout方法一起使用，所以值得大家去学习的正则化方法并不止这一个，我们下节课再讲，"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
