{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursera | Andrew Ng (02-week-2-2.8)—Adam Optimization algorithms\n",
    "\n",
    "该系列仅在原课程基础上部分知识点添加个人学习笔记，或相关推导补充等。如有错误，还请批评指教。在学习了 Andrew Ng 课程的基础上，为了更方便的查阅复习，将其整理成文字。因本人一直在学习英语，所以该系列以英文为主，同时也建议读者以英文为主，中文辅助，以便后期进阶时，为学习相关领域的学术论文做铺垫。- ZJ\n",
    "    \n",
    ">[Coursera 课程](https://www.coursera.org/specializations/deep-learning) |[deeplearning.ai](https://www.deeplearning.ai/) |[网易云课堂](https://mooc.study.163.com/smartSpec/detail/1001319001.htm)\n",
    "\n",
    "---\n",
    "   **转载请注明作者和出处：ZJ 微信公众号-「SelfImprovementLab」**\n",
    "   \n",
    "   [知乎](https://zhuanlan.zhihu.com/c_147249273)：https://zhuanlan.zhihu.com/c_147249273\n",
    "   \n",
    "   [CSDN]()：\n",
    "   \n",
    "\n",
    "---\n",
    "\n",
    "2.8 Adam Optimization algorithms (Adam优化算法)\n",
    "\n",
    "(字幕来源：网易云课堂)\n",
    "\n",
    "During the history of deep learning,many researchers including some very well-known researchers,sometimes proposed optimization algorithms and showed that they worked well in a few problems.But those optimization algorithms subsequently were shownnot to really generalize that wellto the wide range of neural networks you might want to train.So over time, I think the deep learning community actually developedsome amount of skepticism about new optimization algorithms.And a lot of people felt thatgradient descent with momentum really works well,was difficult to propose things that work much better.\n",
    "\n",
    "在深度学习的历史上，包括许多知名研究者在内，提出了优化算法，并很好地解决了一些问题，但随后这些优化算法被指出，并不能一般化，并不适用于多种神经网络，时间久了 深度学习圈子里的人开始，多少有些质疑全新的优化算法，很多人都觉得，Momentum梯度下降法很好用，很难再想出更好的优化算法，所以RMSprop以及Adam优化算法，Adam优化算法也是本视频的内容，就是少有的经受住人们考验的两种算法，已被证明，适用于不同的深度学习结构，这个算法，我会毫不犹豫地推荐给你，因为很多人都试过，并且用它很好地解决了许多问题，Adam优化算法基本上就是，将Momentum和RMSprop结合在一起，\n",
    "\n",
    "\n",
    "So,  RMSprop and the Adam optimization algorithm,which we'll talk about in this video,is one of those rare algorithms that has really stood up,and has been shown towork well across a wide range of deep learning architectures.So, this is one of the algorithmsthat I wouldn't hesitate to recommend you trybecause many people have tried itand seen it work well on many problems.And the Adam optimization algorithm is basicallytaking momentum and RMSprop and putting them together.So, let's see how that works.To implement Adam you would initialize:V_dW=0, S_dW=0, and similarly V_db, S_db=0.And then on iteration t,you would compute the derivatives:compute dW, db using current mini-batch.So usually, you do this with mini-batch gradient descent.And then you do the momentum exponentially weighted average.So V_dW = ß.But now I'm going to call this β_1to distinguish it from the hyperparameter β_2we'll use for the RMSprop proportion of this.So, this is exactly what we hadwhen we're implementing momentum,except it now called hyper parameter β_1 instead of β.And similarly, you have V_db as follows: 1 minus β_1 times db.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "那么来看看如何使用Adam算法，使用Adam算法 首先你要初始化，V_dW=0 S_dW=0 V_db=0 S_db=0，在第t次迭代中，你要计算微分，用当前的mini-batch计算dW db，一般你会用mini-batch梯度下降法，接下来计算momentum指数加权平均数，所以V_dW=β，现在我要用β_1，这样就不会跟超参数β_2混淆，因为后面RMSprop要用到β_2，使用Momentum时，我们肯定会用这个公式，但现在不叫它β 而叫它β_1，同样V_db等于(1-β_1)*db，接着你用RMSprop进行更新，即用不同的超参数β_2，加上(1-β_2)*dW^2，再说一次，这里是对整个微分dW进行平方处理，S_db等于这个加上(1-β_2)*db^2，相当于Momentun更新了超参数β_1，RMSprop更新了超参数β_2，一般使用Adam算法的时候，要计算偏差修正，修正的V_dW，修正也就是在偏差修正之后，等于V_dW/(1-β_1^t)，t是迭代次数，同样修正的V_db等于，V_db/(1-β_1^t)，S也使用偏差修正，也就是S_dW/(1-β_1^t)，修正的S_db等于S_db/(1-β_2^t)，最后更新权重，\n",
    "\n",
    "And then you do the RMSprop update as well.So now, you have a different hyperparemeter β_2plus one minus β_2 dW squared.Again,the squaring there is element-wise squaring of your derivatives dW.And then S_db is equal to this plus one minus β_2 times db.So this is the momentum like update with hyperparameter β_1and this is the RMSprop like update with hyperparameter β_2.In the typical implementation of Adam,you do implement bias correction.So you're going to have V corrected.Corrected means after bias correction.dW = V_dW divided by 1 minus ß1 to the power of tif you've done t iterations.And similarly, V_db corrected equalsV_db divided by 1 minus β_1 to the power of t.And then similarly, you implement this bias correction on S as well.So, that's S_dW divided by one minus β_2 to the t,and S_db corrected equals S_db divided by 1 minus β_2 to the t.\n",
    "\n",
    "所以W更新后是W减去α乘以，如果你只用Momentum，你就用V_dW或者修正后的V_dW，但现在我们加入了RMSprop的部分，所以我们要，除以修正后S_dW的平方根加上ε，根据类似的公式更新b值，修正V_db除以修正后S_db的平方根加上ε，所以Adam算法结合了，Momentum和RMSprop梯度下降法，并且是一种极其常用的学习算法，被证明能有效适用于不同神经网络，适用于广泛的结构，本算法中有很多超参数，超参数学习率α很重要，也经常需要调试，你可以尝试一系列值，然后看哪个有效，β_1常用的缺省值为0.9，这是dW的移动平均数，也就是dW的加权平均数，这是momentum涉及的项，至于超参数β_2，Adam论文的作者，也就是Adam算法的发明者，推荐使用0.999，\n",
    "\n",
    "\n",
    "\n",
    "Finally, you perform the update.So W gets updated as W minus alpha times.So if you're just implementing momentumyou'd use V_dW, or maybe V_dW corrected.But now, we add in the RMSprop portion of this.So we're also going todivide by square roots of S_dW corrected plus epsilon.And similarly, b gets updated as a similar formula,V_db corrected, divided by square root S, corrected db, plus epsilon.And so, this algorithm combines the effect of gradient descent with momentumtogether with gradient descent with rRMSprop.And this is a commonly used learning algorithmthat is proven to be very effective for many different neural networksof a very wide variety of architectures.So, this algorithm has a number of hyperparameters.The learning rate hyper parameteralpha is still importantand usually needs to be tuned.So you just have to try a range of valuesand see what works.\n",
    "\n",
    "\n",
    "这是在计算dW^2以及db^2的，移动加权平均值，关于ε的选择其实没那么重要，Adam论文的作者建议ε为10^(-8)，但你并不需要设置它，因为它并不会影响算法表现，但是在使用Adam的时候，人们往往用缺省值即可，β_1 β_2和ε都是如此，我觉得没人会去调整ε，然后尝试不同的α值，看看哪个效果最好，你也可以调整β_1和β_2，但我认识的业内人士很少人这么干，为什么这个算法叫做Adam？，Adam代表的是Adaptive Moment Estimation，β_1用于计算这个微分，叫做第一矩，β_2用来计算平方数的指数加权平均数，叫做第二矩，所以Adam的名字由此而来，\n",
    "\n",
    "A common choice really the default choice for β_1 is 0.9.So this is a moving average,weighted average of dW right?this is the momentum light term,the hyperparameter for β_2,the authors of the Adam paper,inventors of the Adam algorithmrecommend 0.999.Again this is computing the moving weighted average ofdW squared as well as db squared.And then Epsilon, the choice of epsilon doesn't matter very much.But the authors of the Adam paper recommended it 10 to the minus 8.But this parameter you really don't need to set itand it doesn't affect performance much at all.But when implementing Adam,what people usually do is just use the default value.So, β_1 and β_2 as well as epsilon.I don't think anyone ever really tunes Epsilon.And then, try a range of values of Alphato see what works best.You could also tune β_1 and β_2but it's not done that often among the practitioners I know.So, where does the term 'Adam' come from?Adam stands for Adaptive Moment Estimation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "So β_1 is computing the mean of the derivatives.This is called the first moment.And β_2 is used to compute exponentially weighted average of the squares,and that's called the second moment.So that gives rise to the name adaptive moment estimation.But everyone just calls it the Adam authorization algorithm.And, by the way, one of my long term friends and collaboratorsis call Adam Coates.As far as I know,this algorithm doesn't have anything to do with him,except for the fact that I think he uses it sometimes.But sometimes I get asked that question,so just in case you're wondering.So, that's it for the Adam optimization algorithm.With it, I think you really train your neural networks much more quickly.But before we wrap up for this week,let's keep talking about hyperparameter tuning,as well as gain some more intuitions aboutwhat the optimization problem for neural networks looks like.In the next video, we'll talk about learning rate decay.\n",
    "\n",
    "\n",
    "\n",
    "但是大家都简称为Adam权威算法，顺便提一下 我有一个老朋友兼合作伙伴，叫做Adam Coates，据我所知，他跟Adam算法没有任何关系，不过我觉得他偶尔会用到这个算法，不过有时有人会问我这个问题，我想你可能也有相同的疑惑，这就是有关Adam优化算法的全部内容，有了它 你可以更加地训练神经网络，在结束本周课程之前，我们还要讲一下超参数调整，以及更好地理解，神经网络的优化问题有哪些，下个视频中 我们将讲讲学习率衰减，\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### <font color=#0099ff>重点总结：</font>\n",
    "\n",
    "\n",
    "参考文献：\n",
    "\n",
    "[1]. 大树先生.[吴恩达Coursera深度学习课程 DeepLearning.ai 提炼笔记（2-2）-- 优化算法](http://blog.csdn.net/koala_tree/article/details/78199611)\n",
    "\n",
    "\n",
    "---\n",
    " \n",
    "**PS: 欢迎扫码关注公众号：「SelfImprovementLab」！专注「深度学习」，「机器学习」，「人工智能」。以及 「早起」，「阅读」，「运动」，「英语 」「其他」不定期建群 打卡互助活动。**\n",
    "\n",
    "<center><img src=\"http://upload-images.jianshu.io/upload_images/1157146-cab5ba89dfeeec4b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\"></center>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
