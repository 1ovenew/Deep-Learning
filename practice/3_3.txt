1
00:00:00,210 --> 00:00:04,380
到现在为止 你已经听了许多关于如何搜索最优超参数的内容(字幕来源：网易云课堂)
You have now heard a lot about how
to search for good hyperparameters.

2
00:00:04,380 --> 00:00:08,090
在结束我们关于超参数搜索的讨论之前
Before wrapping up our discussion
on hyperparameter search,

3
00:00:08,090 --> 00:00:11,670
我想最后和你分享一些建议和技巧
I want to share with you just
a couple of final tips and tricks for

4
00:00:11,670 --> 00:00:14,775
关于如何组织你的超参数搜索过程
how to organize your
hyperparameter search process.

5
00:00:14,775 --> 00:00:19,259
如今的深度学习已经应用到许多不同的领域
Deep learning today is applied to
many different application areas and

6
00:00:19,259 --> 00:00:24,042
某个应用领域的超参数设定
that intuitions about hyperparameter
settings from one application area

7
00:00:24,042 --> 00:00:26,670
有可能通用于另一领域
may or may not transfer to a different one.

8
00:00:26,670 --> 00:00:31,466
不同的应用领域出现相互交融
There is a lot of cross-fertilization
among different applications' domains,

9
00:00:31,466 --> 00:00:36,055
比如 我曾经看过计算机视觉领域中涌现的巧妙方法
so for example, I've seen ideas developed
in the computer vision community,

10
00:00:36,055 --> 00:00:40,301
比如说Confonets或ResNets 这我们会在后续课程中讲到
such as Confonets or ResNets,
which we'll talk about in a later course,

11
00:00:40,301 --> 00:00:42,400
它还成功应用于演讲
successfully applied to speech.

12
00:00:42,400 --> 00:00:46,620
我还看过最初起源于演讲的想法成功应用于NLP
I've seen ideas that were first developed
in speech successfully applied in NLP,

13
00:00:46,620 --> 00:00:47,870
等等
and so on.

14
00:00:47,870 --> 00:00:50,613
深度学习领域中 发展很好的一点是
So one nice development in deep learning is that

15
00:00:50,613 --> 00:00:57,223
不同应用领域的人们会阅读越来越多其它研究领域的文章
people from different application domains do read increasingly research
papers from other application domains to

16
00:00:57,223 --> 00:01:00,180
跨领域去寻找灵感
look for inspiration for
cross-fertilization.

17
00:01:00,180 --> 00:01:03,580
就超参数的设定而言
In terms of your settings for the hyperparameters,

18
00:01:03,580 --> 00:01:06,420
我见到过有些直觉想法变得很缺乏新意
though,I've seen that intuitions do get stale.

19
00:01:06,420 --> 00:01:09,360
所以 即使你只研究一个问题 比如说逻辑学
So even if you work on just one problem,say logistics,

20
00:01:09,360 --> 00:01:15,700
你也许已经找到一组好的超参数设置 并继续发展算法
you might have found a good setting for the hyperparameters and
kept on developing your algorithm,

21
00:01:15,700 --> 00:01:20,465
或也许在几个月的过程中 观察到你的数据会逐渐改变
or maybe seen your data gradually change
over the course of several months,

22
00:01:20,465 --> 00:01:25,070
或也许只是在你的数据中心 更新了服务器
or maybe just upgraded
servers in your data center.

23
00:01:25,070 --> 00:01:26,380
正因为有了这些变化
And because of those changes,

24
00:01:26,380 --> 00:01:29,500
你原来的超参数的设定不再好用
the best setting of your
hyperparameters can get stale.

25
00:01:29,500 --> 00:01:31,005
所以我建议
So I recommend maybe just

26
00:01:31,005 --> 00:01:35,860
或许只是重新测试或评估你的超参数 每隔几个月至少一次
retesting or reevaluating your hyperparameters
at least once every several months

27
00:01:35,860 --> 00:01:39,390
以确保你对数值依然很满意
to make sure that you're still
happy with the values you have.

28
00:01:39,390 --> 00:01:43,150
最后 关于如何搜索超参数的问题
Finally, in terms of how
people go about searching for hyperparameters,

29
00:01:43,150 --> 00:01:46,430
我见过大概两种重要的思想流派
I see maybe two
major schools of thought, or

30
00:01:46,430 --> 00:01:50,370
或人们通常采用的两种重要但不同的方式
maybe two major different ways
in which people go about it.

31
00:01:50,370 --> 00:01:52,920
一种是你照看一个模型
One way is if you babysit one model.

32
00:01:52,920 --> 00:01:56,671
通常是有庞大的数据组
And usually you do this if you have
maybe a huge data set

33
00:01:56,671 --> 00:01:59,390
但没有许多计算资源或足够的CPU和GPU的前提下
but not a lot of computational resources, not a lot of CPUs and GPUs,

34
00:01:59,390 --> 00:02:05,100
基本而言 你只可以一次负担起试验一个模型或一小批模型
so you can basically afford to train only one model or
a very small number of models at a time.

35
00:02:05,100 --> 00:02:11,070
这种情况下 即使当它在试验时 你也可以逐渐改良
In that case you might gradually babysit
that model even as it's training.

36
00:02:11,070 --> 00:02:15,180
比如 第0天 你将随机参数初始化
So, for example, on Day 0 you might
initialize your parameter as random and

37
00:02:15,180 --> 00:02:16,370
然后开始试验
then start training.

38
00:02:16,370 --> 00:02:21,626
然后你逐渐观察自己的学习曲线 也许是损失函数J
And you gradually watch your learning
curve, maybe the cost function J or

39
00:02:21,626 --> 00:02:27,333
或者数据设置误差或其他的东西 在第一天内逐渐减少
your data set error or something else,
gradually decrease over the first day.

40
00:02:27,333 --> 00:02:31,300
那这一天末的时候 你可能会说 看 它学习得真不错
Then at the end of day one, you might say,
gee, looks it's learning quite well,

41
00:02:31,300 --> 00:02:35,000
我要试着增加一点学习速率 看看它会怎样
I'm going to try increasing the learning
rate a little bit and see how it does.

42
00:02:35,000 --> 00:02:37,090
也许结果证明它做得更好
And then maybe it does better.

43
00:02:37,090 --> 00:02:38,870
那是你第二天的表现
And then that's your Day 2 performance.

44
00:02:38,870 --> 00:02:42,150
两天后 你会说 它依旧做得不错
And after two days you say, okay,
it's still doing quite well.

45
00:02:42,150 --> 00:02:46,339
也许我现在可以填充下momentum或减少变量
Maybe I'll fill the momentum term a bit or
decrease the learning variable a bit now,

46
00:02:46,339 --> 00:02:47,994
然后 进入第三天
and then you're now into Day 3.

47
00:02:47,994 --> 00:02:52,750
每天 你都会观察它 不断调整你的参数
And every day you kind of look at it and
try nudging up and down your parameters.

48
00:02:52,750 --> 00:02:55,646
也许有一天你会发现你的学习率太大了
And maybe on one day you found
your learning rate was too big.

49
00:02:55,646 --> 00:02:58,649
所以 你可能又回归之前的模型 像这样
So you might go back to the previous
day's model, and so on.

50
00:02:58,649 --> 00:03:01,445
但你可以说是在每天花时间照看此模型
But you're kind of babysitting the model
one day at a time

51
00:03:01,445 --> 00:03:08,080
即使是它在许多天或许多星期的试验过程中
even as it's training over a course of many days or over
the course of several different weeks.

52
00:03:08,080 --> 00:03:12,010
所以 这是一个人们照料一个模型的方法
So that's one approach, and
people that babysit one model,

53
00:03:12,010 --> 00:03:17,390
观察它的表现 耐心地调试学习率
that is watching performance and patiently
nudging the learning rate up or down.

54
00:03:17,390 --> 00:03:21,010
但那通常是因为你没有足够的计算能力
But that's usually what happens if
you don't have enough computational

55
00:03:21,010 --> 00:03:24,210
不能在同一时间试验大量模型时才采取的办法
capacity to train a lot of
models at the same time.

56
00:03:24,210 --> 00:03:28,480
另一种方法则是同时试验多种模型
The other approach would be if you
train many models in parallel.

57
00:03:28,480 --> 00:03:32,010
你设置了一些超参数
So you might have some setting
of the hyperparameters and

58
00:03:32,010 --> 00:03:36,050
尽管让它自己运行 或者是一天甚至多天
just let it run by itself ,either for
a day or even for multiple days,

59
00:03:36,050 --> 00:03:38,180
然后你会获得像这样的学习曲线
and then you get some
learning curve like that; and

60
00:03:38,180 --> 00:03:42,180
这可以是损失函数J或实验误差的损失或
this could be a plot of the cost function
J or cost of your training error or

61
00:03:42,180 --> 00:03:45,670
数据设置误差的损失 但都是你曲线轨迹的度量
cost of your data set error, but
some metric in your tracking.

62
00:03:45,670 --> 00:03:47,636
同时 你可以开始一个有着不同
And then at the same time you might start
up a different model

63
00:03:47,636 --> 00:03:50,490
超参数设定的不同模型
with a different setting of the hyperparameters.

64
00:03:50,490 --> 00:03:54,030
所以 你的第二个模型会生成一个不同的学习曲线
And so, your second model might
generate a different learning curve,

65
00:03:54,030 --> 00:03:55,960
也许是像这样的一条
maybe one that looks like that.

66
00:03:55,960 --> 00:03:57,510
我会说这条看起来更好些
I will say that one looks better.

67
00:03:57,510 --> 00:03:59,980
与此同时 你可以试验第三种模型
And at the same time,
you might train a third model,

68
00:03:59,980 --> 00:04:03,914
其可能产生一条像这样的学习曲线  还有另一条
which might generate a learning curve that
looks like that, and another one that,

69
00:04:03,914 --> 00:04:06,680
也许这条有所偏离 像这样 等等
maybe this one diverges so
it looks like that, and so on.

70
00:04:06,680 --> 00:04:10,280
或者你可以同时平行试验许多不同的模型
Or you might train many
different models in parallel,

71
00:04:10,280 --> 00:04:13,570
橙色的线就是不同的模型 对 是这样
where these orange lines
are different models, right, and so

72
00:04:13,570 --> 00:04:16,620
用这种方式你可以试验许多不同的参数设定
this way you can try a lot of
different hyperparameter settings and

73
00:04:16,620 --> 00:04:21,090
然后只是最后快速选择工作效果最好的那个
then just maybe quickly at the end
pick the one that works best.

74
00:04:21,090 --> 00:04:25,600
在这个例子中 也许这条看起来是最好的
Looks like in this example it was,
maybe this curve that look best.

75
00:04:25,600 --> 00:04:27,340
打个比方
So to make an analogy,

76
00:04:27,340 --> 00:04:30,760
我把左边的方法称为熊猫方式
I'm going to call the approach
on the left the panda approach.

77
00:04:30,760 --> 00:04:33,822
当熊猫有了孩子 他们的孩子非常少
When pandas have children,
they have very few children,

78
00:04:33,822 --> 00:04:35,507
一次通常只有一个
usually one child at a time, and

79
00:04:35,507 --> 00:04:40,350
然后他们花费很多精力抚养熊猫宝宝以确保其能成活
then they really put a lot of effort into
making sure that the baby panda survives.

80
00:04:40,350 --> 00:04:41,640
所以 这的确是一种照料
So that's really babysitting.

81
00:04:41,640 --> 00:04:44,280
一种模型类似于一只熊猫宝宝
One model for one baby panda.

82
00:04:44,280 --> 00:04:48,000
对比而言 右边的方式更像鱼类的行为
Whereas the approach on the right
is more like what fish do.

83
00:04:48,000 --> 00:04:50,380
我称之为鱼子酱方式
I'm going to call this
the caviar strategy.

84
00:04:50,380 --> 00:04:55,540
在交配季节 有些鱼类会产下一亿颗卵
There's some fish that lay over 100
million eggs in one mating season.

85
00:04:55,540 --> 00:04:58,960
但鱼类繁殖的方式是 它们会产很多卵
But the way fish reproduce is
they lay a lot of eggs and

86
00:04:58,960 --> 00:05:01,740
但不对其中任何一个多加照料
don't pay too much attention
to any one of them but

87
00:05:01,740 --> 00:05:05,970
只是希望其中一个 或其中一群 能够表现出色
just see that hopefully one of them, or
maybe a bunch of them, will do well.

88
00:05:05,970 --> 00:05:10,340
我猜 这就是哺乳动物繁衍和
So I guess, this is really
the difference between how mammals

89
00:05:10,340 --> 00:05:15,030
鱼类 很多爬虫类动物繁衍的区别
reproduce versus how fish and
a lot of reptiles reproduce.

90
00:05:15,030 --> 00:05:17,980
我将称之为熊猫方式与鱼子酱方式
But I'm going to call it the panda
approach versus the caviar approach,

91
00:05:17,980 --> 00:05:20,210
因为这很有趣 更容易记住
since that's more fun and memorable.

92
00:05:20,210 --> 00:05:23,050
所以 这两个方式的选择
So the way to choose between these
two approaches is really

93
00:05:23,050 --> 00:05:26,500
是由你拥有的计算资源决定的
a function of how much computational
resources you have.

94
00:05:26,500 --> 00:05:30,460
如果你拥有足够的计算机去平行试验许多模型
If you have enough computers to
train a lot of models in parallel,

95
00:05:31,920 --> 00:05:34,670
那绝对采取鱼子酱方式
then by all means take
the caviar approach and

96
00:05:34,670 --> 00:05:37,780
尝试许多不同的超参数 看效果怎样
try a lot of different hyperparameters and
see what works.

97
00:05:37,780 --> 00:05:42,520
但在一些应用领域 比如在线广告设置
But in some application domains, I see
this in some online advertising settings

98
00:05:42,520 --> 00:05:43,940
和计算机视觉应用领域
as well as in some computer vision applications,

99
00:05:43,940 --> 00:05:48,670
那里的数据太多了 需要试验大量的模型
where there's just so much data and
the models you want to train are so big

100
00:05:48,670 --> 00:05:53,220
所以同时试验大量的模型是很困难的
that it's difficult to train
a lot of models at the same time.

101
00:05:53,220 --> 00:05:55,640
它的确是依赖于应用的过程
It's really application
dependent of course,

102
00:05:55,640 --> 00:06:00,150
但我看到那些应用熊猫方式多一些的组织
but I've seen those communities use
the panda approach a little bit more,

103
00:06:00,150 --> 00:06:03,260
那里 你会像对婴儿一样照看一个模型
where you are kind of babying
a single model along and

104
00:06:03,260 --> 00:06:08,340
调试参数 试着让它工作运转
nudging the parameters up and down and
trying to make this one model work.

105
00:06:08,340 --> 00:06:12,165
尽管 当然 甚至是在熊猫方式中 试验一个模型
Although, of course, even the panda
approach, having trained one model and

106
00:06:12,165 --> 00:06:15,580
观察它工作与否 也许第二或第三个星期后
then seen it work or not work, maybe
in the second week or the third week,

107
00:06:15,580 --> 00:06:17,870
也许我应该建立一个不同的模型
maybe I should initialize a different model and

108
00:06:17,870 --> 00:06:23,910
像熊猫那样照料它 我猜 这样一生中可以培育几个孩子
then baby that one along just like even pandas, I guess, can have
multiple children in their lifetime,

109
00:06:23,910 --> 00:06:28,800
即使它们一次只有一个孩子 或孩子的数量很少
even if they have only one, or a very
small number of children, at any one time.

110
00:06:28,800 --> 00:06:30,888
所以希望你能学会
So hopefully this gives you a good sense of

111
00:06:30,888 --> 00:06:34,170
如何进行超参数的搜索过程
how to go about the hyperparameter search process.

112
00:06:34,170 --> 00:06:36,590
现在 还有另一种技巧
Now, it turns out that there's
one other technique

113
00:06:36,590 --> 00:06:41,360
能使你的神经网络变得更加坚实
that can make your neural network much more
robust to the choice of hyperparameters.

114
00:06:41,360 --> 00:06:44,000
它并不是对所有的神经网络都适用 但当适用时
It doesn't work for all neural networks,
but when it does,

115
00:06:44,000 --> 00:06:48,670
它可以使超参数搜索变得容易许多并加速试验过程
it can make the hyperparameter search much easier
and also make training go much faster.

116
00:06:48,670 --> 00:06:50,780
我们在下个视频中再讲解这个技巧
Let's talk about this
technique in the next video.

