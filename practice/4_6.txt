这周的前几个视频(字幕来源：网易云课堂)
in the earlier videos from this week

和之前几周的视频里
as well as from the videos from the past several weeks

你已经看到过
you've already seen the basic building blocks

正向反向传播的基础组成部分了
of forward propagation and back propagation

它们也是深度神经网络的重要组成部分
the key components you need to implement a deep neural network

现在我们来用它们建一个深度神经网络
let's see how you can put these components together to build a deep net

这是一个层数较少的神经网络 我们选择其中一层
Here's a network with a few layers let's pick one layer

从这一层的计算着手
and look at the computations focusing on just that layer for now

在第l层你有参数W^[l]和b^[l]
so for layer L you have some parameters W^[l] and b^[l]

正向传播里有输入的激活函数
and for the forward prop you will input the activations

输入是前一层a^[l-1] 输出是a^[l]
a^[l-1] from the previous layer and output a^[l]

我们之前讲过z^[l]
so the way we did this previously was you compute z^[l]

等于w^[l]乘以a^[l]减1加上b^[l]
equals w^[l] x a^[l-1] plus b^[l] um

a^[l]又等于g^[l](z^[l])
and then a^[l] equals g^[l](z^[l]) right

那么这就是你如何从输入a^[l-1]走到输出的
so that's how you go from the input al minus one to the output al

之后你就可以
and it turns out that for later use

把z^[l]的值缓存起来
will be useful to also cache the value z^[l]

我在这里也会把这包括在缓存中
so let me include this on cache as well because storing the value z^[l]

因为缓存的z^[l]对以后的正向反向传播的步骤非常有用
will be useful forward backward for the back propagation step later

然后是反向步骤 或者说反向传播步骤
and then for the backward step or three for the back propagation step

同样也是第l层的计算
again focusing on computation for this layer l

你会需要实现一个函数 输入为da^[l]
you're going to implement a function that inputs da^[l]

输出da^[l-1]的函数
and outputs da^[l-1]

一个小细节需要注意 输入在这里其实是da^[l]
and just refreshing the details the input is actually da^[l]

以及所缓存的z^[l]值
as well as the cache so you have available to you

之前计算好的z^[l]值
the value of z^[l] that you compute it

除了输出g^[l-1]的值以外
and in addition to outputting g^[l-1]

也需要输出你需要的梯度
you will output you know the gradients you want

这是为了实现梯度下降学习
in order to implement gradient descent for learning ok

这就是基本的正向步骤的结构
so this is the basic structure of how you implement this forward step

我把它成为称为正向函数
I'm going to call it a forward function

类似的在反向步骤中会称为反向函数
as well as backward step we shall call it backward function

总结起来就是
so just to summarize in layer l you're going to

在l层你会有正向函数
have you know the forward step or the forward prop forward function

输入a^[l-1]并且输出a^[l] 为了计算结果
input a^[l-1] and output a^[l] and in order to make this computation

你需要用W^[l]和b^[l]
you need to use W^[l] and b^[l] um

以及输出到缓存的z^[l]
and also output a cache which contains z^[l]

然后用作反向传播的反向函数
and then on the backward function using the back prop step

是另一个函数
will be another function

输入da^[l] 输出da^[l-1]
then now inputs da^[l] and outputs da^[l-1]

你就会得到对激活函数的导数
so it tells you given the derivatives respect to these activations

也就是da^[l] 这导数是多少呢? 我希望是什么?
that's da^[l] what are the derivatives or how much do i wish

a^[l-1]是会变的
you know a^[l-1] changes

前一层算出的激活函数导数
computed derivatives respect to the activations from the previous layer

在这个方块里你需要w^[l]和b^[l]
within this box right you need to use w^[l] and b^[l]

最后你要算的是dz^[l]
and it turns out along the way you end up computing dz^[l] um

然后这个方块中 这个反向函数
and then this box this backward function

可以计算输出dW^[l]和db^[l]
can also output dW^[l] and db^[l]

我会用红色箭头标注标注反向步骤
well now sometimes using red arrows to denote the backward iterations

如果你们喜欢 我可以把这些箭头涂成红色
so if you prefer we could draw these arrows in red

然后如果实现了这两个函数
so if you can implement these two functions

然后神经网络的计算过程会是这样的
then the basic computation of the neural network will be as follows

把输入特征a^[0]
you're going to take the input features a^[0]

放入第一层并计算第一层的激活函数
Feed that in and that will compute the activations of the first layer

用a^[1]表示 你需要w^[1]和b^[1]来计算
let's call that a^[1] and to do that you needed w^[1] and b^[1]

之后也缓存z^[l]值
and then we'll also you know cache the value z^[l]

之后喂到第二层
now having done that you feed that

第二层里 需要用到w^[2]和b^[2]
this is the second layer and then using w^[2] and b^[2]

你会需要计算第二层的激活函数a^[2] 后面几层以此类推
you're going to compute the activations our next layer a^[2]1 and so on

直到最后你算出了
until eventually you end up

a^[L]第L层的最终输出值y帽
outputting a capital l which is equal to y hat

在这些过程里我们缓存了所有的z值
and along the way we cashed all of these on values z

这就是正向传播的步骤
so that's the forward propagation step

对反向传播的步骤而言
now for the back propagation step what we're going to do

我们需要算一系列的反向迭代
will be a backward sequence of iterations

就是这样反向计算梯度
in which you're going backwards and computing gradients like so

你需要把da^[l]的值放在这里
so as you're going to feed in here da^[l]

然后这个方块会给我们da^[l-1]的值 以此类推
and then this box will give us da^[l-1] and so on

直到我们得到da^[2]和da^[1]
until we get da^[2] da^[1]

你还可以计算多一个输出值 就是da^[0]
you could actually get one more output to compute da^[0]

但这其实是你的输入特征的导数 并不重要
but this is derivative respect your input features which is not useful

起码对于训练监督学习的权重不算重要
at least for training the weights of these are supervised neural networks

你可以止步于此
so you could just stop it there

反向传播步骤中也会输出dw^[l]和db^[l]
along the way back prop also ends up outputting dw^[l] db^[l] right

这个会输出W^[L]和b^[L]
this used upon so W^[L] and b^[L]

这会输出dw^[3]和db^[3]等等
this would output dw^[3] db^[3] and so on

目前为止你算好了所有需要的导数
so you end up computing all the derivatives you need

稍微填一下这个流程图
just a maybe to fill in the structure a little bit more right

这些方块需要用到参数w^[l]和b^[l]
these boxes will use those parameters of slow w^[l] b^[l]

我们之后会看到在这些方块中
and it turns out that we'll see later

在这些方块里 我们最后会计算dz
that inside these boxes we'll end up computing dz as well

神经网络的一步训练 包含了
so one iteration of training for a neural network involves

从a^[0]开始
starting with a^[0]

也就是x 然后经过一系列正向传播计算得到y帽
which is x and going through for profit as follows computing y hat

之后再用输出值计算这个
and then using that to compute this

再实现反向传播
and then back prop right doing that

现在你就有所有的导数项了
and now you have all these derivative terms

w也会在每一层被更新为
and so you know w will get updated as

每一层里 W减去学习率乘以dW
some w minus the learning rate times d^[w] right for each of the layers

b也一样
and similarly for b right

反向传播就都计算完毕 我们有所有的导数值
now we've compute the back prop and have all these derivatives

那么这是神经网络一个梯度下降循环
so that's one iteration of gradient descent for your neural network

继续下去之前再补充一个细节
now before moving on just one more implementational detail

概念上会非常有帮助
conceptually will be useful to

那就是把反向函数
think of the cache here as storing

计算出来的z值缓存下来
the value of Z for the backward functions

当你做编程练习的时候去实现它时
but when you implement this you see this in the programming exercise

你会发现缓存可能很方便
when you implement it you find that the cache may be a convenient way

可以在分享传播函数迅速得到W^[1]和b^[1]的值
to get the value of the parameters at W^[1] b^[1]

非常方便的一个方法
into the backward function as well

在编程练习中你缓存了z
so the program exercise you actually store the cache is z

还有W和b 对吧
as well as W and b all right

缓存z^[2] W^[2] 从实现角度上看
so to store z^[2] W^[2] be to go from an implementational standpoint

我认为是一个很方便的方法
I just find this a convenient way

可以将参数复制到
to just you know get the parameters copied to

你在计算反向传播时
where you need to need to use them later

所需要的地方
when you're computing back propagation

好 这就是实现过程的细节
so that's just an implementational detail that you see

做编程练习时会用到
when you do the programming exercise

现在你们见过
so you've now seen one of the basic building blocks

实现深度神经网络的基本元件
for implementing a deep neural network

在每一层中 有一个正向传播步骤
In each layer there's a forward propagation step

以及对应的反向传播步骤
and there's a corresponding backward propagation step

以及把信息从一步传递到另一步的缓存
and as cache deposit information from one to the other

下一个视频我们会
in the next video we'll talk about

这些元件具体实现过程
how you can actually implement these building blocks

我们来看下一个视频吧
let's go into the next video

