1
00:00:00,000 --> 00:00:01,840
在深度学习研究早期(字幕来源：网易云课堂)
In the early days of deep learning,

2
00:00:01,840 --> 00:00:04,711
人们总是担心优化算法
people used to worry a lot about the optimization algorithm

3
00:00:04,711 --> 00:00:07,415
会困在极差的局部最优
getting stuck in bad local optima.

4
00:00:07,415 --> 00:00:09,660
不过随着深度学习理论不断发展
But as this theory of deep learning has advanced,

5
00:00:09,660 --> 00:00:13,285
我们对局部最优的理解也发生了改变
our understanding of local optima is also changing.

6
00:00:13,280 --> 00:00:17,189
我向你展示一下现在我们怎么看局部最优
Let me show you how we now think about local optima

7
00:00:17,189 --> 00:00:21,279
以及深度学习中的优化问题
and problems in the optimization problem in deep learning.

8
00:00:21,270 --> 00:00:23,980
这是曾经人们在想到局部最优时
This was a picture people used to have in mind

9
00:00:23,980 --> 00:00:25,695
脑海里会出现的图
when they worried about local optima.

10
00:00:25,695 --> 00:00:28,786
也许你想优化一些参数
Maybe you are trying to optimize some set of parameters,

11
00:00:28,786 --> 00:00:30,580
我们把它们称之为W_1和W_2
we call them W_1 and W_2,

12
00:00:30,580 --> 00:00:33,268
平面的高度就是损失函数
and the height in the surface is the cost function.

13
00:00:33,260 --> 00:00:34,810
在图中
In this picture,

14
00:00:34,810 --> 00:00:38,655
似乎各处都分布着局部最优
it looks like there are a lot of local optima in all those places.

15
00:00:38,655 --> 00:00:41,010
梯度下降法
And it'd be easy for gradient descent,

16
00:00:41,010 --> 00:00:44,200
或者某个算法可能困在一个局部最优中
or one of the other algorithms to get stuck in a local optimum

17
00:00:44,200 --> 00:00:47,226
而不会抵达全局最优
rather than find its way to a global optimum.

18
00:00:47,220 --> 00:00:50,337
如果你要作图计算一个数字
It turns out that if you are plotting a figure

19
00:00:50,337 --> 00:00:51,945
比如说这两个维度
like this in two dimensions,

20
00:00:51,940 --> 00:00:54,360
就容易出现
then it's easy to create plots like this

21
00:00:54,360 --> 00:00:56,637
有多个不同局部最优的图
with a lot of different local optima.

22
00:00:56,630 --> 00:00:58,696
而这些低维的图
And these very low dimensional plots

23
00:00:58,696 --> 00:01:00,285
曾经影响了我们的理解used to guide their intuition.

24
00:01:00,285 --> 00:01:02,730
但是这些理解并不正确
But this intuition isn't actually correct.

25
00:01:02,730 --> 00:01:04,878
事实上 如果你要创建一个神经网络
It turns out if you create a neural network,

26
00:01:04,870 --> 00:01:07,060
通常梯度为零的点
most points of zero gradients

27
00:01:07,060 --> 00:01:09,965
并不是这个图中的局部最优点
are not local optima like points like this.

28
00:01:09,960 --> 00:01:12,874
实际上成本函数的零梯度点
Instead most points of zero gradient

29
00:01:12,874 --> 00:01:15,330
通常是鞍点
in a cost function are saddle points.

30
00:01:15,330 --> 00:01:16,700
也就是在这个点
So, that's a point

31
00:01:17,840 --> 00:01:20,171
这里是W_1 W_2
where this t is maybe W_1, W_2,

32
00:01:20,171 --> 00:01:25,150
高度即成本函数J的值
and the height is the value of the cost function J.

33
00:01:25,150 --> 00:01:28,523
但是一个具有高维空间的函数
But informally, a function of very high dimensional space,

34
00:01:28,523 --> 00:01:30,075
如果梯度为0
if the gradient is zero,

35
00:01:30,070 --> 00:01:31,360
那么在每个方向
then in each direction

36
00:01:31,360 --> 00:01:34,555
它可能是凸函数
it can either be a convex light function

37
00:01:34,555 --> 00:01:36,810
也可能是凹函数
or a concave light function.

38
00:01:36,810 --> 00:01:40,785
如果你在2万维空间中
And if you are in, say, a 20,000 dimensional space,

39
00:01:40,785 --> 00:01:42,510
那么要想得到局部最优
then for it to be a local optima,

40
00:01:42,510 --> 00:01:45,795
所有的2万个方向都需要是这样
all 20,000 directions need to look like this.

41
00:01:45,795 --> 00:01:49,274
但发生的机率也许很小
And so the chance of that happening is maybe very small,

42
00:01:49,274 --> 00:01:51,564
也许是2^(-20000)
maybe two to the minus 20,000.

43
00:01:51,560 --> 00:01:54,500
你更有可能遇到
Instead you're much more likely to get some directions

44
00:01:54,500 --> 00:01:57,945
有些方向的曲线会这样向上弯曲
where the curve bends up like so,

45
00:01:57,940 --> 00:02:01,577
另一些方向曲线向下湾
as well as some directions where the curve function is bending down

46
00:02:01,577 --> 00:02:04,720
而不是所有的都向上弯曲
rather than have them all bend upwards.

47
00:02:04,720 --> 00:02:07,128
因此在高维度空间
So that's why in very high-dimensional spaces

48
00:02:07,120 --> 00:02:09,623
你更有可能碰到鞍点
you're actually much more likely to run into a saddle point

49
00:02:09,620 --> 00:02:11,413
就像右边的这种
like that shown on the right,

50
00:02:11,410 --> 00:02:13,081
而不会碰到局部最优
then the local optimum.

51
00:02:13,081 --> 00:02:16,305
至于为什么把一个平面叫作鞍点
As for why the surface is called a saddle point,

52
00:02:16,305 --> 00:02:17,545
你想象一下
if you can picture,

53
00:02:17,540 --> 00:02:19,899
这就像是放在
maybe this is a sort of saddle

54
00:02:19,899 --> 00:02:21,398
马背部的马鞍一样
you put on a horse, right?

55
00:02:21,398 --> 00:02:23,165
如果这是马
Maybe this is a horse.

56
00:02:23,165 --> 00:02:24,540
这是马的头
This is a head of a horse,

57
00:02:24,540 --> 00:02:26,528
这就是马的眼睛
this is the eye of a horse.

58
00:02:30,634 --> 00:02:33,230
画得不好 请多包涵
Well, not a good drawing of a horse but you get the idea.

59
00:02:33,235 --> 00:02:34,530
然后你是骑马的人
Then you, the rider,

60
00:02:34,530 --> 00:02:37,272
要坐在马鞍上
will sit here in the saddle.

61
00:02:39,222 --> 00:02:41,580
因此这里的这个
So that's why this point here,

62
00:02:41,585 --> 00:02:43,445
导数为0的点
where the derivative is zero,

63
00:02:43,445 --> 00:02:47,480
这个点叫做鞍点
that point is called a saddle point.

64
00:02:47,480 --> 00:02:49,216
我想那确实是
There's really the point on this saddle

65
00:02:49,216 --> 00:02:50,370
你坐在马鞍上的那一点
where you would sit, I guess,

66
00:02:50,370 --> 00:02:53,480
而这里导数为0
and that happens to have derivative zero.

67
00:02:53,480 --> 00:02:55,410
所以我们从
And so, one of the lessons

68
00:02:55,410 --> 00:02:56,820
深度学习历史中学到的一课就是
we learned in history of deep learning

69
00:02:56,820 --> 00:02:59,790
我们对低维度空间的大部分直觉
is that a lot of our intuitions about low-dimensional spaces,

70
00:02:59,790 --> 00:03:01,235
比如你可以画出左边的图
like what you can plot on the left,

71
00:03:01,230 --> 00:03:04,962
并不能应用到高维度空间中
they really don't transfer to the very high-dimensional spaces

72
00:03:04,962 --> 00:03:07,695
适用于其他算法
that any other algorithms are operating over.

73
00:03:07,695 --> 00:03:10,860
因为如果你有2万个参数
Because if you have 20,000 parameters,

74
00:03:10,860 --> 00:03:14,635
那么J函数有2万个维度向量
then J as your function over 20,000 dimensional vector,

75
00:03:14,630 --> 00:03:16,685
你更有可能遇到鞍点
then you're much more likely to see saddle points

76
00:03:16,685 --> 00:03:17,964
而不是局部最优点
than local optimum.

77
00:03:17,964 --> 00:03:20,265
如果局部最优不是问题
If local optima aren't a problem,

78
00:03:20,265 --> 00:03:22,002
那么问题是什么？
then what is a problem?

79
00:03:22,000 --> 00:03:25,697
结果是平稳段会减缓学习
It turns out that plateaus can really slow down learning

80
00:03:25,697 --> 00:03:27,700
平稳段是一块区域
and a plateau is a region

81
00:03:27,700 --> 00:03:31,635
其中导数长时间接近于0
where the derivative is close to zero for a long time.

82
00:03:31,635 --> 00:03:33,915
如果你在此处
So if you're here,

83
00:03:33,915 --> 00:03:38,230
梯度会从平面从上向下下降
then gradient descents will move down the surface,

84
00:03:38,230 --> 00:03:41,250
因为梯度等于或接近0
and because the gradient is zero or near zero,

85
00:03:41,250 --> 00:03:42,829
平面很水平
the surface is quite flat.

86
00:03:42,829 --> 00:03:45,300
你得花上很长时间
You can actually take a very long time, you know,

87
00:03:45,300 --> 00:03:51,555
慢慢抵达平稳段的这个点
to slowly find your way to maybe this point on the plateau.

88
00:03:51,555 --> 00:03:53,820
因为左边或右边的随机扰动
And then because of a random perturbation of left or right,

89
00:03:53,820 --> 00:03:57,870
我换个笔墨颜色 大家看得清楚一些
maybe then finally I'm going to switch pen colors for clarity.

90
00:03:57,870 --> 00:04:00,745
然后你的算法能够走出平稳段
Your algorithm can then find its way off the plateau.

91
00:04:00,740 --> 00:04:03,380
我们可以沿着这段长坡走
Let it take this very long slope off

92
00:04:03,380 --> 00:04:05,240
直到这里
before it's found its way here

93
00:04:05,240 --> 00:04:09,130
然后走出平稳段
and they could get off this plateau.

94
00:04:09,130 --> 00:04:11,340
所以此次视频的要点是
So the takeaways from this video are,

95
00:04:11,340 --> 00:04:13,125
首先 你不太可能
first, you're actually pretty unlikely

96
00:04:13,120 --> 00:04:14,500
困在极差的局部最优中
to get stuck in bad local optima,

97
00:04:14,500 --> 00:04:17,150
条件是你在训练较大的神经网络
so long as you're training a reasonably large neural network,

98
00:04:17,150 --> 00:04:18,555
存在大量参数
so a lot of parameters,

99
00:04:18,550 --> 00:04:20,306
并且成本函数J
and the cost function J is

100
00:04:20,306 --> 00:04:23,180
被定义在较高的维度空间
defined over a relatively high dimensional space.

101
00:04:23,180 --> 00:04:25,780
第二点 平稳段是一个问题
But second, that plateaus are a problem

102
00:04:25,780 --> 00:04:28,416
这样使得学习十分缓慢
and you can actually make learning pretty slow.

103
00:04:28,410 --> 00:04:31,078
这也是像Momentum
And this is where algorithms like momentum

104
00:04:31,070 --> 00:04:32,574
或是RmsProp Adam这样的算法
or RmsProp or Adam

105
00:04:32,574 --> 00:04:35,985
能够加速学习算法的地方
can really help your learning algorithm as well.

106
00:04:35,980 --> 00:04:37,514
在这些情况下
And these are scenarios

107
00:04:37,510 --> 00:04:39,922
更成熟的观察算法
where more sophisticated observation algorithms,

108
00:04:39,922 --> 00:04:40,855
如Adam算法
such as Adam,

109
00:04:40,850 --> 00:04:43,080
能够加快速度
can actually speed up the rate

110
00:04:43,080 --> 00:04:46,720
让你尽早往下走出平稳段
at which you could move down the plateau and then get off the plateau.

111
00:04:46,720 --> 00:04:50,068
因为你的网络要解决优化问题
So because your network is solving optimizations problems

112
00:04:50,068 --> 00:04:53,055
说实话 要面临如此之高的维度空间
over such high dimensional spaces, to be honest,

113
00:04:53,055 --> 00:04:55,260
我觉得没有人有那么好的直觉
I don't think anyone has great intuitions

114
00:04:55,260 --> 00:04:57,445
知道这些空间长什么样
about what these spaces really look like,

115
00:04:57,445 --> 00:04:59,910
而且我们对它们的理解还在不断发展
and our understanding of them is still evolving.

116
00:04:59,910 --> 00:05:02,785
不过我希望这一点能够让你更好地理解
But I hope this gives you some better intuition about

117
00:05:02,785 --> 00:05:06,660
优化算法所面临的问题
the challenges that the optimization algorithms may face.

118
00:05:06,660 --> 00:05:08,508
恭喜你学会了
So that's congratulations on

119
00:05:08,508 --> 00:05:11,100
本周所有的内容
coming to the end of this week's content.

120
00:05:11,100 --> 00:05:15,275
麻烦你做一下本周的小测以及练习
Please take a look at this week's quiz as well as the exercise.

121
00:05:15,270 --> 00:05:18,960
希望你能学以致用并乐在其中
I hope you enjoy practicing some of these ideas of this week exercise,

122
00:05:18,960 --> 00:05:23,000
期待你观看下周的视频
and I look forward to seeing you at the start of next week's videos.

