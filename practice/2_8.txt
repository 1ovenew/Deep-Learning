1
00:00:00,000 --> 00:00:02,340
在深度学习的历史上(字幕来源：网易云课堂)
During the history of deep learning,

2
00:00:02,340 --> 00:00:05,700
包括许多知名研究者在内
many researchers including some very well-known researchers,

3
00:00:05,700 --> 00:00:07,790
提出了优化算法
sometimes proposed optimization algorithms

4
00:00:07,790 --> 00:00:09,825
并很好地解决了一些问题
and showed that they worked well in a few problems.

5
00:00:09,820 --> 00:00:13,017
但随后这些优化算法被指出
But those optimization algorithms subsequently were shown

6
00:00:13,017 --> 00:00:14,780
并不能一般化
not to really generalize that well

7
00:00:14,780 --> 00:00:18,130
并不适用于多种神经网络
to the wide range of neural networks you might want to train.

8
00:00:18,130 --> 00:00:21,360
时间久了 深度学习圈子里的人开始
So over time, I think the deep learning community actually developed

9
00:00:21,360 --> 00:00:25,597
多少有些质疑全新的优化算法
some amount of skepticism about new optimization algorithms.

10
00:00:25,590 --> 00:00:26,960
很多人都觉得
And a lot of people felt that

11
00:00:26,960 --> 00:00:29,620
Momentum梯度下降法很好用
gradient descent with momentum really works well,

12
00:00:29,620 --> 00:00:32,720
很难再想出更好的优化算法
was difficult to propose things that work much better.

13
00:00:32,720 --> 00:00:36,540
所以RMSprop以及Adam优化算法
So,  RMSprop and the Adam optimization algorithm,

14
00:00:36,540 --> 00:00:37,730
Adam优化算法也是本视频的内容
which we'll talk about in this video,

15
00:00:37,730 --> 00:00:41,460
就是少有的经受住人们考验的两种算法
is one of those rare algorithms that has really stood up,

16
00:00:41,460 --> 00:00:42,780
已被证明
and has been shown to

17
00:00:42,780 --> 00:00:46,860
适用于不同的深度学习结构
work well across a wide range of deep learning architectures.

18
00:00:46,860 --> 00:00:48,300
这个算法
So, this is one of the algorithms

19
00:00:48,300 --> 00:00:50,150
我会毫不犹豫地推荐给你
that I wouldn't hesitate to recommend you try

20
00:00:50,150 --> 00:00:51,980
因为很多人都试过
because many people have tried it

21
00:00:51,980 --> 00:00:54,705
并且用它很好地解决了许多问题
and seen it work well on many problems.

22
00:00:54,700 --> 00:00:57,342
Adam优化算法基本上就是
And the Adam optimization algorithm is basically

23
00:00:57,342 --> 00:01:01,250
将Momentum和RMSprop结合在一起
taking momentum and RMSprop and putting them together.

24
00:01:01,250 --> 00:01:03,105
那么来看看如何使用Adam算法
So, let's see how that works.

25
00:01:03,105 --> 00:01:05,695
使用Adam算法 首先你要初始化
To implement Adam you would initialize:

26
00:01:05,690 --> 00:01:15,521
V_dW=0 S_dW=0 V_db=0 S_db=0
V_dW=0, S_dW=0, and similarly V_db, S_db=0.

27
00:01:15,521 --> 00:01:19,810
在第t次迭代中
And then on iteration t,

28
00:01:19,810 --> 00:01:22,460
你要计算微分
you would compute the derivatives:

29
00:01:22,460 --> 00:01:27,844
用当前的mini-batch计算dW db
compute dW, db using current mini-batch.

30
00:01:28,860 --> 00:01:33,770
一般你会用mini-batch梯度下降法
So usually, you do this with mini-batch gradient descent.

31
00:01:33,770 --> 00:01:38,980
接下来计算momentum指数加权平均数
And then you do the momentum exponentially weighted average.

32
00:01:38,980 --> 00:01:41,480
所以V_dW=β
So V_dW = ß.

33
00:01:41,480 --> 00:01:43,340
现在我要用β_1
But now I'm going to call this β_1

34
00:01:43,340 --> 00:01:46,960
这样就不会跟超参数β_2混淆
to distinguish it from the hyperparameter β_2

35
00:01:46,960 --> 00:01:52,660
因为后面RMSprop要用到β_2
we'll use for the RMSprop proportion of this.

36
00:01:52,660 --> 00:01:55,965
使用Momentum时
So, this is exactly what we had

37
00:01:55,965 --> 00:01:58,760
我们肯定会用这个公式
when we're implementing momentum,

38
00:01:58,760 --> 00:02:03,788
但现在不叫它β 而叫它β_1
except it now called hyper parameter β_1 instead of β.

39
00:02:03,788 --> 00:02:14,312
同样V_db等于(1-β_1)*db
And similarly, you have V_db as follows: 1 minus β_1 times db.

40
00:02:14,312 --> 00:02:18,685
接着你用RMSprop进行更新
And then you do the RMSprop update as well.

41
00:02:18,680 --> 00:02:21,448
即用不同的超参数β_2
So now, you have a different hyperparemeter β_2

42
00:02:21,448 --> 00:02:26,391
加上(1-β_2)*dW^2
plus one minus β_2 dW squared.

43
00:02:26,391 --> 00:02:27,070
再说一次
Again,

44
00:02:27,070 --> 00:02:33,325
这里是对整个微分dW进行平方处理
the squaring there is element-wise squaring of your derivatives dW.

45
00:02:33,320 --> 00:02:43,017
S_db等于这个加上(1-β_2)*db^2
And then S_db is equal to this plus one minus β_2 times db.

46
00:02:43,017 --> 00:02:49,600
相当于Momentun更新了超参数β_1
So this is the momentum like update with hyperparameter β_1

47
00:02:49,600 --> 00:02:55,318
RMSprop更新了超参数β_2
and this is the RMSprop like update with hyperparameter β_2.

48
00:02:55,318 --> 00:02:58,599
一般使用Adam算法的时候
In the typical implementation of Adam,

49
00:02:58,599 --> 00:03:01,255
要计算偏差修正
you do implement bias correction.

50
00:03:01,255 --> 00:03:04,215
修正的V_dW
So you're going to have V corrected.

51
00:03:04,215 --> 00:03:06,705
修正也就是在偏差修正之后
Corrected means after bias correction.

52
00:03:06,700 --> 00:03:13,960
等于V_dW/(1-β_1^t)
dW = V_dW divided by 1 minus ß1 to the power of t

53
00:03:13,960 --> 00:03:16,244
t是迭代次数
if you've done t iterations.

54
00:03:16,240 --> 00:03:20,731
同样修正的V_db等于
And similarly, V_db corrected equals

55
00:03:20,731 --> 00:03:25,040
V_db/(1-β_1^t)
V_db divided by 1 minus β_1 to the power of t.

56
00:03:25,040 --> 00:03:30,756
S也使用偏差修正
And then similarly, you implement this bias correction on S as well.

57
00:03:30,750 --> 00:03:36,220
也就是S_dW/(1-β_1^t)
So, that's S_dW divided by one minus β_2 to the t,

58
00:03:36,220 --> 00:03:48,700
修正的S_db等于S_db/(1-β_2^t)
and S_db corrected equals S_db divided by 1 minus β_2 to the t.

59
00:03:48,700 --> 00:03:50,660
最后更新权重
Finally, you perform the update.

60
00:03:50,660 --> 00:03:55,060
所以W更新后是W减去α乘以
So W gets updated as W minus alpha times.

61
00:03:55,060 --> 00:03:57,136
如果你只用Momentum
So if you're just implementing momentum

62
00:03:57,136 --> 00:04:03,408
你就用V_dW或者修正后的V_dW
you'd use V_dW, or maybe V_dW corrected.

63
00:04:03,408 --> 00:04:06,615
但现在我们加入了RMSprop的部分
But now, we add in the RMSprop portion of this.

64
00:04:06,610 --> 00:04:07,626
所以我们要
So we're also going to

65
00:04:07,626 --> 00:04:13,390
除以修正后S_dW的平方根加上ε
divide by square roots of S_dW corrected plus epsilon.

66
00:04:13,390 --> 00:04:18,232
根据类似的公式更新b值
And similarly, b gets updated as a similar formula,

67
00:04:18,230 --> 00:04:28,595
修正V_db除以修正后S_db的平方根加上ε
V_db corrected, divided by square root S, corrected db, plus epsilon.

68
00:04:28,590 --> 00:04:33,951
所以Adam算法结合了
And so, this algorithm combines the effect of gradient descent with momentum

69
00:04:33,950 --> 00:04:37,093
Momentum和RMSprop梯度下降法
together with gradient descent with rRMSprop.

70
00:04:37,090 --> 00:04:40,337
并且是一种极其常用的学习算法
And this is a commonly used learning algorithm

71
00:04:40,330 --> 00:04:43,871
被证明能有效适用于不同神经网络
that is proven to be very effective for many different neural networks

72
00:04:43,871 --> 00:04:46,640
适用于广泛的结构
of a very wide variety of architectures.

73
00:04:46,640 --> 00:04:49,805
本算法中有很多超参数
So, this algorithm has a number of hyperparameters.

74
00:04:49,800 --> 00:04:54,560
超参数学习率α很重要
The learning rate hyper parameteralpha is still important

75
00:04:54,560 --> 00:04:57,330
也经常需要调试
and usually needs to be tuned.

76
00:04:57,330 --> 00:04:59,860
你可以尝试一系列值
So you just have to try a range of values

77
00:04:59,860 --> 00:05:01,675
然后看哪个有效
and see what works.

78
00:05:01,675 --> 00:05:06,090
β_1常用的缺省值为0.9
A common choice really the default choice for β_1 is 0.9.

79
00:05:06,090 --> 00:05:08,065
这是dW的移动平均数
So this is a moving average,

80
00:05:08,060 --> 00:05:10,471
也就是dW的加权平均数
weighted average of dW right?

81
00:05:10,470 --> 00:05:12,324
这是momentum涉及的项
this is the momentum light term,

82
00:05:12,324 --> 00:05:15,455
至于超参数β_2
the hyperparameter for β_2,

83
00:05:15,450 --> 00:05:17,342
Adam论文的作者
the authors of the Adam paper,

84
00:05:17,340 --> 00:05:19,192
也就是Adam算法的发明者
inventors of the Adam algorithm

85
00:05:19,192 --> 00:05:20,780
推荐使用0.999
recommend 0.999.

86
00:05:20,780 --> 00:05:23,691
这是在计算dW^2以及db^2的
Again this is computing the moving weighted average of

87
00:05:23,691 --> 00:05:26,485
移动加权平均值
dW squared as well as db squared.

88
00:05:26,485 --> 00:05:31,030
关于ε的选择其实没那么重要
And then Epsilon, the choice of epsilon doesn't matter very much.

89
00:05:31,030 --> 00:05:34,755
Adam论文的作者建议ε为10^(-8)
But the authors of the Adam paper recommended it 10 to the minus 8.

90
00:05:34,750 --> 00:05:38,994
但你并不需要设置它
But this parameter you really don't need to set it

91
00:05:38,994 --> 00:05:42,555
因为它并不会影响算法表现
and it doesn't affect performance much at all.

92
00:05:42,555 --> 00:05:44,280
但是在使用Adam的时候
But when implementing Adam,

93
00:05:44,280 --> 00:05:47,030
人们往往用缺省值即可
what people usually do is just use the default value.

94
00:05:47,030 --> 00:05:49,960
β_1 β_2和ε都是如此
So, β_1 and β_2 as well as epsilon.

95
00:05:49,960 --> 00:05:52,300
我觉得没人会去调整ε
I don't think anyone ever really tunes Epsilon.

96
00:05:52,300 --> 00:05:55,200
然后尝试不同的α值
And then, try a range of values of Alpha

97
00:05:55,200 --> 00:05:56,644
看看哪个效果最好
to see what works best.

98
00:05:56,640 --> 00:05:58,602
你也可以调整β_1和β_2
You could also tune β_1 and β_2

99
00:05:58,602 --> 00:06:02,440
但我认识的业内人士很少人这么干
but it's not done that often among the practitioners I know.

100
00:06:02,440 --> 00:06:06,100
为什么这个算法叫做Adam？
So, where does the term 'Adam' come from?

101
00:06:06,100 --> 00:06:15,267
Adam代表的是Adaptive Moment Estimation
Adam stands for Adaptive Moment Estimation.

102
00:06:15,260 --> 00:06:18,152
β_1用于计算这个微分
So β_1 is computing the mean of the derivatives.

103
00:06:18,152 --> 00:06:19,780
叫做第一矩
This is called the first moment.

104
00:06:19,780 --> 00:06:24,400
β_2用来计算平方数的指数加权平均数
And β_2 is used to compute exponentially weighted average of the squares,

105
00:06:24,400 --> 00:06:25,830
叫做第二矩
and that's called the second moment.

106
00:06:25,830 --> 00:06:29,380
所以Adam的名字由此而来
So that gives rise to the name adaptive moment estimation.

107
00:06:29,380 --> 00:06:32,875
但是大家都简称为Adam权威算法
But everyone just calls it the Adam authorization algorithm.

108
00:06:32,870 --> 00:06:36,460
顺便提一下 我有一个老朋友兼合作伙伴
And, by the way, one of my long term friends and collaborators

109
00:06:36,460 --> 00:06:37,800
叫做Adam Coates
is call Adam Coates.

110
00:06:37,800 --> 00:06:38,577
据我所知
As far as I know,

111
00:06:38,577 --> 00:06:40,425
他跟Adam算法没有任何关系
this algorithm doesn't have anything to do with him,

112
00:06:40,420 --> 00:06:43,254
不过我觉得他偶尔会用到这个算法
except for the fact that I think he uses it sometimes.

113
00:06:43,254 --> 00:06:45,847
不过有时有人会问我这个问题
But sometimes I get asked that question,

114
00:06:45,847 --> 00:06:47,945
我想你可能也有相同的疑惑
so just in case you're wondering.

115
00:06:47,940 --> 00:06:50,663
这就是有关Adam优化算法的全部内容
So, that's it for the Adam optimization algorithm.

116
00:06:50,663 --> 00:06:54,435
有了它 你可以更加地训练神经网络
With it, I think you really train your neural networks much more quickly.

117
00:06:54,435 --> 00:06:56,055
在结束本周课程之前
But before we wrap up for this week,

118
00:06:56,055 --> 00:06:58,950
我们还要讲一下超参数调整
let's keep talking about hyperparameter tuning,

119
00:06:58,950 --> 00:07:01,306
以及更好地理解
as well as gain some more intuitions about

120
00:07:01,306 --> 00:07:04,230
神经网络的优化问题有哪些
what the optimization problem for neural networks looks like.

121
00:07:04,230 --> 00:07:07,260
下个视频中 我们将讲讲学习率衰减
In the next video, we'll talk about learning rate decay.

