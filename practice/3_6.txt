1
00:00:00,000 --> 00:00:02,415
为什么batch归一化会起作用呢？(字幕来源：网易云课堂)
So, why does batch norm work?

2
00:00:02,415 --> 00:00:07,680
一个原因是 你已经看到如何归一化输入特征值x
Here's one reason, you've seen how normalizing the input features,the x's,

3
00:00:07,680 --> 00:00:09,380
使其均值为0 方差为1
to mean zero and variance one,

4
00:00:09,380 --> 00:00:10,740
它又是怎样加速学习的
how that can speed up learning.

5
00:00:10,740 --> 00:00:13,920
与其有一些从0到1
So rather than having some features that range from zero to one,

6
00:00:13,920 --> 00:00:15,735
从1到1000的特征值
and some from one to a 1,000,

7
00:00:15,735 --> 00:00:18,835
通过归一化所有的输入特征值x
by normalizing all the features, input features x,

8
00:00:18,835 --> 00:00:22,975
以获得类似范围的值 可加速学习
to take on a similar range of values that can speed up learning.

9
00:00:22,975 --> 00:00:25,770
所以 batch归一化起作用的原因 直观的一点是
So, one intuition behind why batch norm works is,

10
00:00:25,770 --> 00:00:27,750
它在做类似的工作
this is doing a similar thing,

11
00:00:27,750 --> 00:00:32,770
但不仅仅对于这里的输入值 还有隐藏单元的值
but for the values in your hidden units and not just for your input there.

12
00:00:32,770 --> 00:00:37,380
这只是batch归一化作用的冰山一角
Now, this is just a partial picture for what batch norm is doing.

13
00:00:37,380 --> 00:00:39,180
还有些更深层的原理
There are a couple of further intuitions,

14
00:00:39,180 --> 00:00:43,210
它会有助于你对batch归一化的作用有更深的理解
that will help you gain a deeper understanding of what batch norm is doing.

15
00:00:43,210 --> 00:00:46,320
让我们一起来看看吧
Let's take a look at those in this video.

16
00:00:46,320 --> 00:00:48,490
batch归一化有效的第二个原因是
A second reason why batch norm works,

17
00:00:48,490 --> 00:00:52,575
它可以使权重 比你的网络更滞后或更深层
is it makes weights,later or deeper than your network,

18
00:00:52,575 --> 00:00:56,095
比如 第10层的权重更能经受得住变化
say the weight on layer 10, more robust to changes to

19
00:00:56,095 --> 00:01:00,300
相比于神经网络中前层的权重 比如层一
weights in earlier layers of the neural network, say, in layer one.

20
00:01:00,300 --> 00:01:01,810
为了解释我的意思
To explain what I mean,

21
00:01:01,810 --> 00:01:04,775
让我们来看看这个最生动形象的例子
let's look at this most vivid example.

22
00:01:04,775 --> 00:01:06,420
这是一个网络的训练
Let's see a training on network,

23
00:01:06,420 --> 00:01:07,770
也许是个浅层网络
maybe a shallow network,

24
00:01:07,770 --> 00:01:11,715
比如logistic回归或是一个神经网络
like logistic regression or maybe a neural network,

25
00:01:11,715 --> 00:01:17,260
也许是个浅层网络 像这个回归函数 或一个深层网络
maybe a shallow network like this regression or maybe a deep network,

26
00:01:17,260 --> 00:01:21,120
建立在我们著名的猫脸识别检测上
on our famous cat detection toss.

27
00:01:21,120 --> 00:01:26,745
但假设你已经在所有黑猫的图像上训练了数据集
But let's say that you've trained your data sets on all images of black cats.

28
00:01:26,745 --> 00:01:33,125
如果现在你要把此网络应用于有色猫
If you now try to apply this network to data with colored cats

29
00:01:33,125 --> 00:01:36,895
这种情况下 正面的例子不只是左边的黑猫
where the positive examples are not just black cats like on the left,

30
00:01:36,895 --> 00:01:40,442
还有右边其它颜色的猫
but to color cats like on the right,

31
00:01:40,442 --> 00:01:43,160
那么你的cosfa可能适用的不会很好
then your cosfa might not do very well.

32
00:01:43,160 --> 00:01:47,907
如果 图像中 你的训练集是这个样子的
So in pictures, if your training set looks like this,

33
00:01:47,907 --> 00:01:52,383
你的正面例子在这儿 反面例子在那儿
where you have positive examples here and negative examples here,

34
00:01:52,383 --> 00:01:55,970
但你试图把它们都统一于一个数据集
but you were to try to generalize it to a data set

35
00:01:55,970 --> 00:02:02,335
也许正面例子在这儿 反面例子在那儿
where maybe positive examples are here and the negative examples are here,

36
00:02:02,335 --> 00:02:06,800
你也许无法期待 在左边训练得很好的模块
then you might not expect a module trained on the data on the left

37
00:02:06,800 --> 00:02:09,430
同样在右边也运行得很好
to do very well on the data on the right.

38
00:02:09,430 --> 00:02:13,901
即使存在运行都很好的同一个函数
Even though there might be the same function that actually works well,

39
00:02:13,901 --> 00:02:19,230
但你不会希望你的学习算法去发现
but you wouldn't expect your learning algorithm to discover

40
00:02:19,230 --> 00:02:21,500
绿色的决策边界 如果只看左边数据的话
that green decision boundary,just looking at the data on the left.

41
00:02:21,500 --> 00:02:26,210
所以 使你数据改变分布的这个想法
So, this idea of your data distribution changing

42
00:02:26,210 --> 00:02:31,657
有个有点怪的名字 covariate shift
goes by the somewhat fancy name, covariate shift.

43
00:02:31,657 --> 00:02:33,695
想法是这样的
And the idea is that,

44
00:02:33,695 --> 00:02:35,910
如果你已经学习了x到y的映射
if you've learned some x to y mapping,

45
00:02:35,910 --> 00:02:38,435
如果x的分布改变了
if the distribution of x changes,

46
00:02:38,435 --> 00:02:41,545
那么你可能需要重新训练你的学习算法
then you might need to retrain your learning algorithm.

47
00:02:41,545 --> 00:02:43,925
这种做法同样适用于
And this is true even if the function,

48
00:02:43,925 --> 00:02:45,230
如果真实函数
the ground true function,

49
00:02:45,230 --> 00:02:47,775
由x到y映射 保持不变
mapping from x to y,remains unchanged,

50
00:02:47,775 --> 00:02:49,430
正如此例中
which it is in this example,

51
00:02:49,430 --> 00:02:51,630
因为真实函数
because the ground true function is,

52
00:02:51,630 --> 00:02:53,990
是此图片是否是一只猫
is this picture a cat or not.

53
00:02:53,990 --> 00:02:57,890
训练你的函数的需要变得更加迫切
And the need to retrain your function becomes even more acute

54
00:02:57,890 --> 00:03:03,510
如果真实函数也改变 情况就更糟了
or it becomes even worse if the ground true function shifts as well.

55
00:03:03,510 --> 00:03:08,720
covariate shift的问题怎么应用于神经网络呢？
So, how does this problem of covariate shift apply to a neural network?

56
00:03:08,720 --> 00:03:10,880
试想一个像这样的深度网络
Consider a deep network like this,

57
00:03:10,880 --> 00:03:12,225
让我们从这层
and let's look at the learning process

58
00:03:12,225 --> 00:03:16,995
第三隐藏层来看看学习过程
from the perspective of this certain layer, the third hidden layer.

59
00:03:16,995 --> 00:03:22,145
此网络已经学了参数w^[3] b^[3]
So this network has learned the parameters w^[3] and b^[3].

60
00:03:22,145 --> 00:03:24,860
从第三隐藏层的角度来看
And from the perspective of the third hidden layer,

61
00:03:24,860 --> 00:03:27,665
它从前层中取得一些值
it gets some set of values from the earlier layers,

62
00:03:27,665 --> 00:03:30,020
接着它需要做些什么
and then it has to do some stuff to hopefully make

63
00:03:30,020 --> 00:03:34,305
希望使输出值ŷ 接近真实值y
the output y-hat close to the ground true value y.

64
00:03:34,305 --> 00:03:38,340
让我先遮住左边的东西
So let me cover up the nose on the left for a second.

65
00:03:38,340 --> 00:03:41,785
从第三隐藏层的角度来看
So from the perspective of this third hidden layer,

66
00:03:41,785 --> 00:03:44,265
它得到一些值
it gets some values,

67
00:03:44,265 --> 00:03:48,520
称为a^[2]_1 a^[2]_2 a^[2]_3 a^[2]_4
let's call them a^[2]_1, a^[2]_2, a^[2]_3, and a^[2]_4.

68
00:03:48,520 --> 00:03:58,102
但这些值也可以是特征值x^[1] x^[2] x^[3] x^[4]
But these values might as well be features x^[1], x^[2], x^[3], x^[4],

69
00:03:58,102 --> 00:04:02,240
第三隐藏层的工作是
and the job of the third hidden layer is to

70
00:04:02,240 --> 00:04:08,225
找到一种方式 使这些值映射到ŷ
take these values and find a way to map them to y-hat.

71
00:04:08,225 --> 00:04:10,760
你可以想象做一些截断
So you can imagine doing great intercepts,

72
00:04:10,760 --> 00:04:14,525
所以 这些参数w^[3] b^[3]
so that these parameters w^[3] b^[3]

73
00:04:14,760 --> 00:04:19,525
或w^[4] b^[4] w^[5] b^[5]
as well as maybe w^[4] b^[4],and even w^[5] b^[5],

74
00:04:19,525 --> 00:04:21,800
也许是这学习这些参数
maybe try and learn those parameters,

75
00:04:21,800 --> 00:04:23,360
所以网络做的不错
so the network does a good job,

76
00:04:23,360 --> 00:04:26,360
从左边我用黑笔写的
mapping from the values I drew in black on the left

77
00:04:26,360 --> 00:04:26,460
映射到输出值ŷ
to the output values y-hat.
从左边我用黑笔写的
mapping from the values I drew in black on the left

78
00:04:26,460 --> 00:04:29,460
映射到输出值ŷ
to the output values y-hat.

79
00:04:29,460 --> 00:04:33,470
现在我们把网络的左边揭开
But now let's uncover the left of the network again.

80
00:04:33,470 --> 00:04:42,226
这个网络还有参数w^[2] b^[2]和w^[1] b^[1]
The network is also adapting parameters w^[2] b^[2] and w^[1] b^[1],

81
00:04:42,226 --> 00:04:45,305
如果这些参数改变
and so as these parameters change,

82
00:04:45,305 --> 00:04:49,795
这些a^[2]的值也会改变
these values, a^[2], will also change.

83
00:04:49,795 --> 00:04:53,080
从第三隐藏层的角度来看
So from the perspective of the third hidden layer,

84
00:04:53,080 --> 00:04:56,735
这些隐藏单元的值在不断地改变
these hidden unit values are changing all the time,

85
00:04:56,735 --> 00:05:00,090
所以它就有了covariate shift的问题
and so it's suffering from the problem of covariate shift

86
00:05:00,090 --> 00:05:02,435
上张幻灯片中我们讲过的
that we talked about on the previous slide.

87
00:05:02,435 --> 00:05:04,115
batch归一化做的
So what batch norm does,

88
00:05:04,115 --> 00:05:05,130
是它减少了
is it reduces

89
00:05:05,130 --> 00:05:10,730
这些隐藏值分布变化的数量
the amount that the distribution of these hidden unit values shifts around.

90
00:05:10,730 --> 00:05:14,825
如果是绘制这些隐藏的单元值的分布
And if it were to plot the distribution of these hidden unit values,

91
00:05:14,825 --> 00:05:17,948
也许这是重整值z
maybe this is technically renormalizer z,

92
00:05:17,948 --> 00:05:24,970
这其实是z^[2]_1 z^[2]_2
so this is actually z^[2]_1 and z^[2]_2,

93
00:05:24,970 --> 00:05:27,862
我要绘制两个值而不是四个值
and I also plot two values instead of four values,

94
00:05:27,862 --> 00:05:29,670
以便我们设想为2D
so we can visualize in 2D.

95
00:05:29,670 --> 00:05:32,015
batch归一化讲的是
What batch norm is saying is that,

96
00:05:32,015 --> 00:05:35,745
z^[2]_1 z^[2]_2的值可以改变
the values for z^[2]_1 and z^[2]_2 can change,

97
00:05:35,745 --> 00:05:37,014
他们的确会改变
and indeed they will change

98
00:05:37,014 --> 00:05:41,215
当神经网络更新参数 在之前层中
when the neural network updates the parameters in the earlier layers.

99
00:05:41,215 --> 00:05:44,930
batch归一化可确保 无论其怎样变化
But what batch norm ensures is that no matter how it changes,

100
00:05:44,930 --> 00:05:55,050
z^[2]_1 z^[2]_2的均值和方差保持不变
the mean and variance of z^[2]_1 and z^[2]_2 will remain the same.

101
00:05:55,050 --> 00:05:59,900
所以即使z^[2]_1 z^[2]_2的值改变
So even if the exact values of z^[2]_1 and z^[2]_2 change,

102
00:05:59,900 --> 00:06:07,115
至少他们的均值和方差也会是均值0 方差1
their mean and variance will at least stay same mean zero and variance one.

103
00:06:07,115 --> 00:06:09,940
或者不一定必须是均值0 方差1
Or, not necessarily mean zero and variance one,

104
00:06:09,940 --> 00:06:17,295
而是由β^[2]和γ^[2]决定的值
but whatever value is governed by β^[2] and γ^[2].

105
00:06:17,295 --> 00:06:19,228
如果神经网络选择的话
Which, if the neural networks chooses,

106
00:06:19,228 --> 00:06:22,270
可强制其为均值0 方差1
can force it to be mean zero and variance one.

107
00:06:22,270 --> 00:06:24,655
或 其他任何均值和方差
Or, really, any other mean and variance.

108
00:06:24,655 --> 00:06:26,305
但它做的是
But what this does is,

109
00:06:26,305 --> 00:06:31,290
它限制了在前层的参数更新
it limits the amount to which updating the parameters in the earlier layers

110
00:06:31,290 --> 00:06:35,110
会影响数值分布的程度
can affect the distribution of values

111
00:06:35,110 --> 00:06:38,790
第三层看到的这种情况 因此得学习
that the third layer now sees and therefore has to learn on.

112
00:06:38,790 --> 00:06:44,370
batch归一化减少了输入值改变的问题
And so, batch norm reduces the problem of the input values changing,

113
00:06:44,370 --> 00:06:48,895
它的确是这些值变得更稳定
it really causes these values to become more stable,

114
00:06:48,895 --> 00:06:55,155
神经网络的之后层就会有更坚实的基础
so that the later layers of the neural network has more firm ground to stand on.

115
00:06:55,155 --> 00:06:57,655
即使输入分布改变了一些
And even though the input distribution changes a bit,

116
00:06:57,655 --> 00:07:00,580
它会改变得更少 它做的是
it changes less, and what this does is,

117
00:07:00,580 --> 00:07:03,325
当前层保持学习
even as the earlier layers keep learning,

118
00:07:03,325 --> 00:07:06,540
当层改变时 迫使后层
the amounts that this forces the later layers

119
00:07:06,540 --> 00:07:10,180
适应的程度减小了
to adapt to as early as layer changes is reduced or,

120
00:07:10,180 --> 00:07:12,925
你可以这样想 它减弱了
if you will, it weakens the coupling between

121
00:07:12,925 --> 00:07:15,445
前层参数的作用
what the early layers parameters has to do

122
00:07:15,445 --> 00:07:18,020
与后层参数的作用之间的联系
and what the later layers parameters have to do.

123
00:07:18,020 --> 00:07:22,147
它使得网络每层都可以自己学习
And so it allows each layer of the network to learn by itself,

124
00:07:22,147 --> 00:07:25,210
稍稍独立于其它层
a little bit more independently of other layers,

125
00:07:25,210 --> 00:07:29,145
这有助于加速整个网络的学习
and this has the effect of speeding up learning in the whole network.

126
00:07:29,145 --> 00:07:32,161
所以 希望这能带给你更好的直觉
So I hope this gives some better intuition,

127
00:07:32,161 --> 00:07:35,620
重点是batch归一化的意思是
but the takeaway is that batch norm means that,

128
00:07:35,620 --> 00:07:37,510
尤其从
especially from the perspective of

129
00:07:37,510 --> 00:07:39,010
神经网络后层之一的角度而言
one of the later layers of the neural network,

130
00:07:39,010 --> 00:07:43,090
前层不会左右移动的那么多
the earlier layers don't get to shift around as much,

131
00:07:43,090 --> 00:07:46,320
因为它们被同样的均值和方差所限制
because they're constrained to have the same mean and variance.

132
00:07:46,320 --> 00:07:50,260
所以 这会使得后层的学习工作变得更容易些
And so this makes the job of learning on the later layers easier.

133
00:07:50,260 --> 00:07:52,669
batch归一化还有一个作用
It turns out batch norm has a second effect,

134
00:07:52,669 --> 00:07:55,940
它有轻微的正则化效果
it has a slight regularization effect.

135
00:07:55,940 --> 00:07:59,885
batch归一化中非直观的一件事是每个mini-batch
So one non-intuitive thing of a batch norm is that each mini-batch,

136
00:07:59,885 --> 00:08:02,090
我会说mini-batch x^
I will say mini-batch x^,

137
00:08:02,090 --> 00:08:07,225
的值为z^[t] z^[l]
has the values z^[t],has the values z^[l],

138
00:08:07,225 --> 00:08:12,730
在mini-batch计算的 由均值和方差缩放的
scaled by the mean and variance computed on just that one mini-batch.

139
00:08:12,730 --> 00:08:16,895
因为在mini-batch上计算的均值和方差
Now, because the mean and variance computed on just that mini-batch

140
00:08:16,895 --> 00:08:20,245
而不是在整个数据集上
as opposed to computed on the entire data set,

141
00:08:20,245 --> 00:08:22,960
均值和方差有一些小噪音
that mean and variance has a little bit of noise in it,

142
00:08:22,960 --> 00:08:25,540
因为它只在你的mini-batch上计算
because it's computed just on your mini-batch of,

143
00:08:25,540 --> 00:08:29,745
比如64或128或256
say, 64, or 128,or maybe 256

144
00:08:29,745 --> 00:08:32,335
或更大的训练例子
or larger training examples.

145
00:08:32,335 --> 00:08:33,935
因为均值和方差有一点小噪音
So because the mean and variance is a little bit noisy

146
00:08:33,935 --> 00:08:38,195
因为它只是由一小部分数据估计得出的
because it's estimated with just a relatively small sample of data,

147
00:08:38,195 --> 00:08:43,363
缩放过程从z^[1]到Z̃^[l]
the scaling process,going from z^[1]to Z̃^[l],

148
00:08:43,363 --> 00:08:46,135
过程也有一些噪音
that process is a little bit noisy as well,

149
00:08:46,135 --> 00:08:51,420
因为它是用有些噪音的均值和方差计算得出的
because it's computed, using a slightly noisy mean and variance.

150
00:08:51,420 --> 00:08:54,817
所以和dropout相似
So similar to dropout,

151
00:08:54,817 --> 00:08:57,980
它往每个隐藏层的激活值上增加了噪音
it adds some noise to each hidden layer's activations.

152
00:08:57,980 --> 00:08:59,905
dropout有噪音的方式
The way dropout has noises,

153
00:08:59,905 --> 00:09:04,180
它使一个隐藏的单元 以一定的概率乘以0
it takes a hidden unit and it multiplies it by zero with some probability.

154
00:09:04,180 --> 00:09:06,870
以一定的概率乘以1
And multiplies it by one with some probability.

155
00:09:06,870 --> 00:09:09,350
所以你的dropout含几重噪音
And so your dropout has multiple of noise

156
00:09:09,350 --> 00:09:12,350
因为它乘以0或1
because it's multiplied by zero or one,

157
00:09:12,350 --> 00:09:15,350
对比而言 batch归一化含几重噪音
whereas batch norm has multiples of noise

158
00:09:15,350 --> 00:09:15,360
因为标准偏差的缩放
because of scaling by the standard deviation,
对比而言 batch归一化含几重噪音
whereas batch norm has multiples of noise

159
00:09:15,360 --> 00:09:18,360
因为标准偏差的缩放
because of scaling by the standard deviation,

160
00:09:18,360 --> 00:09:21,655
和减去均值带来的额外噪音
as well as additive noise because it's subtracting the mean.

161
00:09:21,655 --> 00:09:25,825
这里均值和标准偏差的估值也是有噪音的
Well, here the estimates of the mean and the standard deviation are noisy.

162
00:09:25,825 --> 00:09:29,785
所以类似于dropout
And so, similar to dropout,

163
00:09:29,785 --> 00:09:33,220
batch归一化有轻微的正则化效果
batch norm therefore has a slight regularization effect.

164
00:09:33,220 --> 00:09:35,435
因为给隐藏单元添加了噪音
Because by adding noise to the hidden units,

165
00:09:35,435 --> 00:09:38,280
这迫使后部单元
it's forcing the downstream hidden units

166
00:09:38,435 --> 00:09:41,280
不过分依赖任何一个隐藏单元
not to rely too much on any one hidden unit.

167
00:09:41,280 --> 00:09:43,025
类似于dropout
And so similar to dropout,

168
00:09:43,025 --> 00:09:44,920
它给隐藏层增加了噪音
it adds noise to the hidden layers

169
00:09:44,920 --> 00:09:47,620
因此有轻微的正则化效果
and therefore has a very slight regularization effect.

170
00:09:47,620 --> 00:09:50,064
因为添加的噪音很微小
Because the noise added is quite small,

171
00:09:50,064 --> 00:09:52,572
所以并不是巨大的正则化效果
this is not a huge regularization effect,

172
00:09:52,572 --> 00:09:56,760
你可以将batch归一化和dropout一起使用
and you might choose to use batch norm together with dropout,

173
00:09:56,760 --> 00:09:59,880
你可以将两者共同使用
and you might use batch norm together with dropouts

174
00:09:59,880 --> 00:10:03,060
如果你想得到dropout更强大的正则化效果
if you want the more powerful regularization effect of dropout.

175
00:10:03,060 --> 00:10:06,195
也许另一个轻微的非直观的效果是
And maybe one other slightly non-intuitive effect is that,

176
00:10:06,195 --> 00:10:08,454
如果你应用了较大的mini-batch
if you use a bigger mini-batch size,

177
00:10:08,454 --> 00:10:11,200
对 比如说 你用了
right, so if you use a mini-batch size of, say,

178
00:10:11,200 --> 00:10:13,725
512而不是64
512 instead of 64,

179
00:10:13,725 --> 00:10:15,934
通过应用较大的mini-batch
by using a larger mini-batch size,

180
00:10:15,934 --> 00:10:17,234
你减少了噪音
you're reducing this noise

181
00:10:17,240 --> 00:10:20,940
因此减少了正则化效果
and therefore also reducing this regularization effect.

182
00:10:20,940 --> 00:10:24,030
这是dropout的一个奇怪的性质
So that's one strange property of dropout

183
00:10:24,030 --> 00:10:27,435
就是应用较大的mini-batch
which is that by using a bigger mini-batch size,

184
00:10:27,435 --> 00:10:29,870
可以减少正则化效果
you reduce the regularization effect.

185
00:10:29,870 --> 00:10:33,833
说到这儿 我把会把batch归一化当成一个规则
Having said this, I wouldn't really use batch norm as a regularizer,

186
00:10:33,833 --> 00:10:36,625
这确实不是其目的
that's really not the intent of batch norm,

187
00:10:36,625 --> 00:10:40,250
但有时他会对你的算法有额外的期望效应
but sometimes it has this extra intended

188
00:10:40,250 --> 00:10:44,250
或非期望效应
or unintended effect on your learning algorithm.

189
00:10:44,250 --> 00:10:48,390
但是不要把batch归一化当作规则
But, really, don't turn to batch norm as a regularization.

190
00:10:48,390 --> 00:10:52,070
把它当作将你归一化隐藏单元激活值
Use it as a way to normalize your hidden units activations

191
00:10:52,070 --> 00:10:53,770
并加速学习的方式
and therefore speed up learning.

192
00:10:53,770 --> 00:10:57,900
我认为正规化几乎是一个意想不到的副作用
And I think the regularization is an almost unintended side effect.

193
00:10:57,900 --> 00:11:02,430
所以希望这能让你更理解batch归一化的工作
So I hope that gives you better intuition about what batch norm is doing.

194
00:11:02,430 --> 00:11:04,580
在我们结束batch归一化的讨论之前
Before we wrap up the discussion on batch norm,

195
00:11:04,580 --> 00:11:06,855
我想确保你还知道一个细节
there's one more detail I want to make sure you know,

196
00:11:06,855 --> 00:11:11,254
batch归一化一次只能处理一个mini-batch数据
which is that batch norm handles data one mini-batch at a time.

197
00:11:11,254 --> 00:11:14,520
它在mini-batch上计算均值和方差
It computes mean and variances on mini-batches.

198
00:11:14,520 --> 00:11:15,720
所以测试时
So at test time,

199
00:11:15,720 --> 00:11:18,150
你试图做出预测 试着评估神经网络
you try and make predictors, try and evaluate the neural network,

200
00:11:18,150 --> 00:11:20,400
你也许没有mini-batch例子
you might not have a mini-batch of examples,

201
00:11:20,400 --> 00:11:24,035
你也许一次只能进行一个简单的例子
you might be processing one single example at the time.

202
00:11:24,035 --> 00:11:27,400
所以测试时 你需要做一些不同的东西
So, at test time you need to do something slightly differently

203
00:11:27,400 --> 00:11:29,430
以确保你的预测有意义
to make sure your predictions make sense.

204
00:11:29,430 --> 00:11:32,197
在下一个和最后的batch归一化的视频中
Like in the next and final video on batch norm,

205
00:11:32,197 --> 00:11:35,090
让我们详细谈谈你需要的细节
let's talk over the details of what you need to do in order to

206
00:11:35,090 --> 00:11:39,290
来让你的神经网络应用batch归一化来做出预测
take your neural network trained using batch norm to make predictions.

