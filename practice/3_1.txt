1
00:00:00,000 --> 00:00:01,710
大家好 欢迎回来(字幕来源：网易云课堂)
Hi, and welcome back.

2
00:00:01,710 --> 00:00:03,740
目前为止 你已经了解到
You've seen by now that

3
00:00:03,740 --> 00:00:07,415
神经网络的改变会涉及到许多不同超参数的设置
changing neural nets can involve setting a lot of different hyperparameters.

4
00:00:07,415 --> 00:00:11,155
现在 对于超参数而言 你要如何找到一套好的设定呢？
Now, how do you go about finding a good setting for these hyperparameters?

5
00:00:11,155 --> 00:00:13,710
在此视频中 我想和你分享一些指导原则
In this video, I want to share with you some guidelines,

6
00:00:13,710 --> 00:00:18,235
一些关于如何系统地组织超参调试过程的技巧
some tips for how to systematically organize your hyperparameter tuning process,

7
00:00:18,235 --> 00:00:20,640
希望这些能够让你更有效的
which hopefully will make it more efficient for you

8
00:00:20,640 --> 00:00:23,760
聚焦到合适的超参设定中
to converge on a good setting of the hyperparameters.

9
00:00:23,760 --> 00:00:25,929
关于训练深度最难的事情之一是
One of the painful things about training deepness

10
00:00:25,929 --> 00:00:29,250
你要处理的参数的数量
is the sheer number of hyperparameters you have to deal with,

11
00:00:29,250 --> 00:00:35,935
从学习速率α到momentum（术语）β
ranging from the learning rate alpha to the momentum term beta,

12
00:00:35,935 --> 00:00:41,001
如果使用momentum或Adam优化算法的参数
if using momentum,or the hyperparameters for the Adam Optimization Algorithm

13
00:00:41,001 --> 00:00:44,185
即β1 β2和ε
which are beta one, beta two, and epsilon.

14
00:00:44,185 --> 00:00:47,270
也许你还得选择层数
Maybe you have to pick the number of layers,

15
00:00:47,270 --> 00:00:50,820
也许你还得选择不同层中隐藏单元的数量
maybe you have to pick the number of hidden units for the different layers,

16
00:00:50,820 --> 00:00:55,093
也许你还想使用学习率衰退
and maybe you want to use learning rate decay,

17
00:00:55,093 --> 00:00:59,899
所以 你使用的不是单一的学习速率α
so you don't just use a single learning rate alpha.

18
00:00:59,899 --> 00:01:01,065
接着 当然
And then of course,

19
00:01:01,065 --> 00:01:06,220
你可能还需要选择Mini-batch的大小
you might need to choose the mini-batch size.

20
00:01:06,220 --> 00:01:09,990
结果证实 一些超参数比其它的更为重要
So it turns out, some of these hyperparameters are more important than others.

21
00:01:09,990 --> 00:01:12,235
我认为 最广泛的学习应用是α
The most learning applications I would say,alpha,

22
00:01:12,235 --> 00:01:16,015
学习速率是需要调试的最重要的超参数
the learning rate is the most important hyperparameter to tune.

23
00:01:16,015 --> 00:01:21,595
除了α 还有一些参数需要调试
Other than alpha, a few other hyperparameters I tend to would maybe tune next,

24
00:01:21,595 --> 00:01:25,040
例如momentum
would be maybe the momentum term,

25
00:01:25,040 --> 00:01:27,795
0.9就是很个好的默认值
say, 0.9 is a good default.

26
00:01:27,795 --> 00:01:30,500
我还会调试mini-batch的大小
I'd also tune the mini-batch size

27
00:01:30,500 --> 00:01:34,465
以确保最优算法运行有效
to make sure that the optimization algorithm is running efficiently.

28
00:01:34,465 --> 00:01:36,985
我还会经常调试隐藏单元
Often I also fiddle around with the hidden units.

29
00:01:36,985 --> 00:01:39,250
我用橙色圈住的这些
Of the ones I've circled in orange,

30
00:01:39,250 --> 00:01:43,660
这三个是我觉得其次比较重要的
these are really the three that I would consider second in importance to

31
00:01:43,660 --> 00:01:46,060
the learning rate alpha and then third in
the learning rate alpha and then third in

32
00:01:46,060 --> 00:01:49,060
重要性排第三位的是其他因素
importance after fiddling around with the others,

33
00:01:49,060 --> 00:01:51,925
层数有时会产生很大的影响
the number of layers can sometimes make a huge difference,

34
00:01:51,925 --> 00:01:55,000
学习率衰减也是如此
and so can learning rate decay.

35
00:01:55,000 --> 00:01:58,870
当应用Adam算法时 事实上 我从不调试β1
And then when using the Adam algorithm I actually pretty much never tuned beta one,

36
00:01:58,870 --> 00:02:00,434
β2 和ε
beta two, and epsilon.

37
00:02:00,434 --> 00:02:04,930
我总是选定其分别为0.9，0.999和10^(-8)
Pretty much I always use 0.9,0.999 and tenth minus eight

38
00:02:04,930 --> 00:02:08,570
如果你想的话也可以调试它们
although you can try tuning those as well if you wish.

39
00:02:08,570 --> 00:02:12,130
但希望你粗略了解到哪些超参数较为重要
But hopefully it does give you some rough sense of what hyperparameters might be more important than others

40
00:02:12,130 --> 00:02:16,463
α无疑是最重要的
alpha,most important, for sure,

41
00:02:19,005 --> 00:02:22,270
接下来是我用橙色圈住的那些
followed maybe by the ones I've circle in orange,

42
00:02:22,270 --> 00:02:25,235
然后是我用紫色圈住的那些
followed maybe by the ones I circled in purple.

43
00:02:25,235 --> 00:02:25,760
但这不是严格 快速的标准
But this isn't a hard and fast rule

44
00:02:25,760 --> 00:02:30,490
我认为 其它深度学习的研究者
and I think other deep learning practitioners may well

45
00:02:30,490 --> 00:02:33,670
可能会很不同意我的观点或有着不同的直觉
disagree with me or have different intuitions on these.

46
00:02:33,670 --> 00:02:37,240
现在 如果你尝试调整一些超参数
Now, if you're trying to tune some set of hyperparameters,

47
00:02:37,240 --> 00:02:40,060
该如何选择调试值呢？
how do you select a set of values to explore?

48
00:02:40,060 --> 00:02:42,845
在早一代的机器学习算法中
In earlier generations of machine learning algorithms,

49
00:02:42,845 --> 00:02:44,660
如果你有两个超参数
if you had two hyperparameters,

50
00:02:44,660 --> 00:02:47,662
这里我会称之为超参一 超参二
which I'm calling hyperparameter one and hyperparameter two here,

51
00:02:47,662 --> 00:02:53,380
常见的做法是在网格中取样点
it was common practice to sample the points in a grid

52
00:02:53,380 --> 00:02:59,435
像这样 然后系统的研究这些数值
like so and systematically explore these values.

53
00:02:59,435 --> 00:03:00,935
这里我放置的是5*5的网格
Here I am placing down a five by five grid.

54
00:03:00,935 --> 00:03:03,070
实践证明  网格可以是5*5 也可多或少
In practice, it could be more or less than the five by five grid

55
00:03:03,070 --> 00:03:12,430
但对于这个例子 你可以尝试这所有的25个点 然后选择哪个参数效果最好
but you try out in this example all 25 points and then pick whichever hyperparameter works best.

56
00:03:12,430 --> 00:03:18,010
当参数的数量相对较少时 这个方法很实用
And this practice works okay when the number of hyperparameters was relatively small.

57
00:03:18,010 --> 00:03:19,840
在深度学习领域 我们常做的
In deep learning, what we tend to do,

58
00:03:19,840 --> 00:03:21,415
我推荐你采用下面的做法
and what I recommend you do instead,

59
00:03:21,415 --> 00:03:23,975
随机选择点
is choose the points at random.

60
00:03:23,975 --> 00:03:27,970
所以你可以选择同等数量的点 对吗？
So go ahead and choose maybe of same number of points, right?

61
00:03:27,970 --> 00:03:34,590
25个点 接着 用这些随机取的点试验超参数的效果
25 points and then try out the hyperparameters on this randomly chosen set of points.

62
00:03:34,590 --> 00:03:38,350
之所以这么做是因为
And the reason you do that is that it's difficult to know in

63
00:03:38,350 --> 00:03:43,040
对于你要解决的问题而言 你很难提前知道哪个超参数最重要
advance which hyperparameters are going to be the most important for your problem.

64
00:03:43,040 --> 00:03:44,480
正如你之前看到的
And as you saw in the previous slide,

65
00:03:44,480 --> 00:03:47,910
一些超参数的确要比其它的更重要
some hyperparameters are actually much more important than others.

66
00:03:47,910 --> 00:03:49,190
举个例子
So to take an example,

67
00:03:49,190 --> 00:03:53,505
假设超参数一是α 学习速率
let's say hyperparameter one turns out to be alpha, the learning rate.

68
00:03:53,505 --> 00:03:55,175
取一个极端的例子
And to take an extreme example,

69
00:03:55,175 --> 00:03:58,180
假设超参数二
let's say that hyperparameter two was that

70
00:03:58,180 --> 00:04:02,730
是Adam算法中 分母中ε的值
value epsilon that you have in the denominator of the Adam algorithm.

71
00:04:02,730 --> 00:04:07,455
这种情况下 α的取值很重要 而ε取值则无关紧要
So your choice of alpha matters a lot and your choice of epsilon hardly matters.

72
00:04:07,455 --> 00:04:09,410
如果你在网格中取点
So if you sample in the grid

73
00:04:09,410 --> 00:04:16,300
接着 你试验了α的5个取值
then you've really tried out five values of alpha

74
00:04:16,300 --> 00:04:17,550
那你会发现
and you might find that

75
00:04:17,550 --> 00:04:21,190
无论ε取何值 结果基本上都是一样的
all of the different values of epsilon give you essentially the same answer.

76
00:04:21,190 --> 00:04:24,400
所以 你知道共有25种模型
So you've now trained 25 models and

77
00:04:24,400 --> 00:04:27,925
但进行试验的α值只有5个
only got into trial five values for the learning rate alpha,

78
00:04:27,925 --> 00:04:29,740
我认为这是很重要的
which I think is really important.

79
00:04:29,740 --> 00:04:33,430
对比而言 如果你随机取值
Whereas in contrast, if you were to sample at random,

80
00:04:33,430 --> 00:04:39,560
你会试验25个独立的α值
then you will have tried out 25 distinct values of the learning rate alpha

81
00:04:39,560 --> 00:04:40,390
所以 你似乎更可能
and therefore you be more likely

82
00:04:40,390 --> 00:04:43,690
发现效果最好的那个
to find a value that works really well.

83
00:04:43,690 --> 00:04:44,980
我已经解释了
I've explained this example,

84
00:04:44,980 --> 00:04:47,160
两个参数的情况
using just two hyperparameters.

85
00:04:47,160 --> 00:04:50,270
实践中 你搜索的超参数可能不止两个
In practice, you might be searching over many more hyperparameters than these,

86
00:04:50,270 --> 00:04:52,000
假如
so if you have, say,

87
00:04:52,000 --> 00:04:55,080
你有三个超参数 这时你搜索的不是一个方格
three hyperparameters, I guess instead of searching over a square,

88
00:04:55,080 --> 00:05:00,820
而是一个立方体  超参数三代表第三维
you're searching over a cube where this third dimension is hyperparameter three and

89
00:05:00,820 --> 00:05:04,510
接着 在三维立方体中取值
then by sampling within this three-dimensional cube

90
00:05:04,510 --> 00:05:07,080
你会试验大量的更多的值
you get to try out a lot more values of

91
00:05:07,080 --> 00:05:08,080
三个超参数中每个都是
each of your three hyperparameters.

92
00:05:08,080 --> 00:05:09,440
实践中
And in practice

93
00:05:09,440 --> 00:05:12,980
你搜索的可能不止三个超参数
you might be searching over even more hyperparameters than three and

94
00:05:12,980 --> 00:05:17,160
有时 很难预知
sometimes it's just hard to know in advance which ones turn out to be

95
00:05:17,160 --> 00:05:20,120
哪个是最重要的超参数 对于你的具体应用而言
the really important hyperparameters for your application

96
00:05:20,120 --> 00:05:23,390
随机取值而不是网格取值表明
and sampling at random rather than in the grid shows

97
00:05:23,390 --> 00:05:29,785
你探究了更多重要超参数的潜在值
that you are more richly exploring set of possible values for the most important hyperparameters,

98
00:05:29,785 --> 00:05:31,045
无论结果是什么
whatever they turn out to be.

99
00:05:31,045 --> 00:05:33,130
当你给超参数取值时
When you sample hyperparameters,

100
00:05:33,130 --> 00:05:37,875
另一个惯例是采用由粗糙到精细的策略
another common practice is to use a coarse to fine sampling scheme.

101
00:05:37,875 --> 00:05:42,130
比如 在二维的那个例子中 你进行了取值
So let's say in this two-dimensional example that you sample these points,

102
00:05:42,130 --> 00:05:45,600
也许你会发现效果最好的某个点
and maybe you found that this point worked the best and

103
00:05:45,600 --> 00:05:49,210
也许这个点周围的其他一些点效果也很好
maybe a few other points around it tended to work really well,

104
00:05:49,210 --> 00:05:52,530
那在接下来要做的
then in the course of the final scheme what you might do is

105
00:05:52,530 --> 00:06:00,820
是放大这块小区域 然后在其中更密集地取值
to zoom in to a smaller region of the hyperparameters and then sample more density within this space.

106
00:06:00,820 --> 00:06:02,795
或随机取值
Or maybe again at random,

107
00:06:02,795 --> 00:06:08,690
聚焦更多的资源 在这个蓝色的方格中搜索
but to then focus more resources on searching within this blue square

108
00:06:08,690 --> 00:06:12,265
如果你怀疑这些超参数
if you're suspecting that the best setting,the hyperparameters,

109
00:06:12,265 --> 00:06:13,600
在这个区域的最优结果
may be in this region.

110
00:06:13,600 --> 00:06:18,365
那在整个的方格中进行粗略搜索后
So after doing a coarse sample of this entire square,

111
00:06:18,365 --> 00:06:22,375
你会知道接下来应该聚焦到更小的方格中
that tells you to then focus on a smaller square.

112
00:06:22,375 --> 00:06:26,105
在更小的方格中 你可以更密集地取点
You can then sample more densely into smaller square.

113
00:06:26,105 --> 00:06:29,720
所以 这种从粗到细的搜索也经常使用
So this type of a coarse to fine search is also frequently used.

114
00:06:29,720 --> 00:06:32,565
通过试验超参数的不同取值
And by trying out these different values of the hyperparameters

115
00:06:32,565 --> 00:06:37,940
你可以选择对于训练集目标而言的最优值
you can then pick whatever value allows you to do best on your training set objective

116
00:06:37,940 --> 00:06:41,230
或对于开发集而言的最优值
or does best on your development set

117
00:06:41,230 --> 00:06:46,660
或在超参搜索过程中你最想优化的东西
or whatever you're trying to optimize in your hyperparameter search process.

118
00:06:46,660 --> 00:06:48,570
我希望 这能给你提供一种方法
So I hope this gives you a way to more

119
00:06:48,570 --> 00:06:51,670
去系统地组织超参数搜索过程
systematically organize your hyperparameter search process.

120
00:06:51,670 --> 00:06:53,200
另个关键点是
The two key takeaways are,

121
00:06:53,200 --> 00:06:55,930
随机取值和精确搜索
use random sampling and adequate search and

122
00:06:55,930 --> 00:07:01,585
考虑使用由粗糙到精细的搜索过程
optionally consider implementing a coarse to fine search process.

123
00:07:01,585 --> 00:07:04,750
但超参数的搜索内容还不止这些
But there's even more to hyperparameter search than this.

124
00:07:04,750 --> 00:07:07,300
在下一个视频中 我会继续讲解关于如何选择
Let's talk more in the next video about how to choose

125
00:07:07,300 --> 00:07:10,020
超参数取值的合理范围
the right scale on which to sample your hyperparameters.

