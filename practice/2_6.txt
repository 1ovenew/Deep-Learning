1
00:00:00,278 --> 00:00:02,200
还有一种算法叫做Momentum(字幕来源：网易云课堂)
There's an algorithm called momentum,

2
00:00:02,200 --> 00:00:04,571
或者叫做Momentum梯度下降法
or gradient descent with momentum

3
00:00:04,570 --> 00:00:06,697
运行速度几乎总是
that almost always works faster

4
00:00:06,690 --> 00:00:09,148
快于标准的梯度下降算法
than the standard gradient descent algorithm.

5
00:00:09,140 --> 00:00:11,810
简而言之 基本的想法就是
In one sentence, the basic idea is

6
00:00:11,810 --> 00:00:14,985
计算梯度的指数加权平均数
to compute an exponentially weighted average of your gradients,

7
00:00:14,980 --> 00:00:18,219
并利用该梯度更新你的权重
and then use that gradient to update your weights instead.

8
00:00:18,210 --> 00:00:21,651
在本视频中 我们要一起拆解单句描述
In this video, let's unpack that one sentence description and

9
00:00:21,651 --> 00:00:23,848
看看你到底如何计算
see how you can actually implement this.

10
00:00:23,848 --> 00:00:28,619
例如 如果你要优化成本函数
As a example let's say that you're trying to optimize a cost function

11
00:00:28,619 --> 00:00:30,510
函数形状如图
which has contours like this.

12
00:00:30,510 --> 00:00:34,029
红点代表最小值的位置
So the red dot denotes the position of the minimum.

13
00:00:34,020 --> 00:00:36,700
假设你从这里开始梯度下降法
Maybe you start gradient descent here

14
00:00:36,700 --> 00:00:40,607
如果进行梯度下降法的一次迭代
and if you take one iteration of gradient descent

15
00:00:40,600 --> 00:00:42,297
无论是batch或mini-batch下降法
either batch or mini-batch descent

16
00:00:42,290 --> 00:00:44,195
也许会指向这里
maybe end up heading there.

17
00:00:44,190 --> 00:00:47,900
现在在椭圆的另一边
But now you're on the other side of this ellipse,

18
00:00:47,900 --> 00:00:49,565
计算下一步梯度下降
and you take another step of gradient descent,

19
00:00:49,560 --> 00:00:51,297
结果或许如此
maybe you end up doing that.

20
00:00:51,290 --> 00:00:55,103
然后再计算一步 再一步 计算下去
And then another step, another step, and so on.

21
00:00:55,100 --> 00:00:57,454
你发现梯度下降法
And you see that gradient descents will

22
00:00:57,454 --> 00:01:00,460
要很多计算步骤对吧?
sort of take a lot of steps, right?

23
00:01:00,460 --> 00:01:06,598
慢慢摆动到最小值
Just slowly oscillate toward the minimum.

24
00:01:06,590 --> 00:01:09,134
这种上下波动
And this up and down oscillations

25
00:01:09,134 --> 00:01:11,206
减慢了梯度下降法的速度
slows down gradient descent

26
00:01:11,206 --> 00:01:14,500
你就无法使用更大的学习率
and prevents you from using a much larger learning rate.

27
00:01:14,500 --> 00:01:17,580
如果你要用较大的学习率
In particular, if you were to use a much larger learning rate

28
00:01:17,580 --> 00:01:21,533
结果可能会偏离函数的范围
you might end up overshooting and end up diverging like so.

29
00:01:21,530 --> 00:01:24,960
为了避免摆动过大
And so the need to prevent the oscillations from getting too big

30
00:01:24,960 --> 00:01:29,400
你要用一个较小的学习率
forces you to use a learning rate that's not itself too large.

31
00:01:29,400 --> 00:01:31,733
另一个看待问题的角度是
Another way of viewing this problem is that

32
00:01:31,733 --> 00:01:36,380
在纵轴上 你希望学习慢一点
on the vertical axis you want your learning to be a bit slower,

33
00:01:36,380 --> 00:01:38,711
因为你不想要这些摆动
because you don't want those oscillations.

34
00:01:38,710 --> 00:01:41,202
但是在横轴上
But on the horizontal axis,

35
00:01:41,200 --> 00:01:43,998
你希望加快学习
you want faster learning.

36
00:01:45,552 --> 00:01:48,831
你希望快速从左向右移
Right, because you want it to aggressively move from left to right,

37
00:01:48,830 --> 00:01:50,995
移向最小值 移向红点
toward that minimum, toward that red dot.

38
00:01:50,990 --> 00:01:53,327
所以使用Momentum梯度下降法
So here's what you can do

39
00:01:53,320 --> 00:01:56,044
你需要做的是
if you implement gradient descent with momentum.

40
00:01:58,540 --> 00:02:00,041
在每次迭代中
On each iteration,

41
00:02:00,041 --> 00:02:03,611
确切来说在第t次迭代的过程中
or more specifically, during iteration t

42
00:02:07,808 --> 00:02:11,560
你会计算微分dw db
you would compute the usual derivatives dw, db.

43
00:02:11,560 --> 00:02:14,506
我会省略上标括号l
I'll omit the superscript square bracket l's

44
00:02:14,500 --> 00:02:19,509
你用现有的mini-batch计算dw db
but you compute dw, db on the current mini-batch.

45
00:02:19,500 --> 00:02:21,482
如果你用batch梯度下降法
And if you're using batch gradient descent,

46
00:02:21,480 --> 00:02:24,094
现在的mini-batch就是全部的batch
then the current mini-batch would be just your whole batch.

47
00:02:24,090 --> 00:02:26,533
对于batch梯度下降法的效果是一样的
And this works as well off a batch gradient descent.

48
00:02:26,530 --> 00:02:29,453
如果现有的mini-batch就是整个训练集
So if your current mini-batch is your entire training set,

49
00:02:29,450 --> 00:02:30,994
效果也不错
this works fine as well.

50
00:02:30,994 --> 00:02:32,520
你要做的是
And then what you do is

51
00:02:32,520 --> 00:02:45,779
v_dW = β*v_dW + (1-β)*dW
you compute v_dW to be Beta V_dW plus 1 minus Beta dW.

52
00:02:45,770 --> 00:02:49,880
这跟我们之前的计算相似
So this is similar to when we're previously computing

53
00:02:49,880 --> 00:02:55,960
也就是v = β*v + (1-β)*数据t
v data equals beta v data plus 1 minus beta data t.

54
00:02:56,940 --> 00:02:57,902
所以计算得到的是
Right, so it's computing

55
00:02:57,900 --> 00:03:01,567
dw的移动平均数
a moving average of the derivatives for w you're getting.

56
00:03:01,567 --> 00:03:07,754
接着同样地计算v_db
And then you similarly compute v_db equals that

57
00:03:07,750 --> 00:03:13,546
等于β*v_db + (1-β)*db
plus 1 minus Beta times db.

58
00:03:13,540 --> 00:03:16,635
然后重新赋值权重
And then you would update your weights,

59
00:03:16,630 --> 00:03:21,482
W = W - α乘以
using W gets updated as W minus the learning rate times,

60
00:03:21,480 --> 00:03:24,811
这里重新赋值不用dW
instead of updating it with dW, with the derivative

61
00:03:24,810 --> 00:03:27,728
而用v_dW
you update it with v_dW.

62
00:03:27,720 --> 00:03:35,142
同样 b = b - α*v_db
And similarly, b gets updated as b minus alpha times v_db.

63
00:03:35,142 --> 00:03:39,570
这样就可以减缓梯度下降的幅度
So what this does is smooth out the steps of gradient descent.

64
00:03:40,888 --> 00:03:43,924
例如 在上几个导数中
For example, let's say that in the last few derivatives

65
00:03:43,920 --> 00:03:48,133
你计算得到了这个 这个 这个
you computed were this, this, this, this, this.

66
00:03:48,130 --> 00:03:49,800
如果平均这些梯度
If you average out these gradients,

67
00:03:49,800 --> 00:03:52,943
你会发现这些纵轴上的摆动
you find that the oscillations in the vertical direction

68
00:03:52,943 --> 00:03:55,472
平均值接近于零
will tend to average out to something closer to zero.

69
00:03:55,470 --> 00:03:59,540
所以在纵轴方向 你希望放慢一点
So, in the vertical direction, where you want to slow things down

70
00:03:59,540 --> 00:04:02,960
平均过程中 正负数相互抵消
this will average out positive and negative numbers,

71
00:04:02,960 --> 00:04:05,390
所以平均值接近于零
so the average will be close to zero.

72
00:04:05,390 --> 00:04:07,177
但是在横轴方向
Whereas, on the horizontal direction,

73
00:04:07,170 --> 00:04:10,912
所有的微分都指向横轴方向
all the derivatives are pointing to the right of the horizontal direction,

74
00:04:10,910 --> 00:04:13,868
因此横轴方向的平均值仍然较大
so the average in the horizontal direction will still be pretty big.

75
00:04:13,860 --> 00:04:17,562
因此用算法几次迭代后
So that's why with this algorithm, with a few iterations

76
00:04:17,560 --> 00:04:20,468
你发现Momentum梯度下降法
you find that the gradient descent with momentum

77
00:04:20,460 --> 00:04:23,503
最终
ends up eventually just taking steps

78
00:04:23,500 --> 00:04:27,626
纵轴方向的摆动变小了
that are much smaller oscillations in the vertical direction,

79
00:04:27,620 --> 00:04:33,296
横轴方向运动更快
but are more directed to just moving quickly in the horizontal direction.

80
00:04:33,290 --> 00:04:35,453
因此你的算法
And so this allows your algorithm to

81
00:04:35,450 --> 00:04:37,352
走了一条更加直接的路径
take a more straightforward path,

82
00:04:37,350 --> 00:04:42,530
在抵达最小值的路上减少了摆动
or to damp out the oscillations in this path to the minimum.

83
00:04:42,530 --> 00:04:46,188
Momentum的一个本质
One intuition for this momentum,

84
00:04:46,188 --> 00:04:48,389
这对有些人而不是所有人有效
which works for some people, but not everyone,

85
00:04:48,380 --> 00:04:52,416
就是如果你要最小化碗状函数
is that if you're trying to minimize your bowl shape function, right?

86
00:04:52,416 --> 00:04:55,440
这是碗的形状
This is really the contours of a bowl.

87
00:04:55,440 --> 00:04:57,425
我画画不太好
I guess I'm not very good at drawing.

88
00:04:57,420 --> 00:05:00,882
它们能够最小化碗装函数
They kind of minimize this type of bowl shaped function then

89
00:05:00,880 --> 00:05:02,742
这些微分项
these derivative terms

90
00:05:02,742 --> 00:05:06,900
想象它们为你从山上往下滚的一个球
you can think of as providing acceleration

91
00:05:06,900 --> 00:05:11,071
提供了加速度
to a ball that you're rolling down hill.

92
00:05:11,071 --> 00:05:19,151
Momentum项就相当于速度
And these momentum terms you can think of as representing the velocity.

93
00:05:20,810 --> 00:05:23,500
想象你有一个碗
And so imagine that you have a bowl,

94
00:05:23,500 --> 00:05:25,005
你拿一个球
and you take a ball

95
00:05:25,005 --> 00:05:28,854
微分给了这个球一个加速度
the derivative imparts acceleration to this little ball

96
00:05:28,850 --> 00:05:32,305
此时球正向山下滚
so the little ball is rolling down this hill, right?

97
00:05:32,300 --> 00:05:36,282
球因为加速度越滚越快
And so it rolls faster and faster, because of acceleration.

98
00:05:36,282 --> 00:05:40,520
而因为β稍小于1
And beta, because this number a little bit less than one

99
00:05:40,520 --> 00:05:42,940
表现出一些摩擦力
displays a row of friction

100
00:05:42,940 --> 00:05:47,028
所以球不会无限加速下去
and it prevents your ball from speeding up without limit.

101
00:05:47,028 --> 00:05:50,380
所以不像梯度下降法
But so rather than gradient descent,

102
00:05:50,380 --> 00:05:53,665
每一步都独立于之前的步骤
just taking every single step independently of all previous steps.

103
00:05:53,665 --> 00:05:57,240
你的球可以向下滚 获得动量
Now, your little ball can roll downhill and gain momentum,

104
00:05:57,240 --> 00:06:01,251
可以从碗向下加速 获得动量
it can accelerate down this bowl and therefore gain momentum.

105
00:06:01,250 --> 00:06:04,887
我发现这个球从碗滚下的比喻
I find that this ball rolling down a bowl analogy

106
00:06:04,880 --> 00:06:07,706
物理能力强的人接受得比较好
it seems to work for some people who enjoy physics intuitions.

107
00:06:07,700 --> 00:06:08,881
但不是所有人都能接受
But it doesn't work for everyone.

108
00:06:08,880 --> 00:06:12,881
如果球从碗中滚下这个比喻
so if this analogy of a ball rolling down the bowl

109
00:06:12,880 --> 00:06:13,730
你理解不了
doesn't work for you,

110
00:06:13,730 --> 00:06:14,616
别担心
don't worry about it.

111
00:06:14,610 --> 00:06:18,096
最后我们来看具体如何计算
Finally, let's look at some details on how you implement this.

112
00:06:18,090 --> 00:06:19,194
算法在此
Here's the algorithm

113
00:06:19,190 --> 00:06:23,480
所以你有两个超参数
and so you now have two hyperparameters,

114
00:06:23,480 --> 00:06:27,108
学习率α以及参数β
the learning rate alpha, as well as this parameter Beta,

115
00:06:27,108 --> 00:06:30,080
β控制着指数加权平均数
which controls your exponentially weighted average.

116
00:06:30,080 --> 00:06:33,073
β最常用的值是0.9
The most common value for Beta is 0.9.

117
00:06:33,070 --> 00:06:35,410
我们之前平均了过去十天的温度
We're averaging over the last ten days temperature.

118
00:06:35,410 --> 00:06:39,540
所以现在平均了前十次迭代的梯度
So it is averaging of the last ten iteration's gradients.

119
00:06:39,540 --> 00:06:42,768
实际上 β为0.9时 效果不错
And in practice,Beta equals 0.9 works very well.

120
00:06:42,760 --> 00:06:45,303
你可以尝试不同的值
Feel free to try different values and

121
00:06:45,300 --> 00:06:46,942
可以做一些超参数的研究
do some hyperparameter search,

122
00:06:46,940 --> 00:06:49,845
不过0.9是很棒的鲁棒数
but 0.9 appears to be a pretty robust value.

123
00:06:49,845 --> 00:06:51,932
那么关于偏差修正
Well, and how about bias correction, right?

124
00:06:51,930 --> 00:06:55,320
所以你要拿v_dW和v_db
So do you want to take v_dW and v_db and

125
00:06:55,320 --> 00:06:57,792
除以(1-β^t)
divide it by 1 minus beta to the t.

126
00:06:57,792 --> 00:06:59,800
实际上 人们不这么做
In practice, people don't usually do this

127
00:06:59,800 --> 00:07:02,075
因为10次迭代之后
because after just ten iterations,

128
00:07:02,075 --> 00:07:04,780
因为你的移动平均已经过了初始阶段
your moving average will have warmed up,

129
00:07:04,780 --> 00:07:06,499
不再是一个具有偏差的预测
and is no longer a bias estimate.

130
00:07:06,490 --> 00:07:07,485
实际中
So in practice,

131
00:07:07,485 --> 00:07:11,357
在使用梯度下降法或Momentum时
I don't really see people bothering with bias correction

132
00:07:11,357 --> 00:07:14,663
人们不会受到偏差修正的困扰
when implementing gradient descent or momentum.

133
00:07:14,663 --> 00:07:18,785
当然v_dW的初始值是0
And of course this process initialize the v_dW equals 0.

134
00:07:18,780 --> 00:07:21,448
要注意到这是
Note that this is a matrix of zeroes

135
00:07:21,440 --> 00:07:23,789
和dW拥有相同维数的零矩阵
with the same dimension as dW,

136
00:07:23,780 --> 00:07:26,400
也就是跟W拥有相同的维数
which has the same dimension as W.

137
00:07:26,400 --> 00:07:30,474
v_db的初始值也是向量零
And v_db is also initialized to a vector of zeroes.

138
00:07:30,470 --> 00:07:32,269
所以和db拥有相同的维数
So, the same dimension as db,

139
00:07:32,260 --> 00:07:35,057
也就是和b是同一个维数
which in turn has same dimension as b.

140
00:07:35,057 --> 00:07:37,200
最后要说一点
Finally, I just want to mention that

141
00:07:37,200 --> 00:07:39,000
如果你查阅了
if you read the literature on

142
00:07:39,000 --> 00:07:41,453
Momentum梯度下降法相关资料
gradient descent with momentum

143
00:07:41,453 --> 00:07:45,590
通常会看到一个被删除了的专业词汇
often you see it with this term omitted,

144
00:07:45,590 --> 00:07:48,291
1-β被删除了
with this 1 minus Beta term omitted.

145
00:07:48,290 --> 00:07:56,816
最后得到的是v_dW = β*v_dW + dW
So you end up with v_dW equals Beta v_dW plus dW.

146
00:07:56,816 --> 00:08:00,220
用紫色版本的结果就是
And the net effect of using this version in purple is that

147
00:08:00,220 --> 00:08:04,811
所以v_dW缩小了(1-β)倍
v_dW ends up being scaled by a factor of 1 minus Beta,

148
00:08:04,811 --> 00:08:07,300
相当于乘以1/(1-β)
or really 1 over 1 minus Beta.

149
00:08:07,300 --> 00:08:09,980
所以你要用梯度下降最新值的话
And so when you're performing these gradient descent updates,

150
00:08:09,980 --> 00:08:15,853
α要根据1/(1-β)相应变化
alpha just needs to change by a corresponding value of 1 over 1 minus Beta.

151
00:08:15,853 --> 00:08:18,800
实际上 二者效果都不错
In practice, both of these will work just fine,

152
00:08:18,800 --> 00:08:23,374
只是会影响到学习率α的最佳值
it just affects what's the best value of the learning rate alpha.

153
00:08:23,374 --> 00:08:28,350
我觉得这个公式用起来没有那么自然
But I find that this particular formulation is a little less intuitive.

154
00:08:28,350 --> 00:08:30,885
因为有一个影响
Because one impact of this is that

155
00:08:30,880 --> 00:08:33,331
如果你最后要调整超参数β
if you end up tuning the hyperparameter Beta,

156
00:08:33,331 --> 00:08:37,770
就会影响到v_dW和v_db
then this affects the scaling of v_dW and v_db as well.

157
00:08:37,770 --> 00:08:42,710
你也许还要修改学习率α
And so you end up needing to retune the learning rate, alpha, as well, maybe.

158
00:08:42,710 --> 00:08:46,802
所以我更喜欢左边的公式
So I personally prefer the formulation that I have written here on the left,

159
00:08:46,800 --> 00:08:49,220
而不是删去了1-β的这个公式
rather than leaving out the 1 minus Beta term.

160
00:08:49,220 --> 00:08:52,450
所以我更倾向于使用左边的公式
But, so I tend to use the formula on the left,

161
00:08:52,450 --> 00:08:54,896
也就是有1-β的这个公式
the printed formula with the 1 minus Beta term.

162
00:08:54,896 --> 00:08:57,980
但是两个公式都将β设置为0.9
But both versions having Beta equal 0.9

163
00:08:57,980 --> 00:09:00,280
是超参数的常见选择
is a common choice of hyperparameter.

164
00:09:00,280 --> 00:09:03,293
只是在这两个公式中
it's just at alpha the learning rate would need to be tuned differently

165
00:09:03,293 --> 00:09:04,880
学习率α的调整会有所不同
for these two different versions.

166
00:09:04,880 --> 00:09:07,500
所以这就是Momentum梯度下降法
So that's it for gradient descent with momentum.

167
00:09:07,500 --> 00:09:10,360
这个算法肯定要好于
This will almost always work better than

168
00:09:10,360 --> 00:09:13,740
没有momentum的梯度下降算法
the straightforward gradient descent algorithm without momentum.

169
00:09:13,740 --> 00:09:15,180
我们还可以做别的事情
But there's still other things we could do

170
00:09:15,180 --> 00:09:17,020
来加快学习算法
to speed up your learning algorithm.

171
00:09:17,020 --> 00:09:18,531
我们将在接下来的视频中
Let's continue talking about these

172
00:09:18,531 --> 00:09:21,920
继续探讨这些问题
in the next couple videos.

