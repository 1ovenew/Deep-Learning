1
00:00:00,400 --> 00:00:04,000
梯度检验帮我节省了很多时间(字幕来源：网易云课堂)
 Gradient checking is a technique that's helped me save tons of time, and

2
00:00:04,000 --> 00:00:08,500
也多次帮我发现backprop实施过程中的bug
helped me find bugs in my implementations of back propagation many times.

3
00:00:08,500 --> 00:00:10,890
接下来 我们看看如何利用它来调试或
 Let's see how you could use it too to debug, or

4
00:00:10,890 --> 00:00:14,885
检验backprop的实施是否正确
to verify that your implementation and back props correct.

5
00:00:14,885 --> 00:00:20,975
假设你的网络中含有下列参数 W[1]和b[1].... W[L]和b[L]
 So your new network will have some sort of parameters, W1, B1 and so on up to WL bL.

6
00:00:20,975 --> 00:00:23,935
为了执行梯度检验 首先要做的就是
 So to implement gradient checking, the first thing you should do is take all your

7
00:00:23,935 --> 00:00:28,835
把所有参数转换成一个巨大的向量数据
 parameters and reshape them into a giant vector data.

8
00:00:28,835 --> 00:00:34,860
你要做的就是把矩阵W转换成一个向量
 So what you should do is take W which is a matrix, and reshape it into a vector.

9
00:00:34,860 --> 00:00:39,850
把所有W矩阵转换成向量之后
 You gotta take all of these Ws and reshape them into vectors, and then concatenate

10
00:00:39,850 --> 00:00:45,170
做连接运算 得到一个巨型向量θ
 all of these things, so that you have a giant vector theta.

11
00:00:45,170 --> 00:00:47,020
该向量表示为参数θ
 Giant vector parameter as theta.

12
00:00:47,020 --> 00:00:52,720
代价函数J是所有W和b的函数
 So we say that the cos function J being a function of the Ws and

13
00:00:52,720 --> 00:00:58,380
现在你得到了一个θ的代价函数J
 Bs, You would now have the cost function J being just a function of theta.

14
00:00:58,380 --> 00:01:02,160
接着 你得到与W和b顺序相同的数据
 Next, with W and B ordered the same way,

15
00:01:02,160 --> 00:01:07,740
W[1]和db[1]...
 you can also take dW[1], db[1] and so on, and initiate them into big,

16
00:01:07,740 --> 00:01:12,200
用它们来初始化大向量dθ 它与θ具有相同维度
 giant vector d theta of the same dimension as theta.

17
00:01:12,200 --> 00:01:17,210
同样地 把dW[1]转换成矩阵 db[1]已经是一个向量了
 So same as before, we shape dW[1] into the matrix, db[1] is already a vector.

18
00:01:17,210 --> 00:01:21,220
直到把dW[L]转换成矩阵 这样所有的dW都已是矩阵
 We shape dW[L],all of the dW's which are matrices.

19
00:01:21,220 --> 00:01:24,632
注意 dW[1]与W1具有相同维度
 Remember, dW1 has the same dimension as W1.

20
00:01:24,632 --> 00:01:27,080
db1与b1具有相同维度
 db1 has the same dimension as b1.

21
00:01:27,080 --> 00:01:31,252
经过相同的转换和连接运算操作之后
 So the same sort of reshaping and concatenation operation,

22
00:01:31,252 --> 00:01:36,343
你可以把所有导数转换成一个大向量dθ
 you can then reshape all of these derivatives into a giant vector d theta.

23
00:01:36,343 --> 00:01:38,750
它与θ具有相同维度
 Which has the same dimension as theta.

24
00:01:38,750 --> 00:01:43,780
现在的问题是dθ
 So the question is, now,is the theta the gradient or

25
00:01:43,780 --> 00:01:47,310
代价函数J的梯度或坡度有什么关系
 the slope of the cos function J?

26
00:01:47,310 --> 00:01:49,620
这就是实施梯度检验的过程
 So here's how you implement gradient checking, and

27
00:01:49,620 --> 00:01:52,740
英语里通常简称为“grad check”
 often abbreviate gradient checking to grad check.

28
00:01:52,740 --> 00:01:57,690
首先 我们要清楚
 So first we remember that J Is now a function of the giant parameter,

29
00:01:57,690 --> 00:01:58,277
J是超级参数θ的一个函数
 theta, right?

30
00:01:58,277 --> 00:02:04,750
你也可以将J函数展开为J(θ1,θ2,θ3....)
 So expands to j is a function of theta 1,theta 2, theta 3, and so on.

31
00:02:06,880 --> 00:02:11,618
不论超级参数向量θ的维度是多少
 Whatever's the dimension of this giant parameter vector theta.

32
00:02:11,618 --> 00:02:18,519
为了实施梯度检验 你要做的就是循环执行
 So to implement grad check, what you're going to do is implements a loop so

33
00:02:18,519 --> 00:02:23,008
从而对每个i 也就是对每个θ组成元素
 that for each I, so for each component of theta,

34
00:02:23,008 --> 00:02:26,416
计算Dθapprox[i]的值
 let's compute D theta approx i to b.

35
00:02:26,416 --> 00:02:28,170
我使用双边误差
 And I want to take a two sided difference.

36
00:02:28,170 --> 00:02:29,870
也就是θ的J函数
 So I'll take J of theta.

37
00:02:30,020 --> 00:02:34,070
J(θ1, θ2, ….θi)
 Theta 1, theta 2, up to theta i.

38
00:02:34,170 --> 00:02:38,200
θi调整为θi+ε
 And we're going to nudge theta i to add epsilon to this.

39
00:02:38,270 --> 00:02:42,500
只对θi增加ε 其它项保持不变
 So just increase theta i by epsilon, and keep everything else the same.

40
00:02:42,700 --> 00:02:45,120
因为我们使用的是双边误差
And because we're taking a two sided difference,

41
00:02:45,220 --> 00:02:50,970
对另一边做同样的操作 只不过是减去ε
we're going to do the same on the other side with theta i, but now minus epsilon.

42
00:02:51,220 --> 00:02:54,170
θ其它项全都不保持不变
 And then all of the other elements of theta are left alone.

43
00:02:54,250 --> 00:02:59,020
最后得到 J(θ1, θ2, …. θi+ε,…)- J(θ1, θ2, …. θi-ε,…)/2
 And then we'll take this, and we'll divide it by 2 theta.

44
00:02:59,450 --> 00:03:03,120
从上节课中我们了解到
And what we saw from the previous video is that

45
00:03:03,250 --> 00:03:10,000
这个值应该逼近dθi
 this should be approximately equal to d theta [i],

46
00:03:10,170 --> 00:03:16,520
等于偏导数J比上θi 即
  which is supposed to be the partial derivative of J or of respect to(请参考讲义), I guess theta i,

47
00:03:16,650 --> 00:03:21,100
dθi是代价函数的偏导数
 if d theta i is the derivative of the cost function J.

48
00:03:21,200 --> 00:03:24,850
然后你需要对i的每个值都执行这个运算
So what you gonna do is you're gonna compute to this for every value of i.

49
00:03:24,970 --> 00:03:28,360
最后得到两个向量
And at the end, you now end up with two vectors.

50
00:03:28,400 --> 00:03:32,070
得到dθ的逼近值Dθapprox
You end up with this d theta approx, and

51
00:03:32,100 --> 00:03:35,860
它与dθ具有相同维度
  and this is going to be the same dimension as d theta.

52
00:03:35,950 --> 00:03:39,370
它们两个与θ具有相同维度
And both of these are in turn the same dimension as theta.

53
00:03:39,420 --> 00:03:43,920
你要做的就是验证这些向量是否彼此接近
And what you want to do is check if these vectors are approximately equal to each other.

54
00:03:43,970 --> 00:03:45,725
具体来说
 So, in detail, well, how do you define

55
00:03:45,825 --> 00:03:50,750
如何定义两个向量是否真的接近彼此
  whether or not two vectors are really reasonably close to each other?

56
00:03:50,800 --> 00:03:52,590
我一般做下列运算
 What I do is the following.

57
00:03:52,650 --> 00:03:57,320
计算这两个向量的距离
 I would compute the distance between these two vectors,

58
00:03:57,400 --> 00:04:01,670
dθapprox-dθ的欧几里得范数
 d theta approx minus d theta, so just the o2 norm(请参考讲义) of this.

59
00:04:01,750 --> 00:04:03,700
注意 这里没有平方
  Notice there's no square on top, so

60
00:04:03,800 --> 00:04:06,600
它是误差平方之和
 this is the sum of squares of elements of the differences,

61
00:04:06,720 --> 00:04:09,700
然后求平方根 得到欧式距离
and then you take a square root, as you get the Euclidean distance.

62
00:04:09,920 --> 00:04:18,970
然后用向量长度做归一化 结果为||dθapprox-dθ||2/ ||dθapprox||+|| dθ||
And then just to normalize by the lengths of these vectors, divide by dθ approx plus dθ.

63
00:04:19,050 --> 00:04:22,400
使用向量长度的欧几里得范数
 Just take the Euclidean lengths of these vectors.

64
00:04:22,520 --> 00:04:26,920
分母只是用于预防这些向量太小或太大
 And the row for the denominator is just in case any of these vectors are really small

65
00:04:26,970 --> 00:04:32,550
分母使得这个方程式变成比率
 or really large, the denominator turns this formula into a ratio.

66
00:04:32,650 --> 00:04:34,850
我们实际执行这个方程式
 So we implement this in practice,

67
00:04:34,920 --> 00:04:39,890
ε值可能为10的-7次方
I use epsilon equals maybe 10 to the minus 7, so minus 7.

68
00:04:39,920 --> 00:04:44,620
使用这个取值范围内的ε 如果你发现
 And with this range of epsilon, if you find that this formula gives you

69
00:04:44,700 --> 00:04:49,400
计算方程式得到的值为10的-7次方或更小 这就很好
 a value like 10 to the minus 7 or smaller, then that's great.

70
00:04:49,470 --> 00:04:53,220
这意味着导数逼近很有可能是正确的
 It means that your derivative approximation is very likely correct.

71
00:04:53,300 --> 00:04:55,020
它的值非常小
 This is just a very small value.

72
00:04:55,150 --> 00:05:00,620
如果它的值在10-5范围内 我就要小心了
 If it's maybe on the range of 10 to the -5, I would take a careful look.

73
00:05:00,700 --> 00:05:01,920
也许这个值没问题
 Maybe this is okay.

74
00:05:02,020 --> 00:05:05,170
但我会再次检查这个向量的所有项
 But I might double-check the components of this vector, and

75
00:05:05,230 --> 00:05:07,860
确保没有一项误差过大
  make sure that none of the components are too large.

76
00:05:07,870 --> 00:05:10,720
如果有一项误差非常大
  And if some of the components of this difference are very large,

77
00:05:10,800 --> 00:05:12,450
可能这里有bug
 then maybe you have a bug somewhere.

78
00:05:12,520 --> 00:05:17,000
如果左边这个方程式结果是10的-3次方
And if this formula on the left is on the other is -3,

79
00:05:17,100 --> 00:05:21,520
我就会担心是否存在bug
 then I would worry, I would be much more concerned that maybe there's a bug somewhere.

80
00:05:21,570 --> 00:05:25,200
计算结果应该比10的-3次方小很多
 But you should really be getting values much smaller than 10 minus 3,

81
00:05:25,270 --> 00:05:29,690
如果比10-3次方大很多 我就会很担心
  or if there is any bigger than 10 to minus 3, then I would be quite concerned.

82
00:05:29,750 --> 00:05:32,750
担心是否存在bug
 I would seriously worry about whether or not there might be a bug.

83
00:05:32,870 --> 00:05:36,970
这时应该仔细检查所有θ项
And I would then, you should then look at the individual components of data(应该是theta)

84
00:05:37,020 --> 00:05:41,790
看是否有一个具体的i值
  to see if there's a specific value of i for which

85
00:05:42,000 --> 00:05:45,850
使得dθapprox与dθ大不相同
  d theta approx i is very different from d theta i,

86
00:05:45,920 --> 00:05:50,570
并用它来追踪一些求导计算是否正确
and use that to try to track down whether or not some of your derivative computations might be incorrect.

87
00:05:50,750 --> 00:05:53,520
经过一些调试 最终
 And after some amounts of debugging, it finally,

88
00:05:53,650 --> 00:05:56,750
结果会是这种非常小的值
  it ends up being this kind of very small value,

89
00:05:56,870 --> 00:05:59,520
那么 你的实施可能是正确的
  then you probably have a correct implementation.

90
00:05:59,600 --> 00:06:01,320
在实施神经网络时
So when implementing a neural network,

91
00:06:01,400 --> 00:06:04,700
我经常需要执行foreprop和backprop
  what often happens is I'll implement foreprop, implement backprop.

92
00:06:04,840 --> 00:06:08,610
然后我可能发现这个梯度检验有一个相对较大的值
And then I might find that this Grad check has a relatively big value.

93
00:06:08,700 --> 00:06:12,350
我会怀疑存在bug 然后开始调试 调试 调试
 And then I will suspect that there must be a bug, go in debug, debug, debug.

94
00:06:12,460 --> 00:06:16,900
调试一段时间后 我得到一个很小的梯度检验值
And after debugging for a while, If I find that it passes grad check with a small value,

95
00:06:17,000 --> 00:06:19,720
现在我可以很自信的说 神经网络实施是正确的
then you can be much more confident that it's then correct.

96
00:06:19,850 --> 00:06:22,200
现在你已经了解了梯度检验的工作原理
 So you now know how gradient checking works.

97
00:06:22,270 --> 00:06:25,300
它帮助我在神经网络实施中发现了很多bug
 This has helped me find lots of bugs in my implementations of neural nets,

98
00:06:25,400 --> 00:06:27,070
希望它对你也有所帮助
and I hope it'll help you too.

99
00:06:27,150 --> 00:06:30,620
下节课 我准备讲一讲
In the next video, I want to share with you some tips or some notes

100
00:06:30,720 --> 00:06:33,300
实施梯度检验的技巧和注意事项
 on how to actually implement gradient checking.

101
00:06:33,400 --> 00:06:34,640
下节课见
 Let's go onto the next video.

