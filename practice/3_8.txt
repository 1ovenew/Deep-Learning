1
00:00:00,600 --> 00:00:04,010
到目前为止 我们讲到过的分类的例子都使用了(字幕来源：网易云课堂)
So far, the classification examples we've talked about have used

2
00:00:04,010 --> 00:00:08,280
二分分类  这种分类只有两种可能的标记 0或1
binary classification, where you had two possible labels, 0 or 1.

3
00:00:08,280 --> 00:00:10,390
这是一只猫或者不是一只猫
Is it a cat, is it not a cat?

4
00:00:10,390 --> 00:00:12,920
如果我们有多种可能的类型的话呢？
What if we have multiple possible classes?

5
00:00:12,920 --> 00:00:17,010
有一种logistic回归的一般形式 叫做Softmax回归
There's a generalization of logistic regression called Softmax regression

6
00:00:17,010 --> 00:00:21,000
能让你在试图识别某一分类时做出预测
that lets you make predictions where you're trying to recognize one of C or

7
00:00:21,000 --> 00:00:26,150
或者说是多种分类中的一个  不只是识别两个分类
one of multiple classes, rather than just recognize two classes.

8
00:00:26,150 --> 00:00:26,780
我们来一起看一下
Let's take a look.

9
00:00:26,780 --> 00:00:29,644
假设你不单需要识别猫
Let's say that instead of just recognizing cats

10
00:00:29,680 --> 00:00:32,444
而是想要识别猫  狗和小鸡
you want to recognize cats, dogs, and baby chicks.

11
00:00:32,488 --> 00:00:37,920
我把猫叫做类1  狗为类2  小鸡是类3
So I'm going to call cats class 1, dogs class 2, baby chicks class 3.

12
00:00:37,920 --> 00:00:41,555
如果不属于以上任何一类  就分到"其他"或者说
And if none of the above, then there's an other or

13
00:00:41,555 --> 00:00:44,970
"以上均不符合"这一类  我把它叫做类0
a none of the above class, which I'm going to call class 0.

14
00:00:45,044 --> 00:00:49,770
这里显示的图片及其对应的分类就是一个例子
So here's an example of the images and the classes they belong to.

15
00:00:49,770 --> 00:00:52,550
这幅图片上是一只小鸡  所以是类3
That's a picture of a baby chick, so the class is 3.

16
00:00:52,550 --> 00:00:55,622
猫是类1  狗是类2
Cats is class 1, dog is class 2,

17
00:00:55,620 --> 00:00:59,200
我猜这是一只考拉  所以以上均不符合
I guess that's a koala, so that's none of the above,

18
00:00:59,244 --> 00:01:02,360
那就是类0  下一个类3  以此类推
so that is class 0. Class 3 and so on.

19
00:01:03,000 --> 00:01:07,420
我们将会用符号表示  我会用大写C
So the notation we're going to use is, I'm going to use capital C to

20
00:01:07,420 --> 00:01:13,755
来表示你的输入会被分入的类别总个数
denote the number of classes you're trying to categorize your inputs into.

21
00:01:13,800 --> 00:01:16,977
在这个例子中  我们有4种可能的类别
And in this case, you have four possible classes,

22
00:01:17,040 --> 00:01:19,644
包括"其他"或"以上均不符合"这一类
including the other or the none of the above class.

23
00:01:19,688 --> 00:01:24,311
当有4个分类时  指示类别的数字
So when you have four classes, the numbers indexing your classes

24
00:01:24,350 --> 00:01:28,888
就是从0到大写C减1
would be 0 through capital C minus one.

25
00:01:28,911 --> 00:01:31,420
换句话说  就是0  1  2  3
So in other words, that would be zero, one, two or three.

26
00:01:31,420 --> 00:01:36,520
在这个例子中  我们将建立一个神经网络  其输出层有4个
In this case, we're going to build a neural network  where the output layer has four,

27
00:01:36,520 --> 00:01:42,444
或者说变量大写字母C个输出单元
or in this case the variable capital alphabet C output units.

28
00:01:43,010 --> 00:01:48,770
因此n  即输出层也就是L层的单元数量
So n, the number of units output layer which is layer L

29
00:01:48,770 --> 00:01:52,911
等于4  或者一般而言等于C
is going to equal to 4 or in general this is going to equal to C.

30
00:01:52,930 --> 00:01:57,688
我们想要输出层单元的数字告诉我们
And what we want is for the number of units in the output layer to tell us

31
00:01:57,711 --> 00:02:00,730
这4种类型中每一个的概率有多大
what is the probability of each of these four classes.

32
00:02:00,730 --> 00:02:04,190
所以这里的第一个节点输出的应该是
So the first node here is supposed to output,

33
00:02:04,190 --> 00:02:09,222
或者说我们希望它输出"其他"类的概率  在输入x的情况下
or we want it to output the probability that is the other class, given the input x,

34
00:02:09,370 --> 00:02:13,644
这个会输出是猫的概率  在输入x的情况下
This will output probability there's a cat, given an x.

35
00:02:13,688 --> 00:02:17,800
这个会输出是狗的概率  在输入x的情况下
This will output probability that there's a dog, given an x.

36
00:02:17,866 --> 00:02:20,040
这个会输出...
That will output the probability...

37
00:02:20,040 --> 00:02:23,450
我把小鸡缩写为bc
I'm just going to abbreviate baby chick to bc

38
00:02:23,450 --> 00:02:27,780
小鸡即bc的概率  在输入x的情况下
So probability of a baby chick, abbreviated to bc, given the input x.

39
00:02:29,030 --> 00:02:36,470
因此这里的ŷ将是一个4*1维向量
So here, the output labels y hat is going to be a four by one dimensional vector,

40
00:02:36,470 --> 00:02:41,888
因为它必须输出四个数字  给你这四种概率
because it now has to output four numbers, giving you these four probabilities.

41
00:02:42,720 --> 00:02:45,022
因为它加起来应该等于1
And because probably it should sum to one,

42
00:02:45,044 --> 00:02:48,850
输出的ŷ中的四个数字加起来应该等于1
the four numbers in the output y hat, they should sum to one.

43
00:02:50,500 --> 00:02:55,260
让你的网络做到这一点的标准模型要用到
The standard model for getting your network to do this uses what's called

44
00:02:55,260 --> 00:03:00,040
Softmax层  以及输出层来生成输出
a Softmax layer, and the output layer in order to generate these outputs.

45
00:03:00,040 --> 00:03:01,000
让我把式子写下来
Let me write down the math,

46
00:03:01,066 --> 00:03:04,844
然后回过头来  就会对Softmax的作用有一点感觉了
then you can come back and get some intuition about what the Softmax there is doing.

47
00:03:06,480 --> 00:03:08,810
在神经网络的最后一层
So in the final layer of the neural network,

48
00:03:08,810 --> 00:03:13,230
你将会像往常一样计算各层的线性部分
you are going to compute as usual the linear part of the layers.

49
00:03:13,230 --> 00:03:17,670
z 大写L  这是最后一层的z变量
So z, capital L, that's the z variable for the final layer.

50
00:03:17,670 --> 00:03:21,850
记住这是大写L层
So remember this is layer capital L.

51
00:03:21,850 --> 00:03:27,850
和往常一样  计算方法是w^[L]乘以上一层的激活值
So as usual you compute that as w^[L] times the activation of the previous layer

52
00:03:27,850 --> 00:03:32,040
再加上最后一层的偏差
plus the biases for that final layer.

53
00:03:32,040 --> 00:03:33,200
算出了z之后
Now having computer z,

54
00:03:33,244 --> 00:03:37,560
你需要应用Softmax激活函数
you now need to apply what's called the Softmax activation function.

55
00:03:38,750 --> 00:03:43,155
这个激活函数对于Softmax层而言有些不同
So that activation function is a bit unusual for the Softmax layer,

56
00:03:43,155 --> 00:03:44,422
它的作用是这样的
but this is what it does.

57
00:03:45,250 --> 00:03:49,622
首先我们要计算一个临时变量
First, we're going to computes a temporary variable,

58
00:03:49,666 --> 00:03:54,511
我们把它叫做t  它等于e的z^[L]次方
which we're going to call t, which is e to the z^[L].

59
00:03:54,555 --> 00:03:56,288
这适用于每个元素
So this is applied element-wise.

60
00:03:56,311 --> 00:04:01,000
而这里的z^[L]  在我们的例子中  z^[L]是4*1的
So z^[L] here, in our example, z^[L] is going to be four by one

61
00:04:01,066 --> 00:04:03,340
四维向量
This is a four dimensional vector.

62
00:04:03,340 --> 00:04:08,590
t就是e的z^[L]次方  这是对所有元素求幂
So t itself e to the zL, that's an element-wise exponentiation.

63
00:04:08,590 --> 00:04:12,970
t也是一个4*1维向量
T will also be a 4 by 1 dimensional vector.

64
00:04:12,970 --> 00:04:15,533
然后输出的a^[L]
Then the output a^[L],

65
00:04:15,555 --> 00:04:20,755
基本上就是向量t  但是会归一化  使和为1
is going to be basically the vector t but normalized to sum to 1.

66
00:04:20,750 --> 00:04:24,466
因此a^[L]等于e的z^[L]次方
So aL is going to be e to the z^[L]

67
00:04:24,460 --> 00:04:30,355
除以j等于1到4时的总和
divided by sum from J equal 1 through 4,

68
00:04:30,400 --> 00:04:34,330
因为我们有4个类型的t下标i
because we have four classes of t subscript i.

69
00:04:34,330 --> 00:04:40,977
换句话说  a^[L]也是一个4*1维向量
So in other words of saying that a^[L] is also a four by one vector,

70
00:04:41,044 --> 00:04:45,120
而这个四维向量的i元素
and the i element of this four dimensional vector,

71
00:04:45,120 --> 00:04:56,044
我把它写下来  a^[L]下标i等于t_i除以t_i之和
let's write that, a^[L] subscript i that's going to be equal to t_i over sum of t_i, okay?

72
00:04:56,100 --> 00:04:58,130
以防这里的计算不够清晰易懂
In case this math isn't clear,

73
00:04:58,130 --> 00:05:01,760
我们马上会举个例子来详细解释
we'll do an example in a minute that will make this clearer.

74
00:05:02,240 --> 00:05:03,750
以防计算不够清晰
So in case this math isn't clear,

75
00:05:03,750 --> 00:05:06,690
我们来看一个例子  详细解释
let's go through a specific example that will make this clearer.

76
00:05:07,200 --> 00:05:10,820
假设你算出了z^[L]
Let's say that you compute z^[L], and

77
00:05:10,820 --> 00:05:18,190
z^[L]是一个四维向量  假设为5  2  -1  3
z^[L] is a four dimensional vector, let's say is 5, 2, -1, 3.

78
00:05:18,190 --> 00:05:22,170
我们要做的就是用这个元素取幂方法
What we're going to do is use this element-wise exponentiation to

79
00:05:22,170 --> 00:05:23,580
来计算向量t
compute this vector t.

80
00:05:23,580 --> 00:05:29,380
因此t就是e^5 e^2 e^(-1) e^3
So t is going to be e to the 5, e to the 2, e to the -1, e to the 3.

81
00:05:29,380 --> 00:05:32,911
如果你按一下计算器就会得到以下值
And if you plug that in the calculator, these are the values you get.

82
00:05:32,955 --> 00:05:38,670
e^5是148.4  e^2约为7.4
E to the 5 is 148.4, e squared is about 7.4,

83
00:05:38,670 --> 00:05:44,610
e^(-1)是0.4  e^3是20.1
to the -1 is 0.4, and e cubed is 20.1.

84
00:05:44,610 --> 00:05:49,430
我们从向量t得到向量a^[L]就只需
And so, the way we go from the vector t to the vector a^[L] is just

85
00:05:49,430 --> 00:05:52,830
将这些项目归一化使总和为1
to normalize these entries to sum to one.

86
00:05:52,830 --> 00:05:56,720
如果你把t的元素都加起来
So if you sum up the elements of t,

87
00:05:56,720 --> 00:06:03,150
把这四个数字加起来  得到176.3
if you just add up those 4 numbers you get 176.3.

88
00:06:03,150 --> 00:06:09,480
最终a^[L]就等于向量t
So finally, a^[L] is just going to be this vector t,

89
00:06:09,480 --> 00:06:14,430
作为一个向量  除以176.3
as a vector, divided by 176.3.

90
00:06:14,430 --> 00:06:18,500
例如这里的第一个节点
So for example, this first node here,

91
00:06:18,500 --> 00:06:23,800
它会输出e^5除以176.3
this will output e to the 5 divided by 176.3.

92
00:06:23,800 --> 00:06:27,690
也就是0.842
And that turns out to be 0.842.

93
00:06:27,690 --> 00:06:32,590
这样说来  对于这张图片  如果这是你得到的z值
So saying that, for this image, if this is the value of z you get,

94
00:06:32,590 --> 00:06:36,350
它是类0的概率就是84.2%
the chance of it being class zero is 84.2%.

95
00:06:36,350 --> 00:06:42,777
下一个节点输出e^2除以176.3
And then the next nodes outputs e squared over 176.3,

96
00:06:42,933 --> 00:06:48,120
等于0.042  也就是4.2%的几率
that turns out to be 0.042, so this is 4.2% chance.

97
00:06:48,120 --> 00:06:56,810
下一个是e^(-1)除以它  即0.002
The next one is e to -1 over that, which is 0.002.

98
00:06:56,810 --> 00:07:04,555
最后一个是e^3除以它  等于0.114
And the final one is e cubed over that, which is 0.114.

99
00:07:04,600 --> 00:07:08,844
也就是11.4%的概率属于类3
So it is 11.4% chance that this is class number three,

100
00:07:08,844 --> 00:07:10,933
也就是小鸡组  对吧
which is the baby chick class, right?

101
00:07:10,977 --> 00:07:15,480
这就是它属于类0  类1  类2  类3的可能性
So there's a chance of it being class zero, class one, class two, class three.

102
00:07:15,480 --> 00:07:21,850
神经网络的输出a^[L]  也就是ŷ
So the output of the neural network a^[L], this is also y hat,

103
00:07:21,850 --> 00:07:23,800
是一个4乘1维向量
this is a 4 by 1 vector

104
00:07:23,888 --> 00:07:30,980
这个4乘1向量的元素就是我们算出来的这四个数字
where the elements of this 4 by 1 vector are going to be these four numbers that we just compute it.

105
00:07:30,980 --> 00:07:38,577
这种算法通过向量z^[L]计算出总和为1的四个概率
So this algorithm takes the vector z^[L] and mathes it to four probabilities that sum to 1.

106
00:07:38,600 --> 00:07:44,666
如果我们总结一下从z^[L]到a^[L]的计算步骤
And if we summarize what we just did to math from z^[L] to a^[L],

107
00:07:44,711 --> 00:07:47,660
整个计算过程  从计算幂
this whole computation--computing exponentiation to

108
00:07:47,660 --> 00:07:52,000
到得出临时变量t  再归一化
get this temporary variable t and then normalizing,

109
00:07:52,022 --> 00:07:56,511
我们可以将此概括为一个Softmax激活函数
we can summarize this into a Softmax activation function and

110
00:07:56,555 --> 00:08:03,540
设a^[L]等于向量z^[L]的激活函数g
say a^[L] equals the activation function g applied to the vector z^[L].

111
00:08:03,540 --> 00:08:07,333
这一激活函数的与众不同之处在于
The unusual thing about this particular activation function is that,

112
00:08:07,355 --> 00:08:11,822
这个激活函数g需要输入一个4*1维向量
this activation function g, it takes us input a 4 by 1 vector and

113
00:08:11,844 --> 00:08:14,980
然后输出一个4*1维向量
it outputs a 4 by 1 vector.

114
00:08:14,980 --> 00:08:19,200
之前  我们的激活函数都是接受单行数值输入
So previously, our activation functions used to take in a single row value input.

115
00:08:19,200 --> 00:08:22,883
例如Sigmoid和ReLU激活函数
So for example, the Sigmoid and the ReLU activation functions

116
00:08:22,880 --> 00:08:25,577
输入一个实数  输出一个实数
input a real number and output a real number.

117
00:08:25,622 --> 00:08:28,711
Softmax激活函数的特殊之处在于
The unusual thing about the Softmax activation function is,

118
00:08:28,733 --> 00:08:32,466
因为需要将所有可能的输出归一化
because it needs to normalize across the different possible outputs,

119
00:08:32,511 --> 00:08:35,844
就需要输入一个向量  最后输出一个向量
it needs to take in a vector of input and then outputs a vector.

120
00:08:35,888 --> 00:08:39,444
那么Softmax分类器还可以代表其他的什么东西呢？
So what other things that a Softmax classifier can represent?

121
00:08:39,488 --> 00:08:43,955
我来举几个例子  你有两个输入x_1  x_2
I'm going to show you some examples where you have inputs x_1, x_2.

122
00:08:44,000 --> 00:08:48,350
它们直接输入到Softmax层
And these feed directly to a Softmax layer

123
00:08:48,350 --> 00:08:53,420
它有三四个或者更多的输出节点  输出ŷ
that has three or four, or more output nodes that then outputs y hat.

124
00:08:53,420 --> 00:08:57,000
我将向你展示一个没有隐藏层的神经网络
So I'm going to show you a neural network with no hidden layer,

125
00:08:57,111 --> 00:09:05,000
它所做的就是计算z^[1]=w^[1]*x（输入）+b
and all it does is compute z^[1] equals w^[1] times the input x plus b.

126
00:09:05,170 --> 00:09:09,555
而输出的a^[1]或者说ŷ
And then the output a^[1], or y hat

127
00:09:09,640 --> 00:09:13,466
就是z^[1]的Softmax激活函数
is just the Softmax activation function applied to z^[1].

128
00:09:13,488 --> 00:09:16,466
这个没有隐藏层的神经网络
So in this neural network with no hidden layers,

129
00:09:16,511 --> 00:09:20,180
应该能让你对Softmax函数能够代表的东西有所了解
it should give you a sense of the types of things a Softmax function can represent.

130
00:09:20,180 --> 00:09:24,177
这个例子中  原始输入只有x_1和x_2
So here's one example with just raw inputs x_1 and x_2.

131
00:09:24,266 --> 00:09:30,266
一个C等于3个输出分类的Softmax层能够代表
A Softmax layer with C equals 3 output classes can represent

132
00:09:30,280 --> 00:09:32,044
这种类型的决策边界
this type of decision boundaries.

133
00:09:32,040 --> 00:09:35,666
请注意这是几条线性决策边界
Notice this is kind of several linear decision boundaries,

134
00:09:35,770 --> 00:09:39,800
但这使得它能够将数据分到3个类别中
but this allows it to separate out the data into three classes.

135
00:09:39,911 --> 00:09:43,422
在这张图表中  我们所做的是
And in this diagram, what we did was

136
00:09:43,466 --> 00:09:47,250
选择这张图中显示的训练集
we actually took the training set that's kind of shown in this figure and

137
00:09:47,250 --> 00:09:52,755
用数据的3种输出标签来训练Softmax分类器
train the Softmax classifier with three output labels on the data

138
00:09:52,822 --> 00:09:57,933
图中的颜色显示了Softmax分类器的输出的阈值
And then the color on this plot shows threshold in the outputs in the Softmax classifier

139
00:09:58,040 --> 00:10:03,466
输入的着色是基于三种输出中概率最高的那种
and coloring in the input base on which one of the three outputs have the highest probability.

140
00:10:03,710 --> 00:10:07,830
因此我们可以看到这是logistic回归的一般形式
So we can maybe kind of see that this is like a generalization of logistic regression

141
00:10:07,830 --> 00:10:12,977
有类似线性的决策边界  但有超过两个分类
with sort of linear decision boundaries, but with more than two classes

142
00:10:13,022 --> 00:10:16,644
分类不只有0和1  而是可以是0  1或2
instead of class being just 0, 1, the class could be 0, 1, or 2.

143
00:10:16,733 --> 00:10:21,466
这是另一个Softmax分类器可以代表的决策边界的例子
Here's another example of the decision boundary that the Softmax classifier can represent

144
00:10:21,530 --> 00:10:23,755
用有三个分类的数据集来训练
when training on a data set with three classes.

145
00:10:23,800 --> 00:10:25,955
这里还有一个
And here's another one.

146
00:10:26,111 --> 00:10:30,050
对吧  但是直觉告诉我们
Right, so this is a... but one intuition is that

147
00:10:30,050 --> 00:10:34,355
任何两个分类之间的决策边界都是线性的
the decision boundary between any two classes will be linear.

148
00:10:35,370 --> 00:10:37,822
这就是为什么你看到比如这里
That's why you see for example

149
00:10:37,888 --> 00:10:42,044
黄色和红色分类之间的决策边界是线性边界
that decision boundary between the yellow and the red classes, that's the linear boundary

150
00:10:42,040 --> 00:10:44,622
紫色和红色之间的也是线性边界
between purple and red is another linear boundary,

151
00:10:44,730 --> 00:10:47,377
紫色和黄色之间的也是线性决策边界
between the purple and yellow is another linear decision boundary.

152
00:10:47,422 --> 00:10:50,044
但它能用这些不同的线性函数
But it was able to use these different linear functions

153
00:10:50,180 --> 00:10:53,120
来把空间分成三类
in order to separate the space into three classes.

154
00:10:53,120 --> 00:10:55,920
我们来看一下更多分类的例子
Let's look at some examples with more classes.

155
00:10:55,920 --> 00:10:59,000
这个例子中C等于4
So it's an example with C equals 4

156
00:10:59,150 --> 00:11:03,511
因此这个绿色分类和Softmax仍旧可以代表
so that the green class and Softmax can continue to represent

157
00:11:03,555 --> 00:11:07,066
多种分类之间的这些类型的线性决策边界
these types of linear decision boundaries between multiple classes.

158
00:11:07,133 --> 00:11:11,977
另一个例子是C等于5类
So here's one more example with C equals 5 classes,

159
00:11:12,044 --> 00:11:15,022
最后一个例子是C等于6
and here's one last example with C equals 6.

160
00:11:15,088 --> 00:11:18,777
这显示了Softmax分类器在没有隐藏层的情况下
So this shows the type of things the Softmax classifier can do

161
00:11:18,770 --> 00:11:21,155
能够做到的事情
when there is no hidden layer

162
00:11:21,150 --> 00:11:24,488
当然  更深的神经网络会有x
Of course, even much deeper neural network with x and

163
00:11:24,500 --> 00:11:28,820
然后是一些隐藏单元  以及更多隐藏单元等等
then some hidden units, and then more hidden units, and so on,

164
00:11:28,820 --> 00:11:32,333
你就可以学习更复杂的非线性决策边界
then you can learn even more complex non-linear decision boundaries

165
00:11:32,333 --> 00:11:34,400
来区分多种不同分类
to separate out multiple different classes.

166
00:11:34,844 --> 00:11:37,950
我希望你了解了神经网络中的Softmax层
So I hope this gives you a sense of what a Softmax layer or

167
00:11:37,950 --> 00:11:41,780
或者Softmax激活函数有什么作用
the Softmax activation function in the neural network can do.

168
00:11:41,780 --> 00:11:44,400
下一个视频中  我们来看一下你该怎样训练
In the next video, let's take a look at how you can train

169
00:11:44,466 --> 00:11:47,222
一个使用Softmax层的神经网络
a neural network that uses a Softmax layer.

