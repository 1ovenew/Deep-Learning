训练神经网络 尤其是深度神经网络所面临的一个问题是(字幕来源：网易云课堂)，梯度消失或梯度爆炸，也就是说 当你训练深度网络时，导数或坡度有时会变得非常大，或非常小 甚至以指数方式变小，这加大了训练的难度，这节课 你将会了解梯度消失或爆炸问题的真正含义，以及如何更明智地选择随机初始化权重，从而避免这个问题，假设你正在训练这样一个极深的神经网络，为了节约幻灯片上的空间，我画的神经网络每层只有两个隐藏单元，但它可能含有更多，但这个神经网络会有参数w[1]，w[2] w[3]等等 直到w[L]，为了简单起见，假设我们使用激活函数g(z)=z，也就是线性激活函数，我们忽略b 假设b[L]=0，如果那样的话，输出y=w[L]*w[L-1]w[L-2] .....w[3]*Ww[2]*w1}*x，如果你想考验我的数学水平，w[1]*x=z[1]，因为b等于0，所以我想，z[1]=w[1]*x 因为b=0， a[1]=g(z[1])，因为我们使用了一个线性激活函数，它等于z[1]，所以第一项w[1]*x等于a[1]，通过推理 你会得出w[2]*w[1]*x=a[2]，因为a[2]=g(z[2])，还等于g(w[2]a[1])，可以用w[1]*x替换a[1]，所以这一项就等于a[2]，这个就是a[3]，所有这些矩阵数据传递的协议将给出ŷ而不是y的值，假设每个权重矩阵w[L]等于...，假设它比1大一点，w[L]=[1.5,0,1.5,0]，从技术上来讲 最后一项有不同维度，可能它就是余下的权重矩阵， ŷ等于，忽略最后这个不同维度的项，ŷ等于w[1][1.5,0,0,1.5)]的L-1次方乘以x，因为我们假设所有矩阵都等于它，它是1.5倍的单位矩阵 最后的计算结果就是ŷ，ŷ也就等于1.5^（L-1）乘以x，如果对于一个深度神经网络来说 L值较大，那么ŷ的值也会非常大，实际上它呈指数级增长的，它增长的比率是1.5^L，因此对于一个深度神经网络，y的值将爆炸式增长，相反地 如果权重是0.5，它比1小，这项也就变成了0.5^L，矩阵ŷ=w[1]乘以[0.5,0,0,0.5]的(L-1)次方再乘以x 再次忽略w[L]，因此每个矩阵都小于1，假设x[1] x[2]都是1，激活函数将变成1/2 1/2，1/4 1/4 1/8 1/8等，直到最后一项变成1/2L，所以作为自定义函数 激活函数的值将以指数级下降，它是与网络层数量L相关的函数，在深度网络中 激活函数以指数级递减，我希望你得到的直观理解是，权重W只比1略大一点，或者说只比单位矩阵大一点，深度神经网络的激活函数将爆炸式增长，如果W比1略小一点，可能是0.9,0.9，在深度神经网络中，激活函数将以指数级递减，虽然我只是论述了对L函数的激活函数，以指数级增长或下降，它也适用于，与层数L相关的导数或梯度函数，也是呈指数增长，或呈指数递减，对于当前的神经网络 假设L=150，最近Microsoft对152层神经网络的研究取得了很大进展，在这样一个深度神经网络中，如果作为L的函数的激活函数或梯度函数以指数级增长或递减，它们的值将会变得极大或极大，从而导致训练难度上升，尤其是梯度与L相差指数级，梯度下降算法的步长会非常非常小，梯度下降算法将花费很长时间来学习，总结一下，今天我们讲了深度神经网络是如何产生梯度消失或爆炸问题的，实际上 在很长一段时间内 它曾是训练深度神经网络的阻力，虽然有一个不能彻底解决此问题的解决方案，但是已在如何选择初始化权重问题上提供了很多帮助，我们下节课继续讲，