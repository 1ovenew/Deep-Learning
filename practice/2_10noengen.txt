In the early days of deep learning,people used to worry a lot about the optimization algorithmgetting stuck in bad local optima.But as this theory of deep learning has advanced,our understanding of local optima is also changing.Let me show you how we now think about local optimaand problems in the optimization problem in deep learning.This was a picture people used to have in mindwhen they worried about local optima.Maybe you are trying to optimize some set of parameters,we call them W_1 and W_2,and the height in the surface is the cost function.In this picture,it looks like there are a lot of local optima in all those places.And it'd be easy for gradient descent,or one of the other algorithms to get stuck in a local optimumrather than find its way to a global optimum.It turns out that if you are plotting a figurelike this in two dimensions,then it's easy to create plots like thiswith a lot of different local optima.And these very low dimensional plotsused to guide their intuition.But this intuition isn't actually correct.It turns out if you create a neural network,most points of zero gradientsare not local optima like points like this.Instead most points of zero gradientin a cost function are saddle points.So, that's a pointwhere this t is maybe W_1, W_2,and the height is the value of the cost function J.But informally, a function of very high dimensional space,if the gradient is zero,then in each directionit can either be a convex light functionor a concave light function.And if you are in, say, a 20,000 dimensional space,then for it to be a local optima,all 20,000 directions need to look like this.And so the chance of that happening is maybe very small,maybe two to the minus 20,000.Instead you're much more likely to get some directionswhere the curve bends up like so,as well as some directions where the curve function is bending downrather than have them all bend upwards.So that's why in very high-dimensional spacesyou're actually much more likely to run into a saddle pointlike that shown on the right,then the local optimum.As for why the surface is called a saddle point,if you can picture,maybe this is a sort of saddleyou put on a horse, right?Maybe this is a horse.This is a head of a horse,this is the eye of a horse.Well, not a good drawing of a horse but you get the idea.Then you, the rider,will sit here in the saddle.So that's why this point here,where the derivative is zero,that point is called a saddle point.There's really the point on this saddlewhere you would sit, I guess,and that happens to have derivative zero.And so, one of the lessonswe learned in history of deep learningis that a lot of our intuitions about low-dimensional spaces,like what you can plot on the left,they really don't transfer to the very high-dimensional spacesthat any other algorithms are operating over.Because if you have 20,000 parameters,then J as your function over 20,000 dimensional vector,then you're much more likely to see saddle pointsthan local optimum.If local optima aren't a problem,then what is a problem?It turns out that plateaus can really slow down learningand a plateau is a regionwhere the derivative is close to zero for a long time.So if you're here,then gradient descents will move down the surface,and because the gradient is zero or near zero,the surface is quite flat.You can actually take a very long time, you know,to slowly find your way to maybe this point on the plateau.And then because of a random perturbation of left or right,maybe then finally I'm going to switch pen colors for clarity.Your algorithm can then find its way off the plateau.Let it take this very long slope offbefore it's found its way hereand they could get off this plateau.So the takeaways from this video are,first, you're actually pretty unlikelyto get stuck in bad local optima,so long as you're training a reasonably large neural network,so a lot of parameters,and the cost function J isdefined over a relatively high dimensional space.But second, that plateaus are a problemand you can actually make learning pretty slow.And this is where algorithms like momentumor RmsProp or Adamcan really help your learning algorithm as well.And these are scenarioswhere more sophisticated observation algorithms,such as Adam,can actually speed up the rateat which you could move down the plateau and then get off the plateau.So because your network is solving optimizations problemsover such high dimensional spaces, to be honest,I don't think anyone has great intuitionsabout what these spaces really look like,and our understanding of them is still evolving.But I hope this gives you some better intuition aboutthe challenges that the optimization algorithms may face.So that's congratulations oncoming to the end of this week's content.Please take a look at this week's quiz as well as the exercise.I hope you enjoy practicing some of these ideas of this week exercise,and I look forward to seeing you at the start of next week's videos.