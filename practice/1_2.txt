1
00:00:00,375 --> 00:00:04,525
我注意到  几乎所有机器学习从业人员(字幕来源：网易云课堂)
I've noticed that almost all the really good machine learning
2
00:00:04,600 --> 00:00:08,025
都期望深刻理解偏差和方差
practitioners tend to be very sophisticated in understanding of Bias and Variance.
3
00:00:08,150 --> 00:00:12,375
这两个概念易学难精
Bias and Variance is one of those concepts that's easily learned but difficult to master.
4
00:00:12,375 --> 00:00:16,150
即使你自认为已经理解了方差和偏差的基本概念
Even if you think you've seen the basic concepts of Bias and Variance,
5
00:00:16,225 --> 00:00:18,875
却总有一些意想不到的新东西出现
there's often more new ones to it than you'd expect.
6
00:00:19,000 --> 00:00:22,250
关于深度学习的误差问题  另一个趋势是
In the Deep Learning Error, another trend is that
7
00:00:22,325 --> 00:00:26,000
对偏差方差的权衡研究甚浅
there's been less discussion of what's called the bias-variance trade-off.
8
00:00:26,075 --> 00:00:28,600
大家可能听说过这个概念
You might have heard this thing called the bias-variance trade-off.
9
00:00:28,675 --> 00:00:31,330
但深度学习的误差很少权衡二者
But in Deep Learning Error, there's less of a trade-off,
10
00:00:31,375 --> 00:00:33,775
我们总是分别考虑偏差和方差
so we'd still solve the bias, we still solve the variance,
11
00:00:33,860 --> 00:00:37,150
却很少谈及偏差方差的权衡问题
but we just talk less about the bias-variance trade-off.
12
00:00:37,290 --> 00:00:39,425
下面我们来一探究竟
Let's see what this means.
13
00:00:40,250 --> 00:00:42,575
假设这就是数据集
Let's see the data set that looks like this.
14
00:00:42,750 --> 00:00:47,875
如果给这个数据集拟合一条直线  可能得到一个逻辑回归拟合
If you fit a straight line to the data, maybe get a logistic regression fit to that.
15
00:00:47,950 --> 00:00:50,225
但它并不能很好地拟合该数据集
This is not a very good fit to the data.
16
00:00:50,415 --> 00:00:52,380
这是偏差高的情况
And so this is class of a high bias,
17
00:00:52,475 --> 00:00:56,175
我们称为“欠拟合”
what we say that this is underfitting the data.
18
00:00:56,525 --> 00:00:57,850
相反地
On the opposite end,
19
00:00:57,900 --> 00:01:00,640
如果我们拟合一个非常复杂的分类器
if you fit an incredibly complex classifier,
20
00:01:00,700 --> 00:01:05,900
比如深度神经网络或含有隐藏单元的神经网络
maybe deep neural network, or neural network with all the hidden units,
21
00:01:06,025 --> 00:01:10,150
可能就非常适用于这个数据集
maybe you can fit the data perfectly,
22
00:01:10,250 --> 00:01:12,125
但是这看起来也不是一种很好的拟合方式
but that doesn't look like a great fit either.
23
00:01:12,220 --> 00:01:17,535
分类器偏差较高  数据过度拟合
So there's a classifier of high variance and this is overfitting the data.
24
00:01:17,675 --> 00:01:19,650
在两者之间  可能还有一些像图中这样的
And there might be some classifier in between,
25
00:01:19,700 --> 00:01:24,400
复杂程度适中  数据拟合适度的分类器
with a medium level of complexity, that maybe fits it correctly like that.
26
00:01:24,900 --> 00:01:27,300
这个数据拟合看起来更加合理
That looks like a much more reasonable fit to the data,
27
00:01:27,375 --> 00:01:31,375
我们称之为“适度拟合”  是介于过拟合和欠拟合中间的一类
so we call that just right, it's somewhere in between.
28
00:01:31,650 --> 00:01:36,750
在这样一个只有x1和x2两个特征的二维数据集中
So in a 2D example like this, with just two features, X-1 and X-2,
29
00:01:36,775 --> 00:01:39,450
我们可以绘制数据  将偏差和方差可视化
you can plot the data and visualize bias and variance.
30
00:01:39,550 --> 00:01:41,100
在多维空间数据中
In high dimensional problems,
31
00:01:41,200 --> 00:01:44,575
绘制数据和可视化分割边界无法实现
you can't plot the data and visualize division boundary.
32
00:01:44,875 --> 00:01:46,830
但我们可以通过几个指标
Instead, there are couple of different metrics,
33
00:01:46,875 --> 00:01:49,600
来研究偏差和方差
that we'll look at, to try to understand bias and variance.
34
00:01:49,750 --> 00:01:53,400
我们沿用猫咪图片分类这个例子
So continuing our example of cat picture classification,
35
00:01:53,475 --> 00:01:57,400
这张是猫咪图片  这张不是
where that's a positive example and that's a negative example,
36
00:01:57,625 --> 00:02:01,450
理解偏差和方差的两个关键数据是
the two key numbers to look at to understand bias and variance will be
37
00:02:01,525 --> 00:02:06,250
训练集误差和验证集误差
the train set error and the dev set or the development set error.
38
00:02:06,415 --> 00:02:07,716
为方便论证
So for the sake of argument,
39
00:02:07,750 --> 00:02:10,290
假设我们可以辨别图片中的小猫
let's say that you're recognizing cats in pictures,
40
00:02:10,350 --> 00:02:13,375
我们用肉眼识别几乎是不会出错的
is something that people can do nearly perfectly, right?
41
00:02:13,860 --> 00:02:19,075
假定训练集错误率是1%
So let's say, your training set error is 1%
42
00:02:19,800 --> 00:02:23,580
为方便论证
and your dev set error is, for the sake of argument,
43
00:02:23,580 --> 00:02:25,425
假设验证集错误率是11%
let's say is 11%.
44
00:02:25,475 --> 00:02:29,450
可以看出  训练集设置得非常好
So in this example, you're doing very well on the training set,
45
00:02:29,600 --> 00:02:34,250
而验证集设置相对较差
but you're doing relatively poorly on the development set.
46
00:02:34,355 --> 00:02:38,215
我们可能过度拟合了训练集
So this looks like you might have overfit the training set,
47
00:02:38,325 --> 00:02:43,750
某种程度上  验证集并没有充分利用交叉验证集的作用
that somehow you're not generalizing well, to this whole cross-validation set in the development set.
48
00:02:43,850 --> 00:02:46,440
像这种情况
And so if you have an example like this,
49
00:02:46,575 --> 00:02:50,575
我们称之为“高偏差”
we would say this has high variance.
50
00:02:50,675 --> 00:02:54,075
通过查看训练集误差和验证集误差
So by looking at the training set error and the development set error,
51
00:02:54,240 --> 00:02:59,450
我们便可以诊断算法是否具有高偏差
you would be able to render a diagnosis of your algorithm having high variance.
52
00:02:59,525 --> 00:03:04,125
也就是说  衡量训练集和验证集误差
Now, let's say, that you measure your training set and your dev set error,
53
00:03:04,275 --> 00:03:06,090
得出不同结论
and you get a different result.
54
00:03:06,125 --> 00:03:09,675
假设训练集错误率是15%
Let's say, that your training set error is 15%.
55
00:03:09,750 --> 00:03:12,425
我把训练集错误率写在首行
I'm writing your training set error in the top row,
56
00:03:12,600 --> 00:03:15,600
验证集错误率是16%
and your dev set error is 16%.
57
00:03:15,800 --> 00:03:24,050
假设该案例中人的错误率几乎为0%
In this case, assuming that humans achieve roughly 0% error,
58
00:03:24,125 --> 00:03:27,450
人们浏览这些图片  分辨出是不是猫
that humans can look at these pictures and just tell if it's cat or not,
59
00:03:27,550 --> 00:03:31,500
算法并没有在训练集中得到很好训练
then it looks like the algorithm is not even doing very well on the training set.
60
00:03:31,600 --> 00:03:35,380
如果训练数据的拟合度不高
So if it's not even fitting the training data seam that well,
61
00:03:35,450 --> 00:03:38,220
就是数据欠拟合
then this is underfitting the data.
62
00:03:38,300 --> 00:03:40,890
就可以说这种算法偏差比较高
And so this algorithm has high bias.
63
00:03:41,075 --> 00:03:45,725
相反  它对于验证集产生的结果却是合理的
But in contrast, this actually generalizing at a reasonable level to the dev set,
64
00:03:45,750 --> 00:03:49,360
验证集中的错误率只比训练集的多了1%
whereas performance in the dev set is only 1% worse than performance in the training set.
65
00:03:49,425 --> 00:03:52,350
所以这种算法偏差高
So this algorithm has a problem of high bias,
66
00:03:52,475 --> 00:03:56,225
因为它甚至不能拟合训练集
because it was not even fitting the training set.
67
00:03:56,320 --> 00:04:00,800
这与上一张幻灯片中最左边的图片相似
Well, this is similar to the leftmost plots we had on the previous slide.
68
00:04:01,030 --> 00:04:03,125
再举一个例子
Now, here's another example.
69
00:04:03,250 --> 00:04:06,430
训练集错误率是15%
Let's say that you have 15% training set error,
70
00:04:06,450 --> 00:04:08,120
偏差相当高
so that's pretty high bias,
71
00:04:08,200 --> 00:04:11,440
但是  验证集的评估结果更糟糕
but when you evaluate to the dev set, it does even worse,
72
00:04:11,475 --> 00:04:13,450
错误率达到30%
maybe it does 30%.
73
00:04:13,650 --> 00:04:18,170
这种情况下  我会认为这种算法偏差高
In this case, I would diagnose this algorithm as having high bias,
74
00:04:18,250 --> 00:04:23,350
因为它在训练集上结果不理想  方差也很高
because it's not doing that well on the training set, and high variance.
75
00:04:24,150 --> 00:04:27,325
这是方差偏差都很糟糕的情况
So this has really the worst of both worlds.
76
00:04:27,550 --> 00:04:29,320
再看最后一个例子
And one last example,
77
00:04:29,370 --> 00:04:35,100
训练集错误率是0.5%  验证集错误率是1%
if you have 0.5 training set error, and 1% dev set error,
78
00:04:35,200 --> 00:04:37,130
用户看到这样的结果会很开心
then maybe our users are quite happy,
79
00:04:37,200 --> 00:04:43,875
猫咪分类器只有1%的错误率  偏差和方差都很低
that you have a cat classifier with only 1%, than just we have low bias and low variance.
80
00:04:44,925 --> 00:04:47,850
有一点我先在这儿简单提一下
One subtlety, that I'll just briefly mention that
81
00:04:47,925 --> 00:04:50,950
具体的留到后面课程里讲
we'll leave to a later video to discuss in detail,
82
00:04:51,050 --> 00:04:53,950
这些分析都是基于假设预测的
is that this analysis is predicated on the assumption,
83
00:04:54,075 --> 00:04:58,900
假设人眼辨别的错误率接近0%
that human level performance gets nearly 0% error or,
84
00:04:59,020 --> 00:05:04,175
一般来说  最优误差也被称为基本误差
more generally, that the optimal error, sometimes called base error,
85
00:05:04,400 --> 00:05:09,370
所以  最优误差接近0%
so the base in optimal error is nearly 0%.
86
00:05:10,075 --> 00:05:13,560
我就不在这里细讲了
I don't want to go into detail on this in this particular video,
87
00:05:13,625 --> 00:05:18,600
如果最优误差或基本误差非常高
but it turns out that if the optimal error or the base error were much higher, say,
88
00:05:18,775 --> 00:05:21,850
比如15%  我们再看看这个分类器
it were 15%, then if you look at this classifier,
89
00:05:21,950 --> 00:05:25,225
15%的错误率对训练集来说也是非常合理的
15% is actually perfectly reasonable for training set,
90
00:05:25,300 --> 00:05:29,075
偏差不高  方差也非常低
and you wouldn't see it as high bias and also a pretty low variance.
91
00:05:30,375 --> 00:05:33,850
当所有分类器都不适用时
So the case of how to analyze bias and variance,
92
00:05:34,000 --> 00:05:37,460
如何分析偏差和方差呢
when no classifier can do very well, for example,
93
00:05:37,500 --> 00:05:40,575
比如  图片很模糊
if you have really blurry images,
94
00:05:40,675 --> 00:05:45,900
即使是人眼  或者没有系统可以准确无误地识别图片
so that even a human or just no system could possibly do very well,
95
00:05:46,081 --> 00:05:49,237
这种情况下  基本误差会更高
then maybe base error is much higher,
96
00:05:49,250 --> 00:05:52,075
那么分析过程就要做些改变了
and then there are some details of how this analysis will change.
97
00:05:52,295 --> 00:05:54,725
我们暂时先不讨论这些细微差别
But leaving aside this subtlety for now,
98
00:05:54,800 --> 00:05:59,200
重点是通过查看训练集误差
the takeaway is that by looking at your training set error,
99
00:05:59,675 --> 00:06:04,200
我们可以判断数据拟合情况  至少对于训练数据是这样
you can get a sense of how well you are fitting, at least the training data,
100
00:06:04,331 --> 00:06:06,770
可以判断是否有偏差问题
and so that tells you if you have a bias problem.
101
00:06:06,900 --> 00:06:10,190
然后查看错误率有多高
And then looking at how much higher your error goes,
102
00:06:10,250 --> 00:06:12,960
当完成训练集训练  开始验证集验证时
when you go from the training set to the dev set,
103
00:06:13,100 --> 00:06:16,975
我们可以判断方差是否过高
that should give you a sense of how bad is the variance problem,
104
00:06:17,050 --> 00:06:20,575
从训练集到验证集的这个过程中
so you'll be doing a good job generalizing from a training set to the dev set,
105
00:06:20,650 --> 00:06:22,640
我们可以判断方差是否过高
that gives you sense of your variance.
106
00:06:22,825 --> 00:06:26,650
以上分析的前提都是假设基本误差很小
All this is under the assumption that the base error is quite small
107
00:06:26,725 --> 00:06:30,230
训练集和验证集数据来自相同分布
and that your training and your dev sets are drawn from the same distribution.
108
00:06:30,350 --> 00:06:32,210
如果没有这些假设作为前提
If those assumptions are violated,
109
00:06:32,275 --> 00:06:34,320
分析过程更更加复杂
there's a more sophisticated analysis you could do,
110
00:06:34,320 --> 00:06:36,275
我们将会在稍后课程里讨论
which we'll talk about in the later video.
111
00:06:36,425 --> 00:06:38,780
上一张幻灯片
Now, on the previous slide,
112
00:06:38,780 --> 00:06:42,175
我们讲了高偏差和高方差的情况
you saw what high bias, high variance looks like,
113
00:06:42,250 --> 00:06:44,920
大家应该对优质分类器有了一定的认识
and I guess you have the sense of what it a good class can look like.
114
00:06:44,970 --> 00:06:47,925
偏差和方差都高是什么样子呢
What does high bias and high variance looks like?
115
00:06:48,110 --> 00:06:50,225
这种情况对于两个衡量标准来说都是非常糟糕的
This is kind of the worst of both worlds.
116
00:06:50,375 --> 00:06:53,410
我们之前讲过  这样的分类器
So you remember, we said that a classifier like this,
117
00:06:53,425 --> 00:06:55,750
会产生高偏差
then your classifier has high bias,
118
00:06:55,750 --> 00:06:57,825
因为它的数据拟合低
because it underfits the data.
119
00:06:58,185 --> 00:07:04,030
像这种接近线性的分类器  数据拟合度低
So this would be a classifier that is mostly linear and therefore underfits the data,
120
00:07:04,075 --> 00:07:05,570
我用紫色线画出
we're drawing this is purple.
121
00:07:05,570 --> 00:07:09,546
但如果我们稍微改变一下分类器
But if somehow your classifier does some weird things,
122
00:07:09,650 --> 00:07:14,325
它会过度拟合部分数据
then it is actually overfitting parts of the data as well.
123
00:07:14,460 --> 00:07:19,550
用紫色线画出的分类器具有高偏差和高方差
So the classifier that I drew in purple, has both high bias and high variance.
124
00:07:19,800 --> 00:07:21,300
偏差高是因为
Where it has high bias,
125
00:07:21,350 --> 00:07:24,800
它几乎是一条线性分类器  并未拟合数据
because by being a mostly linear classifier, is just not fitting.
126
00:07:24,925 --> 00:07:28,460
这种二次曲线能够很好地拟合数据
You know, this quadratic line shape that well,
127
00:07:28,575 --> 00:07:31,200
这条曲线中间部分灵活性非常高
but by having too much flexibility in the middle,
128
00:07:34,300 --> 00:07:36,670
却过度拟合了这两个样本
it somehow gets this example, and this example overfits those two examples as well.
129
00:07:36,720 --> 00:07:40,515
这类分类器偏差很高  因为它几乎是线性的
So this classifier kind of has high bias, because it was mostly linear,
130
00:07:40,550 --> 00:07:43,620
而采用曲线函数或二次元函数
but you need maybe a curve function or quadratic function,
131
00:07:43,725 --> 00:07:45,150
会产生高偏差
and it has high variance,
132
00:07:45,225 --> 00:07:49,450
因为它曲线灵活性太高  以致拟合了这两个错误样本
because it had too much flexibility to fit those two mislabel,
133
00:07:49,590 --> 00:07:51,850
和中间这些活跃数据
or those live examples in the middle as well.
134
00:07:51,975 --> 00:07:53,975
这看起来有些不自然
In case this seems contrived, well,
135
00:07:54,075 --> 00:07:57,580
从两个维度上看都不太自然
this example is a little bit contrived in two dimensions,
136
00:07:57,585 --> 00:07:59,883
但对于高维数据
but with very high dimensional inputs,
137
00:07:59,975 --> 00:08:04,575
有些数据区域偏差高  有些数据区域方差高
you actually do get things with high bias in some regions and high variance in some regions,
138
00:08:04,625 --> 00:08:11,200
所以在高维数据中采用这种分类器看起来就不会这么牵强了
and so it is possible to get classifiers like this in high dimensional inputs that seem less contrived.
139
00:08:11,410 --> 00:08:12,575
总结一下
So to summarize,
140
00:08:12,625 --> 00:08:16,750
今天我们讲了如何通过分析训练集训练算法产生的误差
you've seen how by looking at your algorithm's error on the training set
141
00:08:16,850 --> 00:08:20,550
和验证集验证算法产生的误差
and your algorithm's error on the dev set, you can try to diagnose,
142
00:08:20,625 --> 00:08:23,400
来诊断算法是否存在高偏差或高方差
whether it has problems of high bias or high variance,
143
00:08:23,410 --> 00:08:25,420
是否两个值都高  或者两个值都不高
or maybe both, or maybe neither.
144
00:08:25,500 --> 00:08:28,990
根据算法偏差和方差的具体情况
And depending on whether your algorithm suffers from bias or variance,
145
00:08:29,000 --> 00:08:31,475
决定接下来你要做的工作
it turns out that there are different things you could try.
146
00:08:31,625 --> 00:08:34,200
下节课  我会根据
So in the next video, I want to present to you,
147
00:08:34,300 --> 00:08:37,390
算法偏差和方差的高低情况
what I call a basic recipe for Machine Learning,
148
00:08:37,450 --> 00:08:40,900
讲解一些机器学习的基本方法
that lets you more systematically try to improve your algorithm,
149
00:08:40,900 --> 00:08:44,150
帮助大家更系统地优化算法
depending on whether it has high bias or high variance issues.
150
00:08:44,275 --> 00:08:45,950
我们下节课见
So let's go on to the next video.
