WEBVTT

1
00:00:00.770 --> 00:00:04.780
One of the challenges of machine
translation is that given a French

2
00:00:04.780 --> 00:00:09.145
sentence, there could be multiple English
translations that are equally good

3
00:00:09.145 --> 00:00:11.290
translations to that French sentence.

4
00:00:11.290 --> 00:00:15.470
So how do you evaluate a machine
translation system if there are multiple

5
00:00:15.470 --> 00:00:19.210
equally good answers,
unlike say image recognition,

6
00:00:19.210 --> 00:00:21.020
where there's one right answer.

7
00:00:21.020 --> 00:00:22.430
You just measure accuracy.

8
00:00:22.430 --> 00:00:26.390
If there are multiple great answers,
how do you measure accuracy?

9
00:00:26.390 --> 00:00:30.230
The way this is done conventionally is
with something called the Bleu score.

10
00:00:30.230 --> 00:00:32.830
So in this optional video,
I want to share with you,

11
00:00:32.830 --> 00:00:36.098
I want to give you a sense
of how the Bleu score works.

12
00:00:36.098 --> 00:00:39.258
Let's say you are given
this French sentence,

13
00:00:39.258 --> 00:00:42.971
le chat est sur la tapis,and
you are given a reference,

14
00:00:42.971 --> 00:00:47.790
human generated translation of this,
which is the cat is on the mat, but

15
00:00:47.790 --> 00:00:52.846
there are multiple pretty good
translations of this, so different human,

16
00:00:52.846 --> 00:00:57.290
different person might translate
it as there is a cat on the mat.

17
00:00:57.290 --> 00:00:59.240
And both of these are actually just fine,

18
00:00:59.240 --> 00:01:01.720
perfectly fine translations
of the French sentence.

19
00:01:03.180 --> 00:01:08.627
What the Bleu score does is,
given a machine generated translation,

20
00:01:08.627 --> 00:01:13.613
it allows you to automatic compute
a score then measures how good

21
00:01:13.613 --> 00:01:16.031
is that machine translation.

22
00:01:20.380 --> 00:01:25.568
And the intuition is, so
long as the machine generated translation

23
00:01:25.568 --> 00:01:30.210
is pretty close to any of
the references provided by humans,

24
00:01:30.210 --> 00:01:32.952
then it would get a high Bleu score.

25
00:01:37.151 --> 00:01:42.517
Bleu, by the way, stands for
bilingual evaluation,

26
00:01:45.308 --> 00:01:47.470
Understudy.

27
00:01:47.470 --> 00:01:48.835
So in the theater world,

28
00:01:48.835 --> 00:01:54.000
an understudy is someone that learns
the role of a more senior actor so

29
00:01:54.000 --> 00:01:58.388
they can take over the role of
the more senior actor if necessary.

30
00:01:58.388 --> 00:02:05.670
And motivation for Bleu is that,

31
00:02:05.670 --> 00:02:10.830
whereas you could ask human evaluators to
evaluate the machine translation system,

32
00:02:10.830 --> 00:02:15.690
the Bleu score is an understudy,
it could be a substitute for

33
00:02:15.690 --> 00:02:19.820
having humans evaluate every output
of a machine translation system.

34
00:02:23.079 --> 00:02:26.942
So the Bleu score was
due to Kishore Papineni,

35
00:02:26.942 --> 00:02:30.915
Salim Roukos, Todd Ward, and Wei-Jing Zhu.

36
00:02:30.915 --> 00:02:35.300
This paper has been incredibly
influential and is actually

37
00:02:35.300 --> 00:02:41.035
quite a readable paper so I encourage
you to take a look if you have time.

38
00:02:42.195 --> 00:02:46.630
So the intuition behind the Bleu score is,

39
00:02:46.630 --> 00:02:51.150
we're going to look at the machine
generated output and see if the types of

40
00:02:51.150 --> 00:02:58.090
words it generates appear in at least
one of the human generated references.

41
00:02:58.090 --> 00:03:02.080
And so these human generated
references would be provided

42
00:03:02.080 --> 00:03:06.200
as part of the depvset or
as part of the test set.

43
00:03:06.200 --> 00:03:08.260
Now, let's look at some
of the extreme examples.

44
00:03:08.260 --> 00:03:13.350
Let's say that the machine
translation system abbreviating

45
00:03:13.350 --> 00:03:18.310
machine translation is MT, so the machine
translation of the MT outputs is the, the,

46
00:03:18.310 --> 00:03:22.230
the, the, the, the, the, so this is
clearly a pretty terrible translation.

47
00:03:23.440 --> 00:03:29.510
So one way to measure how good
the machine translation output is,

48
00:03:29.510 --> 00:03:32.670
is to look at each of
the words in the output and

49
00:03:32.670 --> 00:03:37.010
see if it appears in the references.

50
00:03:37.010 --> 00:03:42.600
And so this we call the precision
of the machine translation output.

51
00:03:42.600 --> 00:03:48.860
And in this case there are seven words
in the machine translation output.

52
00:03:48.860 --> 00:03:54.997
And every one of these seven words appears
in either Reference 1 or Reference 2.

53
00:03:54.997 --> 00:04:00.276
Right, so the word the appears in both
references so each of these words looks

54
00:04:00.276 --> 00:04:05.653
like a pretty good word to include, so
this will have a precision of 7 over 7.

55
00:04:05.653 --> 00:04:07.590
It looks like it's a great precision.

56
00:04:07.590 --> 00:04:12.325
So this is why the basic
precision measure of what fraction

57
00:04:12.325 --> 00:04:15.755
of the words in the MT output
also appear in the references.

58
00:04:15.755 --> 00:04:19.425
This is not a particularly useful measure
because it seems to imply that this

59
00:04:19.425 --> 00:04:22.375
MT output has very high precision.

60
00:04:22.375 --> 00:04:27.505
So instead, what we're going to use
is a modified precision measure,

61
00:04:27.505 --> 00:04:32.320
in which we would give each
word credit only up to

62
00:04:32.320 --> 00:04:37.270
the maximum number of times it
appears in the reference sentences.

63
00:04:37.270 --> 00:04:41.940
So in Reference one, the word
the appears twice, in reference two,

64
00:04:41.940 --> 00:04:44.170
the word the appears just once.

65
00:04:44.170 --> 00:04:51.410
So two is bigger than one, and so we're
going to say that the word the gets credit

66
00:04:51.410 --> 00:04:56.520
up to twice, so with a modified precision,

67
00:04:56.520 --> 00:05:00.940
we will say that it gets
a score of two out of seven,

68
00:05:00.940 --> 00:05:07.920
because out of seven words will give
it a two credits for appearing.

69
00:05:10.360 --> 00:05:15.870
So here the denominator is the count
of the number of times the word

70
00:05:15.870 --> 00:05:21.070
the appears,
there are seven words in total.

71
00:05:21.070 --> 00:05:26.246
And the numerator is the count of
the number of times the word the appears,

72
00:05:26.246 --> 00:05:30.770
but we clip this count, we take a max,
we clip this count at two.

73
00:05:32.750 --> 00:05:35.370
So this gives us the modified
precision measure.

74
00:05:36.980 --> 00:05:40.148
Now, so far we've been looking
at words in isolation.

75
00:05:40.148 --> 00:05:43.410
In the Bleu score, you don't want to
just look at isolated words,

76
00:05:43.410 --> 00:05:46.065
you maybe want to look at
pairs of words as well.

77
00:05:46.065 --> 00:05:52.280
And so

78
00:05:52.280 --> 00:05:56.710
let's define a portion of
the Bleu score on bigrams, and

79
00:05:56.710 --> 00:06:01.780
bigrams just means pairs of words
appearing next to each other.

80
00:06:01.780 --> 00:06:03.760
Continuing our example from before,

81
00:06:03.760 --> 00:06:08.130
let's now use a slightly more
sophisticated machine translation output.

82
00:06:08.130 --> 00:06:12.970
Let's say this time the MT
machine translation, so

83
00:06:12.970 --> 00:06:18.270
now let's see how we could use
bigrams to define the Bleu score,

84
00:06:18.270 --> 00:06:22.250
and this would just be a portion
of the final Bleu score.

85
00:06:22.250 --> 00:06:25.170
And we'll take unigrams or
single words as well as bigrams,

86
00:06:25.170 --> 00:06:29.780
which means pairs of words,
into account as well as maybe even longer

87
00:06:29.780 --> 00:06:34.380
sequences worth such as trigrams which
means three words paired together.

88
00:06:35.470 --> 00:06:39.930
So, Let's continue our
example from before.

89
00:06:39.930 --> 00:06:43.780
We have the same Reference 1 and
Reference 2, but now let's say the machine

90
00:06:43.780 --> 00:06:48.110
translation or the MT system
has a slightly better output.

91
00:06:48.110 --> 00:06:49.240
The cat the cat on the mat.

92
00:06:49.240 --> 00:06:52.220
Still not a great translation, but
maybe better than the last one.

93
00:06:53.890 --> 00:06:57.848
So here the possible bigrams are,
well, there's the cat,

94
00:06:57.848 --> 00:07:03.460
we'll ignore case, and then there's
cat the, that's another bigram.

95
00:07:03.460 --> 00:07:08.850
And then there's the cat again, but
we've already had that so let's skip that.

96
00:07:08.850 --> 00:07:11.790
And then cat on is the next one.

97
00:07:11.790 --> 00:07:13.490
And then on the and the mat.

98
00:07:13.490 --> 00:07:16.230
So these are the bigrams in
the machine translation output.

99
00:07:21.149 --> 00:07:28.250
And so let's count up how many
times each of these bigrams appear.

100
00:07:28.250 --> 00:07:34.030
The cat appears twice, cat the appears
once, and the others all appear just once.

101
00:07:35.450 --> 00:07:39.140
And then finally,
let's define the clipped count.

102
00:07:39.140 --> 00:07:43.390
So, count and then subscript clip.

103
00:07:43.390 --> 00:07:47.000
And to define that,
let's take this column of numbers but

104
00:07:47.000 --> 00:07:51.550
give our algorithm credit only up to the
maximum number of times that that bigram

105
00:07:51.550 --> 00:07:55.540
appears in either Reference one or
Reference two.

106
00:07:57.270 --> 00:08:03.230
So the cat appears a maximum of
once in either of the references,

107
00:08:03.230 --> 00:08:05.990
so we'll clip that count to one.

108
00:08:05.990 --> 00:08:12.100
Cat the, well, doesn't appear in Reference
1 or reference 2 so clip that to zero.

109
00:08:12.100 --> 00:08:15.810
Cat on, yeah, that appears once,
we give it credit for once.

110
00:08:15.810 --> 00:08:20.530
On the appears once, we give it credit for
once and the mat appears once.

111
00:08:20.530 --> 00:08:25.027
So these are the clipped counts,
we're taking all the counts and

112
00:08:25.027 --> 00:08:29.932
clipping them, reducing them to be no
more than the number of times that

113
00:08:29.932 --> 00:08:33.390
bigram appears in at least
one of the references.

114
00:08:36.733 --> 00:08:38.308
And then finally,

115
00:08:38.308 --> 00:08:43.730
our modified bigram precision will
be the sum of the count clips.

116
00:08:43.730 --> 00:08:49.630
So that's one, two, three, four,
divided by the total number of bigrams.

117
00:08:49.630 --> 00:08:53.230
That's two, three, four, five, six.

118
00:08:53.230 --> 00:08:59.510
So four out of six or two thirds is
the modified precision on bigrams.

119
00:09:01.670 --> 00:09:05.130
So let's just formalize
this a little bit further.

120
00:09:05.130 --> 00:09:12.100
With what we had developed with
on unigrams we defined this

121
00:09:12.100 --> 00:09:18.419
modified precision computed on
unigrams as p substrate 1, so

122
00:09:18.419 --> 00:09:23.390
p stands for precision and the substrate
1 here means we're referring to unigrams.

123
00:09:23.390 --> 00:09:28.000
But that is defined as
sum over the unigrams so

124
00:09:28.000 --> 00:09:33.980
that just means sum over the words that
appear in the machine translation output.

125
00:09:33.980 --> 00:09:38.084
So this is called y hat of

126
00:09:38.084 --> 00:09:43.020
count clip of that unigram.

127
00:09:45.776 --> 00:09:50.264
Divided by sum of raw unigrams in

128
00:09:50.264 --> 00:09:56.085
the machine translation output of number

129
00:09:56.085 --> 00:10:01.082
of counts of that unigram, right.

130
00:10:01.082 --> 00:10:05.582
And so
this is what we had gotten as I guess,

131
00:10:05.582 --> 00:10:09.090
as 2 out of 7, two slides back.

132
00:10:10.130 --> 00:10:13.280
So, the one here refers to unigram,

133
00:10:13.280 --> 00:10:16.440
meaning we're looking at
single words in isolation.

134
00:10:16.440 --> 00:10:22.890
You can also define p n
as the n-gram version.

135
00:10:25.986 --> 00:10:28.870
Instead of a unigram for n-gram.

136
00:10:28.870 --> 00:10:33.756
So this would be sum over the n-gram in

137
00:10:33.756 --> 00:10:38.950
the machine translation output of count

138
00:10:38.950 --> 00:10:43.989
clip of that n-gram divided by sum over

139
00:10:43.989 --> 00:10:49.045
n-grams of the count of that n-gram.

140
00:10:49.045 --> 00:10:53.804
And so these precisions, or these modified

141
00:10:53.804 --> 00:10:58.820
precision scores, measured on unigrams, or

142
00:10:58.820 --> 00:11:03.834
on bigrams,
which we did on a previous slide,

143
00:11:03.834 --> 00:11:08.720
or on trigrams,
which are triples of words,

144
00:11:08.720 --> 00:11:13.758
or even higher values of n for
other n-grams.

145
00:11:13.758 --> 00:11:18.748
This allows you to measure
the degree to which

146
00:11:18.748 --> 00:11:23.740
the machine translation
output is similar or

147
00:11:23.740 --> 00:11:27.959
maybe overlaps with the references.

148
00:11:31.916 --> 00:11:36.833
And one thing that you could
probably convince yourself of is,

149
00:11:36.833 --> 00:11:42.861
if the MT of output is exactly the same
as either Reference 1 or Reference 2,

150
00:11:42.861 --> 00:11:49.389
then all of these values, P1, and P2 and
so on, they'll all be equal to 1.0.

151
00:11:49.389 --> 00:11:54.211
So to get a precision or
a modified precision of 1.0,

152
00:11:54.211 --> 00:11:59.300
you just have to be exactly
equal to one of the references.

153
00:11:59.300 --> 00:12:04.700
And sometimes it's possible to
achieve this even if you aren't

154
00:12:04.700 --> 00:12:10.200
exactly the same as any other
references but kind of combined them

155
00:12:10.200 --> 00:12:15.311
in a way that hopefully still
results in a good translation.

156
00:12:23.460 --> 00:12:29.908
Finally, let's put this together
to form the final Bleu score.

157
00:12:29.908 --> 00:12:34.474
P subscript n is the Bleu score
computed on n-grams only,

158
00:12:34.474 --> 00:12:38.955
also the modified precision,
computed on n-grams only

159
00:12:43.186 --> 00:12:49.564
By convention, And

160
00:12:49.564 --> 00:12:55.464
by convention, to compute one number,
you compute p1, p2,

161
00:12:55.464 --> 00:13:01.709
p3 and p4 and combine them together
using the following formula.

162
00:13:01.709 --> 00:13:06.621
It's going to be the average, so
sum from n equals 1 to 4 of pn and

163
00:13:06.621 --> 00:13:10.810
divide that by 4, so
basically taking the average.

164
00:13:11.860 --> 00:13:15.680
By convention the Bleu school
is defined as E to the this.

165
00:13:15.680 --> 00:13:22.610
And exponentiation is a strictly
monotonically increasing

166
00:13:22.610 --> 00:13:27.535
operation and then we actually adjust
this with one more factor called

167
00:13:27.535 --> 00:13:35.987
the BP penalty.

168
00:13:35.987 --> 00:13:45.824
So BP Stands for

169
00:13:45.824 --> 00:13:48.007
brevity penalty.

170
00:13:49.950 --> 00:13:54.735
The details maybe aren't super important,
but just give you a sense,

171
00:13:54.735 --> 00:13:58.563
it turns out that if you output
very short translations,

172
00:13:58.563 --> 00:14:03.429
it's easier to get high precision
because probably most of the words you

173
00:14:03.429 --> 00:14:05.846
output appear in your references.

174
00:14:05.846 --> 00:14:09.705
So, but we don't want
translations that are very short.

175
00:14:09.705 --> 00:14:14.134
So the BP, or the brevity penalty,
is an adjustment factor that penalizes

176
00:14:14.134 --> 00:14:18.710
translation systems that output
translations that are too short.

177
00:14:18.710 --> 00:14:21.435
So the formula for
the brevity penalty is the following.

178
00:14:21.435 --> 00:14:26.430
It's equal to one if your machine
translation system actually outputs

179
00:14:26.430 --> 00:14:32.120
things that are longer
than the human generated

180
00:14:32.120 --> 00:14:37.378
reference outputs and otherwise,
it's some formula like that that overall

181
00:14:37.378 --> 00:14:42.173
penalizes shorter translations.

182
00:14:45.667 --> 00:14:48.340
So in the details,
you can find in this paper.

183
00:14:51.100 --> 00:14:53.844
So once again,
earlier in this set of clauses,

184
00:14:53.844 --> 00:14:58.376
you saw the importance of having a single
row number evaluation metric because

185
00:14:58.376 --> 00:15:02.837
it allows you to try out two ideas, see
which one achieves the higher score and

186
00:15:02.837 --> 00:15:07.120
then try to stick with the one that
have achieves the highest score.

187
00:15:07.120 --> 00:15:09.996
So the reason the Blue
score was revolutionary for

188
00:15:09.996 --> 00:15:13.353
machine translation was because
this gave a pretty good,

189
00:15:13.353 --> 00:15:17.875
by no means perfect, but pretty good
single real number evaluation matrix and

190
00:15:17.875 --> 00:15:23.030
so that accelerated the progress of
the entire field of machine translation.

191
00:15:23.030 --> 00:15:26.640
I hope this video gave you a sense
of how the Bleu score works.

192
00:15:26.640 --> 00:15:30.560
In practice, few people would
implement the Bleu score from scratch.

193
00:15:30.560 --> 00:15:33.600
There are open source
implementations you can download and

194
00:15:33.600 --> 00:15:36.100
just use to evaluate your own system.

195
00:15:36.100 --> 00:15:41.380
But today, BLEU score is used to evaluate
many systems that generate text,

196
00:15:41.380 --> 00:15:45.500
such as machine translation systems as
well as the example I showed briefly

197
00:15:45.500 --> 00:15:50.520
earlier of image captioning systems,
where you would have a system,

198
00:15:50.520 --> 00:15:55.660
have a neural network generated image
caption and then use the Bleu score to see

199
00:15:55.660 --> 00:15:58.830
how much that overlaps with
maybe a reference caption or

200
00:15:58.830 --> 00:16:02.530
multiple reference captions
that were generated by people.

201
00:16:02.530 --> 00:16:05.430
So the Bleu score is a useful, single,

202
00:16:05.430 --> 00:16:09.530
real number evaluation measure to use
whenever you want your algorithm to

203
00:16:09.530 --> 00:16:14.860
generate a piece of text and you want
to see whether it has similar meaning

204
00:16:14.860 --> 00:16:18.770
as a reference piece of
text generated by humans.

205
00:16:18.770 --> 00:16:19.850
This is not used for

206
00:16:19.850 --> 00:16:24.130
speech recognition because in speech
recognition there's usually, you have one

207
00:16:24.130 --> 00:16:28.970
ground you just use other measures to see
if you got the speech transcription on

208
00:16:28.970 --> 00:16:33.520
pretty much exactly word for word correct
but for things like image captioning and

209
00:16:33.520 --> 00:16:36.510
multiple captions for a picture
it could be about equally good or

210
00:16:36.510 --> 00:16:40.680
for machine translation there are multiple
translations about equally good.

211
00:16:40.680 --> 00:16:45.200
The Bleu score gives you a way to
evaluate that automatically and

212
00:16:45.200 --> 00:16:47.960
therefore speed up your
algorithm development.

213
00:16:47.960 --> 00:16:51.890
So with that, I hope you have
a sense of how the Bleu score works.