{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursera | Andrew Ng (01-week2)—神经网络基础\n",
    "\n",
    "在吴恩达深度学习视频以及大树先生的博客提炼笔记基础上添加个人理解，原大树先生博客可查看该链接地址[大树先生的博客](http://blog.csdn.net/koala_tree)- ZJ\n",
    "    \n",
    ">[Coursera 课程](https://www.coursera.org/specializations/deep-learning) |[deeplearning.ai](https://www.deeplearning.ai/) |[网易云课堂](https://mooc.study.163.com/smartSpec/detail/1001319001.htm)\n",
    "\n",
    "   \n",
    "   [CSDN](http://blog.csdn.net/junjun_zhao/article/details/79226016)：http://blog.csdn.net/junjun_zhao/article/details/79226016\n",
    "   \n",
    "\n",
    "---\n",
    "\n",
    "**第二周 神经网络基础 (Basics of Neural Network Programming )**\n",
    "\n",
    "\n",
    " \n",
    "## 2.1 二分分类 ( Binary Classification)\n",
    "\n",
    "\n",
    "**神经网络编程的基础知识**\n",
    "\n",
    "例如 m 个样本的训练集，你可能会习惯性地去用一个 for 循环，来遍历这 m 个样本。实现一个神经网络，如果你要遍历整个训练集，并不需要直接使用 for 循环。\n",
    "\n",
    "本周介绍为什么神经网络的计算过程可以分为 **前向传播和反向传播** 两个分开的过程。 用**logistic 回归** 来阐述，以便于更好地理解。\n",
    "\n",
    "**logistic 回归是一个用于二分分类的算法。**\n",
    "\n",
    "二分分类问题的例子：\n",
    "\n",
    "假如你有一张图片作为输入，这样子的，你想输出识别此图的标签，**如果是猫 输出 1，如果不是 则输出 0，我们用 y 来表示输出的结果标签。**\n",
    "\n",
    "\n",
    "![Binary  Classification](http://img.blog.csdn.net/20171215160051266?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "一张图片在计算机中的表示：\n",
    "\n",
    "**三个独立矩阵**，分别对应图片中的 **红、绿、蓝**三个颜色通道，如果输入图片是 64×64 像素的，就有三个 64×64 的矩阵，分别对应图片中 红、绿、蓝 三种像素的亮度。放入一个特征向量 $x$。 **向量 x 的总维度 就是 64×64×3=12288**\n",
    "\n",
    "### Notation 符号\n",
    "\n",
    "![notation](http://img.blog.csdn.net/20171219145657600?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    " - 样本：$(x,y)$，训练样本包含 m 个；\n",
    " - 其中$x\\in R^{n_{x}}$，表示样本 x  包含$n_{x}$个特征；\n",
    " - $y∈0,1$，目标值属于 0、1分类；\n",
    " - 训练数据： $\\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\\cdots,(x^{(m)},y^{(m)})\\}$\n",
    "\n",
    "训练数据样本形状： $X.shape=(n_{x},m)$\n",
    "\n",
    "<center>![X](http://img.blog.csdn.net/20171219152401429?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)</center>\n",
    "\n",
    "对应标签数据的形状：$Y=[y^{(1)},y^{(2)},\\cdots,y^{(m)}]$   ,  $Y.shape=(1,m)$\n",
    "\n",
    "\n",
    "\n",
    "## 2.2 Logistic 回归 (Logistic Regression)\n",
    "\n",
    "\n",
    "**logistic 回归**，用在**监督学习问题**中的学习算法 ，输出 y 标签是 0 或 1 ，这是一个**二元分类问题**\n",
    "\n",
    "\n",
    "<center> ![这里写图片描述](http://img.blog.csdn.net/20171220161521549?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)</center>\n",
    "\n",
    "\n",
    "逻辑回归中，预测值： \n",
    "\n",
    "<center>$\\hat h = P(y=1|x)$</center>\n",
    "\n",
    "其表示为 1 的概率，取值范围在 [0,1] 之间。\n",
    "\n",
    "<center>![这里写图片描述](http://img.blog.csdn.net/20171220162150821?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)</center>\n",
    "\n",
    "\n",
    "引入 Sigmoid 函数，预测值： \n",
    "\n",
    " <center> $\\hat y = Sigmoid(w^{T}x+b)=\\sigma(w^{T}x+b)$</center>\n",
    "\n",
    "其中 \n",
    "\n",
    "<center>$Sigmoid(z)=\\dfrac{1}{1+e^{-z}}$</center>\n",
    "\n",
    "**注意点**：函数的一阶导数可以用其自身表示， \n",
    "\n",
    "该部分的求导可查看：[Logistic回归-代价函数求导过程 | 内含数学相关基础](http://blog.csdn.net/JUNJUN_ZHAO/article/details/78564557)\n",
    "\n",
    "<center>$\\sigma'(z)=\\sigma(z)(1-\\sigma(z))$</center>\n",
    "\n",
    "这里可以解释梯度消失的问题，当 $z=0$ 时，导数最大，但是导数最大为$\\sigma'(0)=\\sigma(0)(1-\\sigma(0))=0.5(1-0.5)=0.25$，这里导数仅为原函数值的 0.25 倍。\n",
    "\n",
    "<center>![sigmoid](http://img.blog.csdn.net/20171220162759856?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)</center>\n",
    "\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20171220165331857?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "## 2.3  Logistic 回归成本函数 (Logistic Regression Cost function)\n",
    "\n",
    "\n",
    "logistic 回归的模型，为了训练 logistic 回归模型的参数 w 以及 b，需要定义一个成本函数，用 logistic 回归来训练的**成本函数**。\n",
    "\n",
    "让模型来通过学习**调整参数**:\n",
    "\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20171221151806625?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "- m 个样本的训练集， 通过在训练集 找到参数 w 和 b ，得到输出，对训练集中的预测值 $\\hat y^{(i)}$ \n",
    "\n",
    "误差平方：\n",
    "\n",
    "一般经验来说，使用平方错误（squared error）来衡量 Loss Function： \n",
    "<center>$L(\\hat y, y)=\\dfrac{1}{2}(\\hat y-y)^{2}$</center>\n",
    "\n",
    "注意： 但是，对于 logistic regression 来说，一般不适用平方错误来作为 Loss Function，这是因为上面的平方错误损失函数一般是**非凸函数（non-convex）**，其在使用**梯度下降算法**的时候，容易得到**多个局部最优解**，而不是**全局最优解**。因此**要选择凸函数**。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20171221151916206?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "**logistic 回归的损失函数** （Loss Function）：\n",
    "\n",
    "$ Loss = -(y*log(\\hat y) + (1-y)log(1-\\hat y))$\n",
    "\n",
    "\n",
    " ![这里写图片描述](http://img.blog.csdn.net/20171221152135265?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "- 当 $y=1$ 时，$L(\\hat y, y)=-\\log \\hat y$。如果 $\\hat y$ 越接近 1，$L(\\hat y, y) \\approx 0$，表示预测效果越好；如果$\\hat y$越接近 0，$L(\\hat y, y) \\approx +\\infty$，表示预测效果越差；\n",
    "\n",
    "- 当 $y=0$ 时，$L(\\hat y, y)=-\\log (1-\\hat y)$。如果$\\hat y$越接近0，$L(\\hat y, y) \\approx 0$，表示预测效果越好；如果$\\hat y$越接近1，$L(\\hat y, y) \\approx +\\infty$，表示预测效果越差；\n",
    "\n",
    "- 我们的目标是最小化样本点的损失 Loss Function，损失函数是针对单个样本点的。\n",
    "\n",
    "补充 log 函数图:\n",
    "\n",
    " ![这里写图片描述](http://img.blog.csdn.net/20171221155211807?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "**损失函数**是，在**单个训练样本**中定义的，它衡量了在**单个训练样本上的表现**。\n",
    "\n",
    "**成本函数**， 它衡量的是在**全体训练样本上的表现**。\n",
    "\n",
    "\n",
    "全部训练数据集的 Loss function 总和的平均值即为训练集的代价函数（Cost function）。\n",
    "$$\n",
    "J(w,b)=\\dfrac{1}{m}\\sum_{i=1}^{m}L(\\hat y^{(i)}, y^{(i)})=-\\dfrac{1}{m}\\sum_{i=1}^{m}\\left[y^{(i)}\\log\\hat y^{(i)}+(1-y^{(i)})\\log(1-\\hat y^{(i)})\\right]\n",
    "$$\n",
    "\n",
    "- Cost function 是**待求系数 w 和 b**的函数；\n",
    "- 我们的**目标**就是**迭代计算出最佳的 w 和 b 的值**，**最小化 Cost function**，让其尽可能地接近于0。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 2.4 梯度下降法 （Gradient Descent ）\n",
    "\n",
    "回顾：\n",
    "<font color=#0099ff>\n",
    "**1.  Loss function (损失函数)：是衡量单一训练样本的效果。**\n",
    "**2.  Cost function (成本函数)：成本函数是在全部训练集上，来衡量参数 w 和 b 的效果。**\n",
    "</font>\n",
    "\n",
    "\n",
    "**如何使用梯度下降法来训练或学习训练集上的参数 w 和 b **\n",
    "\n",
    "\n",
    "1.Logistic 回归：\n",
    "\n",
    "<center>$\\hat y =\\sigma(w^{T}x+b)$, $\\sigma(z)=\\dfrac{1}{1+e^{-z}}$ ,$z=w^{T}x+b$</center>\n",
    "\n",
    "2.Cost function (成本函数)：\n",
    "\n",
    "\n",
    "<center>$J(w,b)=\\dfrac{1}{m}\\sum_{i=1}^{m}L(\\hat y^{(i)}, y^{(i)})=-\\dfrac{1}{m}\\sum_{i=1}^{m}\\left[y^{(i)}\\log\\hat y^{(i)}+(1-y^{(i)})\\log(1-\\hat y^{(i)})\\right]$</center>\n",
    "\n",
    "\n",
    "目的：找到（学习训练得到）w and b ，使得成本函数$J(w,b)$最小。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20171222143743794?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "<font color=#0099ff>凸函数：全局最优解。(这是我们想要的)\n",
    "非凸函数：局部最优解。</font>\n",
    "\n",
    " <center> $w:=w-\\alpha\\dfrac{\\partial J(w,b)}{\\partial w}$</center>  \n",
    " \n",
    " 学习率 learning_rate ：$α$\n",
    "  \n",
    " ![这里写图片描述](http://img.blog.csdn.net/20171222150257570?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "我们用 dw，作为导数的变量名，现在我们确保梯度下降法更新是有用的。w 在这对应的成本函数 J(w) 在曲线上的这一点。**记住导数的定义，是函数在这个点的斜率，而函数的斜率是高除以宽。** \n",
    "\n",
    "\n",
    "<center>![这里写图片描述](http://img.blog.csdn.net/20171222152139112?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)</center>\n",
    "\n",
    "\n",
    "**无论你初始化的位置是在左边还是右边，梯度下降法会朝着全局最小值方向移动**\n",
    "\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20171222153008728?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "**梯度下降法**\n",
    "\n",
    "用梯度下降法（Gradient Descent）算法来最小化 Cost function，以计算出合适的 w 和 b 的值。\n",
    "\n",
    "<font color=#0099ff>**迭代更新的修正表达式：**</font>\n",
    "\n",
    "<center>$w:=w-\\alpha\\dfrac{\\partial J(w,b)}{\\partial w}$</center>\n",
    "\n",
    "<center>$b:=b-\\alpha\\dfrac{\\partial J(w,b)}{\\partial b}$</center>\n",
    "\n",
    "\n",
    "在程序代码中，我们通常使用 $dw$ 来表示$\\dfrac{\\partial J(w,b)}{\\partial w}$，用 $db$ 来表示$\\dfrac{\\partial J(w,b)}{\\partial b}$。\n",
    "\n",
    "    \n",
    "**偏导数符号** 使用 $\\partial $ 还是小写字母  $d$：**取决于你的函数 J 是否含有两个以上的变量** :\n",
    "\n",
    "**1. 变量超过两个就用偏导数符号 $\\partial $** ，\n",
    "**2. 如果函数只有一个变量就用小写字母 d。**\n",
    "\n",
    "\n",
    "## 2.5 导数（Derivatives）\n",
    "\n",
    "导数，函数的斜率，**斜率定义**， **高除以宽**。\n",
    "\n",
    "<center>![这里写图片描述](http://img.blog.csdn.net/20171225161346861?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)</center>\n",
    "\n",
    "\n",
    "<font color=#0099ff>【wiki | 导数】:https://zh.wikipedia.org/wiki/%E5%AF%BC%E6%95%B0</font>\n",
    "\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20171225161155030?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "\n",
    "## 2.6 更多导数的例子 (More derivatives examples)\n",
    "\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20171226111545129?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "重点：\n",
    "\n",
    "1. 函数的导数，就是函数的斜率，而函数的斜率在不同的点是不同的。\n",
    "2. 如果你想知道 一个函数的导数，你可参考微积分课本或者维基百科，然后你应该就能，找到这些函数的导数公式。\n",
    "\n",
    "\n",
    "## 2.7 计算图  （Computation Graph）\n",
    "\n",
    "\n",
    "一个神经网络的计算，都是按照前向或反向传播过程来实现的。\n",
    "\n",
    "\n",
    "<center>![这里写图片描述](http://img.blog.csdn.net/20171227111613911?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)</center>\n",
    "\n",
    "\n",
    "## 2.8 计算图的导数计算 （Derivatives with a Computation Graph ）\n",
    "\n",
    "<center>![这里写图片描述](http://img.blog.csdn.net/20171227152752648?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)</center>\n",
    "\n",
    "\n",
    "## 2.9 logistic 回归中的梯度下降法 （Logistic Regression Gradient Descent）\n",
    "\n",
    "\n",
    "**对单个样本而言，逻辑回归 Loss function 表达式：**\n",
    " \n",
    "<center> $z= w^{T}x+b\\\\\\hat y=a=\\sigma(z)\\\\L(a, y)=-(y\\log (a)+(1-y)\\log(1-a))$</center>\n",
    "\n",
    "**反向传播过程：（红色部分）**\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20171228150610025?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "**前面过程的 $da$、$dz$ 求导：**\n",
    "\n",
    "<center> $da = \\dfrac{\\partial L}{\\partial a}=-\\dfrac{y}{a}+\\dfrac{1-y}{1-a}\\\\dz = \\dfrac{\\partial L}{\\partial z}=\\dfrac{\\partial L}{\\partial a}\\cdot\\dfrac{\\partial a}{\\partial z}=(-\\dfrac{y}{a}+\\dfrac{1-y}{1-a})\\cdot a(1-a)=a-y$</center>\n",
    "\n",
    "**再对$w1、w2和b$进行求导：**\n",
    "\n",
    "<center> $dw_{1} = \\dfrac{\\partial L}{\\partial w_{1}}=\\dfrac{\\partial L}{\\partial z}\\cdot\\dfrac{\\partial z}{\\partial w_{1}}=x_{1}\\cdot dz=x_{1}(a-y)$</center>\n",
    "\n",
    "**梯度下降法：**\n",
    "\n",
    "<center> $w_{1}:=w_{1}-\\alpha dw_{1}$</center>\n",
    "\n",
    "<center> $w_{2}:=w_{2}-\\alpha dw_{2}$</center>\n",
    "\n",
    "<center> $b:=b-\\alpha db$</center>\n",
    "\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20171228144957788?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "## 2.10 m 个样本的梯度下降 （Gradient Descent on m examples）\n",
    "\n",
    "\n",
    "**m 个样本的梯度下降**\n",
    "\n",
    "对 m 个样本来说，其 Cost function 表达式如下：\n",
    "\n",
    "<center>$z^{(i)}= w^{T}x^{(i)}+b\\\\\\hat y^{(i)}=a^{(i)}=\\sigma(z^{(i)})\\\\J(w,b)=\\dfrac{1}{m}\\sum_{i=1}^{m}L(\\hat y^{(i)}, y^{(i)})=-\\dfrac{1}{m}\\sum_{i=1}^{m}\\left[y^{(i)}\\log\\hat y^{(i)}+(1-y^{(i)})\\log(1-\\hat y^{(i)})\\right]$</center>\n",
    "\n",
    "\n",
    "Cost function 关于w和b的偏导数可以写成所有样本点偏导数和的平均形式：\n",
    "\n",
    "<center>$dw_{1} =\\dfrac{1}{m}\\sum_{i=1}^{m}x_{1}^{(i)}(a^{(i)}-y^{(i)})$</center>\n",
    "\n",
    "<center>$db = \\dfrac{1}{m}\\sum_{i=1}^{m}(a^{(i)}-y^{(i)})$</center>\n",
    "\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20171228155151513?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "## 2.11  （向量化） Vectorization \n",
    "\n",
    "\n",
    "\n",
    "**向量化（Vectorization）**\n",
    "\n",
    "在深度学习的算法中，我们通常拥有大量的数据，在程序的编写过程中，应该尽最大可能的少使用 loop 循环语句，利用 python 可以实现矩阵运算，进而来提高程序的运行速度，避免 for 循环的使用。\n",
    "\n",
    "**逻辑回归向量化**\n",
    "\n",
    "- 输入矩阵$X$：$（n_x,m）$\n",
    "- 权重矩阵$w$：$（n_x,1）$\n",
    "- 偏置$b$：为一个常数\n",
    "- 输出矩阵$Y$：$（1,m）$\n",
    "\n",
    "所有 m 个样本的线性输出 Z 可以用矩阵表示：\n",
    "\n",
    "$Z = w^{T}X+b$\n",
    "\n",
    "```python\n",
    "Z = np.dot(w.T,X) + b\n",
    "A = sigmoid(Z)\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "import numpy as np \n",
    "\n",
    "# 2.11  Vectorization| 向量化 --Andrew Ng \n",
    "\n",
    "a = np.array([1,2,3,4,5])\n",
    "\n",
    "print(a)\n",
    "\n",
    "# [1 2 3 4 5]\n",
    "# [Finished in 0.3s]\n",
    "\n",
    "import time\n",
    "\n",
    "# 随机创建 百万维度的数组 1000000 个数据\n",
    "a = np.random.rand(1000000)\n",
    "b = np.random.rand(1000000)\n",
    "\n",
    "# 记录当前时间 \n",
    "tic = time.time()\n",
    "\n",
    "# 执行计算代码 2 个数组相乘\n",
    "c = np.dot(a,b)\n",
    "\n",
    "# 再次记录时间\n",
    "toc = time.time()\n",
    "\n",
    "# str(1000*(toc-tic)) 计算运行之间 * 1000 毫秒级\n",
    "print('Vectorization vresion:',str(1000*(toc-tic)),' ms')\n",
    "print(c)\n",
    "# Vectorization vresion: 6.009101867675781  ms\n",
    "# [Finished in 1.1s]\n",
    "\n",
    "c = 0\n",
    "tic = time.time()\n",
    "for i in range(1000000):\n",
    "\tc += a[i]*b[i]\n",
    "toc = time.time()\n",
    "print(c)\n",
    "\n",
    "print('For loop :',str(1000*(toc-tic)),' ms')\n",
    "# For loop : 588.9410972595215  ms\n",
    "# c= 249960.353586\n",
    "# NOTE: It is obvious that the for loop method  is too slow\n",
    "```\n",
    "\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20171229100707277?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "## 2.12 更多向量化的例子 (More Vectorization examples) \n",
    "\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20171229111104499?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20171229111121607?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20171229111140140?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "## 2.13 向量化 Logistic 回归 (Vectorizing Logistic Regression )\n",
    " \n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180101231309979?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "## 2.14 向量化 logistic 回归的梯度输出 \n",
    "\n",
    "Vectorizing Logistic Regression's Gradient Computation\n",
    "\n",
    " \n",
    "所有 m 个样本的线性输出 $Z$ 可以用矩阵表示：\n",
    "\n",
    "$Z = w^{T}X+b$\n",
    "\n",
    "```\n",
    "Z = np.dot(w.T,X) + b\n",
    "A = sigmoid(Z)\n",
    "```\n",
    "\n",
    "**逻辑回归梯度下降输出向量化**\n",
    "\n",
    "- $dZ$对于$m$个样本，维度为$（1,m）$，表示为： \n",
    "\n",
    "\t<center>$dZ = A - Y$</center>\n",
    "\n",
    "- db可以表示为： \n",
    "\n",
    "<center>$db = \\dfrac{1}{m}\\sum_{i=1}^{m}dz^{(i)}$</center>\n",
    "\n",
    "```\n",
    " db = 1/m * np.sum(dZ)\n",
    "```\n",
    "\n",
    "- dw可表示为： \n",
    "\n",
    "<center>$dw = \\dfrac{1}{m}X\\cdot dZ^{T}$</center>\n",
    "\n",
    "```\n",
    "dw = 1/m*np.dot(X,dZ.T)\n",
    "```\n",
    "\n",
    "**单次迭代梯度下降算法流程:**\n",
    "\n",
    "```\n",
    "Z = np.dot(w.T,X) + b\n",
    "A = sigmoid(Z)\n",
    "dZ = A-Y\n",
    "dw = 1/m*np.dot(X,dZ.T)\n",
    "db = 1/m*np.sum(dZ)\n",
    "\n",
    "w = w - alpha*dw\n",
    "b = b - alpha*db\n",
    "```\n",
    "\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180102101755019?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180102103747491?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "## 2.15 Python 中的广播（Boadcasting in Python ）\n",
    "\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# 2.15 Boradcasting in python\n",
    "\n",
    "A = np.mat([[56.0, 0.0, 4.4, 68.0],\n",
    "            [1.2, 104.0, 52.0, 8.0],\n",
    "            [1.8, 135.0, 99.0, 0.9]])\n",
    "print(A)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "# axis=0 代表竖直方向相加 列相加\n",
    "cal = A.sum(axis=0)\n",
    "print(cal)\n",
    "# [[  59.   239.   155.4   76.9]]\n",
    "\n",
    "# print(\"cal.reshape(1,4)\",cal.reshape(1,4))\n",
    "# A/cal 相当于（换算百分比） 100* （56/59） = 94.915 \n",
    "# A 矩阵中的每一个元素，与当前所在列的总和相除\n",
    "# cal 根据上面的计算本身就是 1 *4 矩阵，所以cal.reshape(1,4) 这个可以不用\n",
    "percentage = 100 * A / (cal.reshape(1, 4))\n",
    "print('percentage=', percentage)\n",
    "\n",
    "# [[ 94.91525424   0.           2.83140283  88.42652796]\n",
    "#  [  2.03389831  43.51464435  33.46203346  10.40312094]\n",
    "#  [  3.05084746  56.48535565  63.70656371   1.17035111]]\n",
    "```\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180102143728919?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180102143757006?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180102143911182?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "## 2.16  关于 python _ numpy 向量的说明(A note on Python_numpy vectors)\n",
    "\n",
    "\n",
    "```\n",
    "# 2.16 A note on python/numpy vectors\n",
    "\n",
    "# 产生随机 5 个高斯变量存储在 a 中\n",
    "# 官方文档中给出的用法是：numpy.random.rand(d0,d1,…dn) \n",
    "# 以给定的形状创建一个数组，数组元素来符合标准正态分布N(0,1) \n",
    "# 若要获得一般正态分布N(μ,σ^2) 描述则可用sigma * np.random.randn(…) + mu进行表示 \n",
    "\n",
    "a = np.random.randn(5)\n",
    "print('a=', a)\n",
    "# [-0.23427061 -0.79637413 -0.06117785  0.15440186 -1.43061057]\n",
    "\n",
    "# a 的大小 \n",
    "print(a.shape)\n",
    "# (5,)\n",
    "\n",
    "# a 的转置 ，\n",
    "print(a.T)\n",
    "# [-0.5694968  -0.23773807 -0.08906264  0.87211753 -0.08380342]\n",
    "\n",
    "# a 和 a 转置的内积\n",
    "print(np.dot(a, a.T))\n",
    "# 1.15639015502\n",
    "\n",
    "# 因为 a.shape (5,) 不规范\n",
    "\n",
    "# tips tricks 技巧，若要生成随机数组 给出指定的 行列向量\n",
    "\n",
    "b = np.random.randn(5, 1)\n",
    "print(b)\n",
    "# 这是标准的 5 * 1 的列向量\n",
    "# [[ 0.10087547]\n",
    "#  [-1.2177768 ]\n",
    "#  [ 1.55482844]\n",
    "#  [ 1.39440708]\n",
    "#  [-1.72344715]]\n",
    "print(b.T)\n",
    "# 这是标准的 1 * 5 的行向量\n",
    "# [[ 0.10087547 -1.2177768   1.55482844  1.39440708 -1.72344715]]\n",
    "# 5 *1  乘以 1 *5 得到的是一个矩阵 5*5\n",
    "print(np.dot(b, b.T))\n",
    "# [[ 0.08517485  0.38272589 -0.11342526  0.23506654  0.16852131]\n",
    "#  [ 0.38272589  1.71974596 -0.5096667   1.05625134  0.75723604]\n",
    "#  [-0.11342526 -0.5096667   0.15104565 -0.31303236 -0.2244157 ]\n",
    "#  [ 0.23506654  1.05625134 -0.31303236  0.64873937  0.46508706]\n",
    "#  [ 0.16852131  0.75723604 -0.2244157   0.46508706  0.33342507]]\n",
    "\n",
    "\n",
    "# >>> a = np.mat([[1],[2],[3],[4],[5]])\n",
    "# >>> b = np.mat([[2,2,2,2,2]])\n",
    "# >>> c = np.dot(a,b)\n",
    "# >>> c\n",
    "# matrix([[ 2,  2,  2,  2,  2],\n",
    "#         [ 4,  4,  4,  4,  4],\n",
    "#         [ 6,  6,  6,  6,  6],\n",
    "#         [ 8,  8,  8,  8,  8],\n",
    "#         [10, 10, 10, 10, 10]])\n",
    "```\n",
    "\n",
    "\n",
    "- 虽然在 Python 有广播的机制，但是在 Python 程序中，为了保证矩阵运算的正确性，可以使用`reshape()`函数来对矩阵设定所需要进行计算的维度，这是个好的习惯；\n",
    "\n",
    "- 如果用下列语句来定义一个向量，则这条语句生成的 a 的维度为`（5，）`，既不是行向量也不是列向量，称为秩（rank）为 1 的 array，如果对 a 进行转置，则会得到 a 本身，这在计算中会给我们带来一些问题。\n",
    "\n",
    "`a = np.random.randn(5)`\n",
    "\n",
    "- 如果需要定义`（5，1）`或者`（1，5）`向量，要使用下面标准的语句：\n",
    "\n",
    "```\n",
    "a = np.random.randn(5,1)\n",
    "b = np.random.randn(1,5)\n",
    "```\n",
    "\n",
    "- 可以使用`assert`语句对向量或数组的维度进行判断。`assert`会对内嵌语句进行判断，即判断 a 的维度是不是`（5，1）`，如果不是，则程序在此处停止。使用`assert`语句也是一种很好的习惯，能够帮助我们及时检查、发现语句是否正确。\n",
    "\n",
    "`assert(a.shape == (5,1))`\n",
    "\n",
    "- 可以使用`reshape`函数对数组设定所需的维度\n",
    "\n",
    "`a.reshape((5,1))`\n",
    "\n",
    "\n",
    "\n",
    "## 2.17 Jupyter _ ipython 笔记本的快速指南 (Quick tour of Jupyter/ipython notebooks)\n",
    "\n",
    "\n",
    "Windows jupyter install  (Python) 本地搭建\n",
    "\n",
    "`pip3 install  jupyter`\n",
    "\n",
    "启动 jupyter notebook 命令 cmd\n",
    "\n",
    "`jupyter notebook`\n",
    "\n",
    "mac 启动\n",
    "\n",
    "`jupyter-notebook`\n",
    "\n",
    "\n",
    "## 2.18 logistic 损失函数的解释 (Explanation of logistic Regression cost function)\n",
    "\n",
    "\n",
    "**$logistic$ regression 代价函数的解释：**\n",
    "\n",
    "Cost function的由来：\n",
    "\n",
    "预测输出y^的表达式：\n",
    "\n",
    "$\\hat y =\\sigma(w^{T}x+b)$\n",
    "\n",
    "其中，\n",
    "\n",
    "$\\sigma(z)=\\dfrac{1}{1+e^{-z}}$\n",
    "\n",
    "$\\hat y$ 可以看作预测输出为正类（+1）的概率：\n",
    "\n",
    "$\\hat y = P( y=1|x)$\n",
    "\n",
    "当 $y=1$时，$P(y|x)=\\hat y$；\n",
    "当$y=0$时，$P(y|x)=1-\\hat y$。\n",
    "\n",
    "将两种情况整合到一个式子中，可得：\n",
    "\n",
    "$P(y|x)=\\hat y^{y}(1-\\hat y )^{(1-y)}$\n",
    "\n",
    "对上式进行 $log$ 处理（这里是因为 $log$ 函数是单调函数，不会改变原函数的单调性）：\n",
    "\n",
    "$\\log P(y|x)=\\log\\left[\\hat y^{y}(1-\\hat y )^{(1-y)}\\right]=y\\log\\hat y+(1-y)\\log(1-\\hat y)$\n",
    "\n",
    "概率$P(y|x)$越大越好，即判断正确的概率越大越好。这里对上式加上负号，则转化成了单个样本的 Loss function，我们期望其值越小越好：\n",
    "\n",
    "$L(\\hat y, y)=-(y\\log\\hat y+(1-y)\\log(1-\\hat y))$\n",
    "\n",
    "对于 m 个训练样本来说，假设样本之间是独立同分布的，我们总是希望训练样本判断正确的概率越大越好，则有：\n",
    "\n",
    "$\\max \\prod\\limits_{i=1}^{m} {P(y^{(i)}|x^{(i)})}$\n",
    "\n",
    "\n",
    "同样引入 log 函数，加负号，则可以得到 Cost function：\n",
    "\n",
    "$J(w,b)=\\dfrac{1}{m}\\sum_{i=1}^{m}L(\\hat y^{(i)}, y^{(i)})=-\\dfrac{1}{m}\\sum_{i=1}^{m}\\left[y^{(i)}\\log\\hat y^{(i)}+(1-y^{(i)})\\log(1-\\hat y^{(i)})\\right]$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "参考文献：\n",
    "\n",
    "[1]. 大树先生.[吴恩达Coursera深度学习课程 DeepLearning.ai 提炼笔记（1-2）-- 神经网络基础](http://blog.csdn.net/koala_tree/article/details/78045596)\n",
    "\n",
    "\n",
    "---\n",
    " \n",
    "**PS: 欢迎扫码关注公众号：「SelfImprovementLab」！专注「深度学习」，「机器学习」，「人工智能」。以及 「早起」，「阅读」，「运动」，「英语 」「其他」不定期建群 打卡互助活动。**\n",
    "\n",
    "<center><img src=\"http://upload-images.jianshu.io/upload_images/1157146-cab5ba89dfeeec4b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\"></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
