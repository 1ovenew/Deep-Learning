{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursera | Andrew Ng (04-week3)—浅层神经网络\n",
    "\n",
    "在吴恩达深度学习视频以及大树先生的博客提炼笔记基础上添加个人理解，原大树先生博客可查看该链接地址[大树先生的博客](http://blog.csdn.net/koala_tree)- ZJ\n",
    "    \n",
    ">[Coursera 课程](https://www.coursera.org/specializations/deep-learning) |[deeplearning.ai](https://www.deeplearning.ai/) |[网易云课堂](https://mooc.study.163.com/smartSpec/detail/1001319001.htm)\n",
    "\n",
    "   [CSDN](http://blog.csdn.net/junjun_zhao/article/details/79228114)：http://blog.csdn.net/junjun_zhao/article/details/79228114\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## <font  color=#0099ff>3.1 Neural Networks Overview （神经网络概览）\n",
    "\n",
    "1. $z^{[1]}$，我们会使用新的符号，**上标 方括号 1，表示与这些节点相关的量，所谓的“层\"** \n",
    "2. **$x$上标 $(i)$ 表示，第 $i$ 个训练样本, 圆括号是用来表示单个训练样本的**。\n",
    "3.  **在神经网络中我们要做多次计算，反复计算 z 和 a，反复计算 a 和 z，最后计算损失函数。**\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180105091310471?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "---\n",
    "\n",
    "## <font  color=#0099ff>3.2 Neural Network Representation （神经网络表示）\n",
    "\n",
    "\n",
    "简单神经网络示意图：\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180105093450886?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "神经网络基本的结构和符号可以从上面的图中看出，这里不再复述。\n",
    "\n",
    "主要需要注意的一点，是层与层之间参数矩阵的规格大小：\n",
    "\n",
    "- 输入层和隐藏层之间 \n",
    "\n",
    "\t$w^{[1]}−>(4,3)$：前面的 4 是隐层神经元的个数，后面的 3 是输入层神经元的个数；\n",
    "\t$b^{[1]}−>(4,1)$：和隐藏层的神经元个数相同；\n",
    "\t\n",
    "- 隐藏层和输出层之间 \n",
    "\n",
    "\t$w^{[2]}−>(1,4)$：前面的 1 是输出层神经元的个数，后面的 4 是隐层神经元的个数；\n",
    "\t$b^{[2]}−>(1,1)$：和输出层的神经元个数相同；\n",
    "\n",
    "\n",
    "**隐藏层\"的含义是，在训练集中这些中间节点的真正数值，我们是不知道的，训练集中只能看到 输入值和输出值，但是隐藏层中的值，是无法看到的，这就是所谓的“隐藏层”，只是表示你无法在训练集中看到。**\n",
    "\n",
    "由上面我们可以总结出，在神经网络中，我们以相邻两层为观测对象，前面一层作为输入，后面一层作为输出，两层之间的 w 参数矩阵大小为 $(n_{out},n_{in})$，b 参数矩阵大小为 $(n_{out},1)$，这里是作为 $z=wX+b$ 的线性关系来说明的，在神经网络中，$w^{[i]}=w^{T}$。\n",
    "\n",
    "在 **Logistic regression** 中，一般我们都会用 $(n_{in},n_{out})$来表示参数大小，计算使用的公式为：$z = w^{T}X+b$，要注意这两者的区别。\n",
    "\n",
    "**一个双层神经网络 （2 layer NN ）是，只有一个隐藏层的神经网络。**\n",
    "\n",
    "---\n",
    "## <font  color=#0099ff>3.3 Computing a Neural Networks's Output  （计算神经网络的输出）\n",
    "\n",
    "\n",
    "**计算神经网络的输出**\n",
    "\n",
    "除输入层之外每层的计算输出可由下图总结出：\n",
    "\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180105103108080?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "其中，每个结点都对应这两个部分的运算，z 运算和 a 运算。\n",
    "\n",
    "在编程中，我们使用向量化去计算神经网络的输出：\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180105104029500?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "在对应图中的神经网络结构，我们只用 Python 代码去实现右边的四个公式即可实现神经网络的输出计算。\n",
    "\n",
    "$X =a^{[0]}$\n",
    "\n",
    "$z^{[1]}= W^{[1]} × a^{[0]} + b^{[1]}$ \n",
    "\n",
    "$a[1] = \\sigma (z^{[1]})$\n",
    "\n",
    "$z^{[2]}= W^{[2]} × a^{[1]} + b^{[2]}$ \n",
    "\n",
    "$a[2] = \\sigma (z^{[2]})$\n",
    "\n",
    "---\n",
    "## <font  color=#0099ff>3.4  Vectorizing across multiple examples （多个例子中的向量化）\n",
    "\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180107181524643?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "$z^{[1]}_{(i)}= W^{[1]} x^{(i)} + b^{[1]} $\n",
    "\n",
    "$A^{[1]}_{(i)}=σ(z^{[1]}_{(i)})$\n",
    "\n",
    "$Z^{[2]}_{(i)}= W^{[2]} A^{[1]}_{(i)}+ b^{[2]}$\n",
    "\n",
    "$A^{[2]}_{(i)}=σ(Z^{[2]}_{(i)})$\n",
    "\n",
    "---\n",
    "## <font  color=#0099ff>3.5   Explanation for Vectorized implementation (向量化实现的解释)\n",
    "\n",
    "Recap:\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180107195339836?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "**向量化实现**\n",
    "\n",
    "假定在 m 个训练样本的神经网络中，计算神经网络的输出，用向量化的方法去实现可以避免在程序中使用 for 循环，提高计算的速度。\n",
    "\n",
    "下面是实现向量化的解释：\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180107194514293?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "由图可以看出，在 m 个训练样本中，每次计算都是在重复相同的过程，均得到同样大小和结构的输出，所以利用向量化的思想将单个样本合并到一个矩阵中，其大小为$(x_n,m)$，其中 $x_n$ 表示每个样本输入网络的神经元个数，也可以认为是单个样本的特征数，m 表示训练样本的个数。\n",
    "\n",
    "通过向量化，可以更加便捷快速地实现神经网络的计算。\n",
    "\n",
    "---\n",
    "## <font  color=#0099ff>3.6  Activation functions （激活函数）\n",
    "\n",
    "\n",
    "Recap:\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180108102933015?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "**激活函数的选择**\n",
    "\n",
    "几种不同的激活函数 $g(x)$：\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180108102857784?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "sigmoid：$a = \\dfrac{1}{1+e^{-z}}$\n",
    "\n",
    "- 导数：$a' = a(1-a)$\n",
    "\n",
    "tanh：$a=\\dfrac{e^{z}-e^{-z}}{e^{z}+e^{-z}}$\n",
    "\n",
    "- 导数：$a'=1-a^{2}$\n",
    "\n",
    "ReLU（修正线性单元）：$a=max(0,z)$\n",
    "\n",
    "Leaky ReLU：$a=max(0.01z,z)$\n",
    "\n",
    "\n",
    "**激活函数的选择：**\n",
    "\n",
    "$sigmoid$ 函数和 $tanh$ 函数比较：\n",
    "\n",
    "隐藏层：$tanh$ 函数的表现要好于 $sigmoid$ 函数，因为 $tanh$ 取值范围为[−1,+1]，输出分布在 0 值的附近，均值为 0，从隐藏层到输出层数据起到了归一化（均值为 0）的效果。\n",
    "\n",
    "输出层：对于二分类任务的输出取值为 {0,1}，故一般会选择$sigmoid$函数。\n",
    "然而$sigmoid$和$tanh$函数在当$|z|$很大的时候，梯度会很小，在依据梯度的算法中，更新在后期会变得很慢。在实际应用中，要使$|z|$尽可能的落在 0 值附近。\n",
    "\n",
    "$ReLU$弥补了前两者的缺陷，当 z>0 时，梯度始终为 1，从而提高神经网络基于梯度算法的运算速度。然而当 z<0 时，梯度一直为0，但是实际的运用中，该缺陷的影响不是很大。\n",
    "\n",
    "$Leaky ReLU$保证在 z<0 的时候，梯度仍然不为 0。\n",
    "\n",
    "在选择激活函数的时候，如果在不知道该选什么的时候就选择$ReLU$，当然也没有固定答案，要依据实际问题在交叉验证集合中进行验证分析。\n",
    "\n",
    "---\n",
    "## <font  color=#0099ff>3.7 Why do you need non-linear activation functions? (为什么需要非线性激活函数？)\n",
    "\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180108105903549?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "线性激活函数 术语： 恒等激活函数\n",
    "\n",
    "如果你要用线性激活函数，或者叫恒等激活函数，或者如果没有激活函数，那么神经网络只是把输入线性组合再输出,那么无论你的神经网络有多少层，一直在做的只是计算线性激活函数，所以不如直接去掉全部隐藏层，所以使用线性激活函数是没有用的。\n",
    "\n",
    "唯一可以用线性激活函数的地方，通常就是输出层。\n",
    "\n",
    "---\n",
    "## <font  color=#0099ff>3.8 Derivatives of activation functions (激活函数的导数 )\n",
    "\n",
    "sigmoid：$a = \\dfrac{1}{1+e^{-z}}$\n",
    "\n",
    "- 导数：$a' = a(1-a)$\n",
    "\n",
    "tanh：$a=\\dfrac{e^{z}-e^{-z}}{e^{z}+e^{-z}}$\n",
    "\n",
    "- 导数：$a'=1-a^{2}$\n",
    "\n",
    "ReLU（修正线性单元）：$a=max(0,z)$\n",
    "\n",
    "Leaky ReLU：a=$max(0.01z,z)$\n",
    "\n",
    "\n",
    "**激活函数的选择：**\n",
    "\n",
    "$sigmoid$ 函数和 $tanh$ 函数比较：\n",
    "\n",
    "隐藏层：$tanh$ 函数的表现要好于 $sigmoid$ 函数，因为 $tanh$ 取值范围为[−1,+1]，输出分布在 0 值的附近，均值为 0，从隐藏层到输出层数据起到了归一化（均值为 0）的效果。\n",
    "\n",
    "输出层：对于二分类任务的输出取值为 {0,1}，故一般会选择$sigmoid$函数。\n",
    "然而$sigmoid$和$tanh$函数在当$|z|$很大的时候，梯度会很小，在依据梯度的算法中，更新在后期会变得很慢。在实际应用中，要使$|z|$尽可能的落在 0 值附近。\n",
    "\n",
    "$ReLU$弥补了前两者的缺陷，当 z>0 时，梯度始终为 1，从而提高神经网络基于梯度算法的运算速度。然而当 z<0 时，梯度一直为0，但是实际的运用中，该缺陷的影响不是很大。\n",
    "\n",
    "$Leaky ReLU$保证在 z<0 的时候，梯度仍然不为 0。\n",
    "\n",
    "在选择激活函数的时候，如果在不知道该选什么的时候就选择$ReLU$，当然也没有固定答案，要依据实际问题在交叉验证集合中进行验证分析。\n",
    "\n",
    "---\n",
    "## <font  color=#0099ff>3.9 Gradient descent for neural networks （神经网络的梯度下降法）\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180108144422405?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180108144524459?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "神经网络的梯度下降法\n",
    "\n",
    "以本节中的浅层神经网络为例，我们给出神经网络的梯度下降法的公式。\n",
    "\n",
    "- 参数：$W^{[1]},b^{[1]},W^{[2]},b^{[2]}$；\n",
    "- 输入层特征向量个数：$n_{x}=n^{[0]}$；\n",
    "- 隐藏层神经元个数：$n^{[1]}$；\n",
    "- 输出层神经元个数：$n^{[2]}=1$；\n",
    "- $W^{[1]}$ 的维度为$(n^{[1]},n^{[0]})$，$b^{[1]}$的维度为 $(n^{[1]},1)$；\n",
    "- $W^{[2]}$的维度为$(n^{[2]},n^{[1]})$，$b^{[2]}$的维度为 $(n^{[2]},1)$；\n",
    "\n",
    "\n",
    "下面为该例子的神经网络反向梯度下降公式（左）和其代码向量化（右）：\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180108145800240?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "---\n",
    "## <font  color=#0099ff>3.10  Backpropagation intuition (直观理解反向传播)\n",
    "\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180108155032705?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180108155151882?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "---\n",
    "## <font  color=#0099ff>3.11 Random Initialization (随机初始化 ) \n",
    "\n",
    "\n",
    "**随机初始化**\n",
    "\n",
    "如果在初始时，两个隐藏神经元的参数设置为相同的大小，那么两个隐藏神经元对输出单元的影响也是相同的，通过反向梯度下降去进行计算的时候，会得到同样的梯度大小，所以在经过多次迭代后，两个隐藏层单位仍然是对称的。无论设置多少个隐藏单元，其最终的影响都是相同的，那么多个隐藏神经元就没有了意义。\n",
    "\n",
    "在初始化的时候，W 参数要进行随机初始化，b 则不存在对称性的问题它可以设置为 0。\n",
    "\n",
    "以 2 个输入，2 个隐藏神经元为例：\n",
    "\n",
    "```\n",
    "W = np.random.rand((2,2))* 0.01\n",
    "b = np.zero((2,1))\n",
    "```\n",
    "\n",
    "这里我们将 W 的值乘以 0.01 是为了尽可能使得权重 W 初始化为较小的值，这是因为如果使用 sigmoid 函数或者 tanh 函数作为激活函数时，W 比较小，则 $Z = WX+b$ 所得的值也比较小，处在 0 的附近，0 点区域的附近梯度较大，能够大大提高算法的更新速度。而如果 W 设置的太大的话，得到的梯度较小，训练过程因此会变得很慢。\n",
    "\n",
    "ReLU 和 Leaky ReLU 作为激活函数时，不存在这种问题，因为在大于 0 的时候，梯度均为 1。\n",
    "\n",
    "\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180108164629007?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180108164902949?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "参考文献：\n",
    "\n",
    "[1]. 大树先生.[吴恩达Coursera深度学习课程 DeepLearning.ai 提炼笔记（1-3）-- 浅层神经网络](http://blog.csdn.net/koala_tree/article/details/78059952)\n",
    "\n",
    "\n",
    "---\n",
    " \n",
    "**PS: 欢迎扫码关注公众号：「SelfImprovementLab」！专注「深度学习」，「机器学习」，「人工智能」。以及 「早起」，「阅读」，「运动」，「英语 」「其他」不定期建群 打卡互助活动。**\n",
    "\n",
    "<center><img src=\"http://upload-images.jianshu.io/upload_images/1157146-cab5ba89dfeeec4b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
