{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursera | Andrew Ng (02-week2)—改善深层神经网络：优化算法\n",
    "\n",
    "在吴恩达深度学习视频以及大树先生的博客提炼笔记基础上添加个人理解，原大树先生博客可查看该链接地址[大树先生的博客](http://blog.csdn.net/koala_tree)- ZJ\n",
    "    \n",
    ">[Coursera 课程](https://www.coursera.org/specializations/deep-learning) |[deeplearning.ai](https://www.deeplearning.ai/) |[网易云课堂](https://mooc.study.163.com/smartSpec/detail/1001319001.htm)\n",
    "\n",
    "[CSDN]()：\n",
    "\n",
    "---\n",
    "\n",
    "Optimization Algorithms\n",
    "\n",
    "\n",
    "## <font color=#0099ff>2.1 Mini-batch Gradient Descent (Mini-batch 梯度下降法)\n",
    "\n",
    "\n",
    "学习目标：本周你将学习优化算法，这能让你的神经网络运行得更快。，**机器学习的应用是一个高度依赖经验的过程**，伴随着大量迭代的过程，你需要训练诸多模型才能找到合适的那一个。优化算法能够帮助你快速训练模型。\n",
    "\n",
    "**我们可以利用一个巨大的数据集来训练神经网络，而在巨大的数据集基础上进行训练速度很慢，因此，你会发现使用快速的优化算法，使用好用的优化算法能够，大大提高你和团队的效率。**\n",
    "\n",
    "\n",
    "Mini-batch 梯度下降法\n",
    "\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180118153101581?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "对整个训练集进行梯度下降法的时候，我们必须处理整个训练数据集，然后才能进行一步梯度下降，即每一步梯度下降法需要对整个训练集进行一次处理，如果训练数据集很大的时候，如有 500 万或 5000 万的训练数据，处理速度就会比较慢。\n",
    "\n",
    "但是如果每次处理训练数据的一部分即进行梯度下降法，则我们的算法速度会执行的更快。而处理的这些一小部分训练子集即称为 Mini-batch。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180118154649146?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "对于普通的梯度下降法，一个 epoch 只能进行一次梯度下降；而对于 Mini-batch 梯度下降法，一个 epoch 可以进行 Mini-batch 的个数次梯度下降。\n",
    "\n",
    "---\n",
    "\n",
    "## <font color=#0099ff>2.2 Understanding Mini-batch Gradient descent (理解  mini-batch  梯度下降法 )\n",
    "\n",
    "\n",
    "**不同 size 大小的比较**\n",
    "\n",
    "普通的 batch 梯度下降法和 Mini-batch 梯度下降法代价函数的变化趋势，如下图所示：\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180118181131876?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "**batch梯度下降：** \n",
    "\n",
    "- 对所有 m 个训练样本执行一次梯度下降，每一次迭代时间较长；\n",
    "- Cost function 总是向减小的方向下降。\n",
    "\n",
    "**随机梯度下降：** \n",
    "\n",
    "- 对每一个训练样本执行一次梯度下降，但是丢失了向量化带来的计算加速；\n",
    "- Cost function 总体的趋势向最小值的方向下降，但是无法到达全局最小值点，呈现波动的形式。\n",
    "\n",
    "**Mini-batch梯度下降：** \n",
    "\n",
    "- 选择一个$1<size<m$ 的合适的size进行Mini-batch梯度下降，可以实现快速学习，也应用了向量化带来的好处。\n",
    "- Cost function 的下降处于前两者之间。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180118184128200?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "**Mini-batch 大小的选择**\n",
    "\n",
    "- 如果训练样本的大小比较小时，如 m⩽ 2000 时 —— 选择 batch 梯度下降法；\n",
    "- 如果训练样本的大小比较大时，典型的大小为： \n",
    "\t $2^{6}、2^{7}、\\cdots、2^{10}$；\n",
    "- Mini-batch 的大小要符合 CPU/GPU 内存。\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## <font color=#0099ff>2.3 Exponentially weighted averages (指数加权平均)\n",
    "\n",
    "**指数加权平均**\n",
    "\n",
    "指数加权平均的关键函数： \n",
    "\n",
    "<center>$v_{t} = \\beta v_{t-1}+(1-\\beta)\\theta_{t}$</center>\n",
    "\n",
    "下图是一个关于天数和温度的散点图：\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180118193050412?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "- 当 $β=0.9$ 时，指数加权平均最后的结果如图中红色线所示；\n",
    "- 当 $β=0.98$ 时，指数加权平均最后的结果如图中绿色线所示；\n",
    "- 当 $β=0.5$ 时，指数加权平均最后的结果如下图中黄色线所示；\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180118193104409?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## <font color=#0099ff>2.4 Understanding Exponentially weighted averages (理解指数加权平均)\n",
    "\n",
    "\n",
    "例子，当 β=0.9 时：\n",
    "\n",
    "$v_{100} = 0.9v_{99}+0.1\\theta_{100}\\\\v_{99} = 0.9v_{98}+0.1\\theta_{99}\\\\v_{98} = 0.9v_{97}+0.1\\theta_{98}\\\\ \\ldots$\n",
    "\n",
    "展开：\n",
    "\n",
    "$v_{100}=0.1\\theta_{100}+0.9(0.1\\theta_{99}+0.9(0.1\\theta_{98}+0.9v_{97}))\\\\=0.1\\theta_{100}+0.1\\times0.9\\theta_{99}+0.1\\times(0.9)^{2}\\theta_{98}+0.1\\times(0.9)^{3}\\theta_{97}+\\cdots$\n",
    "\n",
    "\n",
    "上式中所有 θ 前面的系数相加起来为 1 或者接近于 1，称之为偏差修正。\n",
    "\n",
    "总体来说存在，$(1-\\varepsilon)^{1/\\varepsilon}=\\dfrac{1}{e}$，在我们的例子中，$1-\\varepsilon=\\beta=0.9$，即 $0.9^{10}\\approx 0.35\\approx\\dfrac{1}{e}$。相当于大约 10 天后，系数的峰值（这里是0.1）下降到原来的$\\dfrac{1}{e}$，只关注了过去 10 天的天气。\n",
    "\n",
    "**指数加权平均实现**\n",
    "\n",
    "$v_{0} =0\\\\\n",
    "v_{1}= \\beta v_{0}+(1-\\beta)\\theta_{1}\\\\\n",
    "v_{2}= \\beta v_{1}+(1-\\beta)\\theta_{2}\\\\\n",
    "v_{3}= \\beta v_{2}+(1-\\beta)\\theta_{3}\\\\\n",
    "\\ldots$\n",
    "\n",
    "因为，在计算当前时刻的平均值，只需要前一天的平均值和当前时刻的值，所以在数据量非常大的情况下，指数加权平均在节约计算成本的方面是一种非常有效的方式，可以很大程度上减少计算机资源存储和内存的占用。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## <font color=#0099ff>2.5 Bias correction in Exponentially weighted averages (指数加权平均的偏差修正)\n",
    "\n",
    "在我们执行指数加权平均的公式时，当 β=0.98 时，我们得到的并不是图中的绿色曲线，而是下图中的紫色曲线，其起点比较低。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180119085627702?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "- 原因： \n",
    "\n",
    "<center>$v_{0}=0\\\\v_{1}=0.98v_{0}+0.02\\theta_{1}=0.02\\theta_{1}\\\\v_{2}=0.98v_{1}+0.02\\theta_{2}=0.98\\times0.02\\theta_{1}+0.02\\theta_{2}=0.0196\\theta_{1}+0.02\\theta_{2}$</center>\n",
    "\n",
    "\n",
    "如果第一天的值为如40，则得到的 $v_{1}=0.02\\times40=8$，则得到的值要远小于实际值，后面几天的情况也会由于初值引起的影响，均低于实际均值。\n",
    "\n",
    "- **偏差修正：** \n",
    "\n",
    "使用:$\\dfrac{v_{t}}{1-\\beta^{t}}$\n",
    "\n",
    "当 t=2 时： \n",
    "\n",
    "<center>$1-\\beta^{t}=1-(0.98)^{2}=0.0396$</center>\n",
    "\n",
    "<center>$\\dfrac{v_{2}}{0.0396}=\\dfrac{0.0196\\theta_{1}+0.02\\theta_{2}}{0.0396}$</center>\n",
    "\n",
    "\n",
    "偏差修正得到了绿色的曲线，在开始的时候，能够得到比紫色曲线更好的计算平均的效果。随着 t 逐渐增大，$\\beta^{t}$接近于 0，所以后面绿色的曲线和紫色的曲线逐渐重合了。\n",
    "\n",
    "虽然存在这种问题，但是在实际过程中，一般会忽略前期均值偏差的影响。\n",
    "\n",
    "---\n",
    "\n",
    "## <font color=#0099ff>2.6 Gradient descent with Momentum (动量梯度下降法)\n",
    "\n",
    "**动量（ Momentum ）梯度下降法**\n",
    "\n",
    "动量梯度下降的基本思想就是计算梯度的指数加权平均数，并利用该梯度来更新权重。\n",
    "\n",
    "在我们优化 Cost function 的时候，以下图所示的函数图为例：\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180119094833477?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "在利用梯度下降法来最小化该函数的时候，每一次迭代所更新的代价函数值如图中蓝色线所示在上下波动，而这种幅度比较大波动，减缓了梯度下降的速度，而且我们只能使用一个较小的学习率来进行迭代。\n",
    "\n",
    "如果用较大的学习率，结果可能会如紫色线一样偏离函数的范围，所以为了避免这种情况，只能用较小的学习率。\n",
    "\n",
    "但是我们又希望在如图的纵轴方向梯度下降的缓慢一些，不要有如此大的上下波动，在横轴方向梯度下降的快速一些，使得能够更快的到达最小值点，而这里用动量梯度下降法既可以实现，如红色线所示。\n",
    "\n",
    "**算法实现**\n",
    "\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180119095627388?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "β 常用的值是 0.9。\n",
    "\n",
    "在我们进行动量梯度下降算法的时候，由于使用了指数加权平均的方法。原来在纵轴方向上的上下波动，经过平均以后，接近于0，纵轴上的波动变得非常的小；但在横轴方向上，所有的微分都指向横轴方向，因此其平均值仍然很大。最终实现红色线所示的梯度下降曲线。\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## <font color=#0099ff>2.7 RMSprop\n",
    "\n",
    "**RMSprop**\n",
    "\n",
    "除了上面所说的 Momentum 梯度下降法，RMSprop（root mean square prop）也是一种可以加快梯度下降的算法。\n",
    "\n",
    "同样算法的样例实现如下图所示：\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180119132731280?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "这里假设参数b的梯度处于纵轴方向，参数w的梯度处于横轴方向（当然实际中是处于高维度的情况），利用RMSprop算法，可以减小某些维度梯度更新波动较大的情况，如图中蓝色线所示，使其梯度下降的速度变得更快，如图绿色线所示。\n",
    "\n",
    "在如图所示的实现中，RMSprop将微分项进行平方，然后使用平方根进行梯度更新，同时为了确保算法不会除以0，平方根分母中在实际使用会加入一个很小的值如 $\\varepsilon=10^{-8}$。\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## <font color=#0099ff>2.8 Adam Optimization algorithms ( Adam 优化算法)\n",
    "    \n",
    "**Adam 优化算法**\n",
    "\n",
    "Adam 优化算法的基本思想就是将 Momentum 和 RMSprop 结合起来形成的一种适用于不同深度学习结构的优化算法。\n",
    "\n",
    "**算法实现**\n",
    "\n",
    "- 初始化：$V_{dw} = 0，S_{dw}=0，V_{db}=0，S_{db} = 0$\n",
    "- 第 t 次迭代： \n",
    "\t- Compute $ dw，db$ on the current mini-batch\n",
    "\t- $V_{dw}=\\beta_{1}V_{dw}+(1-\\beta_{1})dw，V_{db}=\\beta_{1}V_{db}+(1-\\beta_{1})db$\n",
    "\t- $S_{dw}=\\beta_{2}S_{dw}+(1-\\beta_{2})(dw)^{2}，S_{db}=\\beta_{2}S_{db}+(1-\\beta_{2})(db)^{2}$\n",
    "\t- $V_{dw}^{corrected} = V_{dw}/(1-\\beta_{1}^{t})，V_{db}^{corrected} = V_{db}/(1-\\beta_{1}^{t})$\n",
    "\t- $S_{dw}^{corrected} = S_{dw}/(1-\\beta_{2}^{t})，S_{db}^{corrected} = S_{db}/(1-\\beta_{2}^{t})$\n",
    "\t- $w:=w-\\alpha\\dfrac{V_{dw}^{corrected}}{\\sqrt{S_{dw}^{corrected}}+\\varepsilon}，b:=b-\\alpha\\dfrac{V_{db}^{corrected}}{\\sqrt{S_{db}^{corrected}}+\\varepsilon}$\n",
    "\n",
    "**超参数的选择**\n",
    "\n",
    "- $\\alpha$：需要进行调试；\n",
    "- $\\beta_{1}$：常用缺省值为 0.9，$dw$ 的加权平均；\n",
    "- $\\beta_{2}$：推荐使用 0.999，$dW^2$ 的加权平均值；\n",
    "- $ε$：推荐使用$10^{-8}$。\n",
    "\n",
    "**Adam 代表的是 Adaptive Moment Estimation**。\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## <font color=#0099ff>2.9 Learning rate decay (学习率衰减)\n",
    "\n",
    "\n",
    "**学习率衰减**\n",
    "\n",
    "在我们利用  mini-batch  梯度下降法来寻找 Cost function 的最小值的时候，如果我们设置一个固定的学习速率 α，则算法在到达最小值点附近后，由于不同batch 中存在一定的噪声，使得不会精确收敛，而一直会在一个最小值点较大的范围内波动，如下图中蓝色线所示。\n",
    "\n",
    "但是如果我们使用学习率衰减，逐渐减小学习速率 α，在算法开始的时候，学习速率还是相对较快，能够相对快速的向最小值点的方向下降。但随着α的减小，下降的步伐也会逐渐变小，最终会在最小值附近的一块更小的区域里波动，如图中绿色线所示。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180119151118645?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "**学习率衰减的实现**\n",
    "\n",
    "- 常用：\n",
    "\n",
    "<center>$\\alpha = \\dfrac{1}{1+decay\\_rate*epoch\\_num}\\alpha_{0}$</center>\n",
    "\t\t\n",
    "- 指数衰减：\n",
    "\n",
    "<center>$\\alpha = 0.95^{epoch\\_num}\\alpha_{0}$</center>\n",
    "\n",
    "- 其他：\n",
    "\n",
    "<center>$\\alpha = \\dfrac{k}{epoch\\_num}\\cdot\\alpha_{0}$</center>\n",
    "\n",
    "- 离散下降（不同阶段使用不同的学习速率）\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## <font color=#0099ff>2.10 The problem of local optima (局部最优的问题)\n",
    "\n",
    "**局部最优问题**\n",
    "\n",
    "在低纬度的情形下，我们可能会想象到一个Cost function 如左图所示，存在一些局部最小值点，在初始化参数的时候，如果初始值选取的不得当，会存在陷入局部最优点的可能性。\n",
    "\n",
    "但是，如果我们建立一个神经网络，通常梯度为零的点，并不是如左图中的局部最优点，而是右图中的鞍点（叫鞍点是因为其形状像马鞍的形状）。\n",
    "\n",
    "![这里写图片描述](http://img.blog.csdn.net/20180119163616094?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvSlVOSlVOX1pIQU8=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n",
    "\n",
    "\n",
    "在一个具有高维度空间的函数中，如果梯度为 0，那么在每个方向，Cost function 可能是凸函数，也有可能是凹函数。但如果参数维度为 2万维，想要得到局部最优解，那么所有维度均需要是凹函数，其概率为 $2^{−20000}$，可能性非常的小。也就是说，在低纬度中的局部最优点的情况，并不适用于高纬度，我们在梯度为 0 的点更有可能是鞍点。\n",
    "\n",
    "**在高纬度的情况下：** \n",
    "\n",
    "* 几乎不可能陷入局部最小值点； \n",
    "* 处于鞍点的停滞区会减缓学习过程，利用如 Adam 等算法进行改善。\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "参考文献：\n",
    "\n",
    "[1]. 大树先生.[吴恩达Coursera深度学习课程 DeepLearning.ai 提炼笔记（2-2）-- 优化算法](http://blog.csdn.net/koala_tree/article/details/78199611)\n",
    "\n",
    "---\n",
    " \n",
    "**PS: 欢迎扫码关注公众号：「SelfImprovementLab」！专注「深度学习」，「机器学习」，「人工智能」。以及 「早起」，「阅读」，「运动」，「英语 」「其他」不定期建群 打卡互助活动。**\n",
    "\n",
    "<center><img src=\"http://upload-images.jianshu.io/upload_images/1157146-cab5ba89dfeeec4b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
